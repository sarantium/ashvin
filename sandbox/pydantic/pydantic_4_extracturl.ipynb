{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def guess_sitemap_url(base_url):\n",
    "    \"\"\"Tries various methods to find the sitemap URL.\"\"\"\n",
    "    parsed_url = urlparse(base_url)\n",
    "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "\n",
    "    # Common sitemap patterns\n",
    "    common_patterns = ['sitemap.xml', 'sitemap_index.xml', 'sitemap1.xml', 'sitemap_index.xml.gz']\n",
    "    for pattern in common_patterns:\n",
    "        url = urljoin(base_url, pattern)\n",
    "        if check_url(url):\n",
    "            return url\n",
    "\n",
    "    # Check robots.txt\n",
    "    robots_url = urljoin(base_url, 'robots.txt')\n",
    "    sitemap_url = parse_robots_txt(robots_url)\n",
    "    if sitemap_url:\n",
    "        return sitemap_url\n",
    "\n",
    "    # Check main page HTML for sitemap links\n",
    "    main_page_sitemap_url = find_sitemap_in_html(base_url)\n",
    "    if main_page_sitemap_url:\n",
    "        return main_page_sitemap_url\n",
    "\n",
    "    return None\n",
    "\n",
    "def check_url(url):\n",
    "    \"\"\"Check if the URL is accessible and returns a status code of 200.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.status_code == 200\n",
    "\n",
    "def parse_robots_txt(robots_url):\n",
    "    \"\"\"Parse the robots.txt file to find sitemap URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(robots_url)\n",
    "        if response.status_code == 200:\n",
    "            lines = response.text.splitlines()\n",
    "            for line in lines:\n",
    "                if line.lower().startswith('sitemap:'):\n",
    "                    sitemap_url = line.split(':')[1].strip()\n",
    "                    if check_url(sitemap_url):\n",
    "                        return sitemap_url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse robots.txt: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def find_sitemap_in_html(url):\n",
    "    \"\"\"Inspect the main page HTML to find a sitemap link.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            if 'sitemap' in link['href'].lower():\n",
    "                full_sitemap_url = urljoin(url, link['href'])\n",
    "                if check_url(full_sitemap_url):\n",
    "                    return full_sitemap_url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to find sitemap in HTML: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def extract_urls_from_sitemap(sitemap_url):\n",
    "    \"\"\"Extracts URLs from the given sitemap URL.\"\"\"\n",
    "    response = requests.get(sitemap_url)\n",
    "    xml_content = response.content.decode(\"utf-8\")\n",
    "    root = ET.ElementTree(ET.fromstring(xml_content)).getroot()\n",
    "    ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "    url_elements = root.findall('.//ns:url/ns:loc', namespaces=ns)\n",
    "    urls = [url_element.text for url_element in url_elements]\n",
    "    return urls\n",
    "\n",
    "def fetch_url_content(url):\n",
    "    \"\"\"Fetches main content for a given URL, handling HTML content, Markdown styled text, including code blocks, and media files.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extracting regular content\n",
    "        content = []\n",
    "        for element in soup.select('article, section, div.main-content'):\n",
    "            paragraphs = element.find_all(['p', 'h1', 'h2', 'h3', 'li'])\n",
    "            for para in paragraphs:\n",
    "                text = para.get_text(strip=True)\n",
    "                if text:\n",
    "                    content.append(text.replace('\\n', ' ').strip())\n",
    "\n",
    "        # Extracting code blocks\n",
    "        code_blocks = soup.find_all(['pre', 'code'])\n",
    "        for block in code_blocks:\n",
    "            code_text = block.get_text(strip=True)\n",
    "            if code_text:\n",
    "                # Adding code text with Markdown code block syntax\n",
    "                content.append(f\"```{block.get('class')[0] if block.get('class') else 'python'}\\n{code_text}\\n```\")\n",
    "        \n",
    "        # Extracting images and videos\n",
    "        media_files = soup.find_all(['img', 'video', 'a'])\n",
    "        for media in media_files:\n",
    "            if media.name == 'img' and media.get('src'):\n",
    "                content.append(f\"![Image]({media['src']})\")\n",
    "            elif media.name == 'video' and media.get('src'):\n",
    "                content.append(f\"![Video]({media['src']})\")\n",
    "            elif media.name == 'a' and media.get('href') and any(media['href'].endswith(ext) for ext in ['.pdf', '.zip', '.docx']):\n",
    "                content.append(f\"[Download File]({media['href']})\")\n",
    "\n",
    "        return ' '.join(content), len(' '.join(content))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"Error\", 0\n",
    "\n",
    "\n",
    "\n",
    "def fetch_sitemap_content(sitemap_url):\n",
    "    \"\"\"Fetches the content of all URLs found in a sitemap sequentially for better progress tracking.\"\"\"\n",
    "    urls = extract_urls_from_sitemap(sitemap_url)\n",
    "    page_texts = []\n",
    "    total_length = 0\n",
    "\n",
    "    print(f\"Starting to fetch content from {len(urls)} URLs...\")\n",
    "\n",
    "    for url in tqdm(urls, desc=\"Fetching URLs\"):\n",
    "        text, length = fetch_url_content(url)\n",
    "        page_texts.append(text)\n",
    "        total_length += length\n",
    "\n",
    "    return page_texts, len(urls), total_length\n",
    "\n",
    "def extract_stats(sitemap_url, save_to_file=False):\n",
    "    \"\"\"Extracts stats from sitemap processing and optionally saves to a file.\"\"\"\n",
    "    page_contents, num_urls, total_text_length = fetch_sitemap_content(sitemap_url)\n",
    "    print(f\"Total URLs fetched: {num_urls}\")\n",
    "    print(f\"Total length of text fetched: {total_text_length}\")\n",
    "\n",
    "    if save_to_file:\n",
    "        with open('pages.txt', 'w', encoding='utf-8') as f:\n",
    "            for content in page_contents:\n",
    "                if content != \"Error\":\n",
    "                    f.write(f\"{content}\\n\")\n",
    "        print(\"Content saved to 'pages.txt'.\")\n",
    "\n",
    "# Example usage:\n",
    "# base_url = 'https://www.example.com'\n",
    "# sitemap_url = guess_sitemap_url(base_url)\n",
    "# if sitemap_url:\n",
    "#     extract_stats(sitemap_url, save_to_file=True)\n",
    "# else:\n",
    "#     print(\"Sitemap not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch content from 37 URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching URLs: 100%|██████████| 37/37 [00:24<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs fetched: 37\n",
      "Total length of text fetched: 0\n",
      "Content saved to 'pages.txt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://kraftful.com'\n",
    "sitemap_url = guess_sitemap_url(base_url)\n",
    "\n",
    "if sitemap_url:\n",
    "    extract_stats(sitemap_url, True)\n",
    "else:\n",
    "    print('Sitemap not found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
