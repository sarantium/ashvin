{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import enum\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "from pydantic import BaseModel, Field, StringConstraints, conlist, field_validator\n",
    "from typing import List, Union\n",
    "from typing_extensions import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load API key\n",
    "\n",
    "dotenv_path = Path(r\"C:\\Storage\\python_projects\\ashvin\\.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# main constants\n",
    "\n",
    "GPT_MODEL = \"gpt-4o\" # points to latest GPT model\n",
    "\n",
    "#instantiate client\n",
    "client = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query plan primitives\n",
    "\n",
    "class QueryType(str, enum.Enum):\n",
    "    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n",
    "\n",
    "    SINGLE_QUESTION = \"SINGLE\"\n",
    "    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n",
    "\n",
    "class Query(BaseModel):\n",
    "    \"\"\"Class representing a single question in a query plan.\"\"\"\n",
    "\n",
    "    id: int = Field(..., description=\"Unique id of the query\")\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"Question asked using a question answering system\",\n",
    "    )\n",
    "    dependencies: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of sub questions that need to be answered before asking this question\",\n",
    "    )\n",
    "    node_type: QueryType = Field(\n",
    "        default=QueryType.SINGLE_QUESTION,\n",
    "        description=\"Type of question, either a single question or a multi-question merge\",\n",
    "    )\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n",
    "\n",
    "    query_graph: List[Query] = Field(\n",
    "        ..., description=\"The query graph representing the plan\"\n",
    "    )\n",
    "\n",
    "    def _dependencies(self, ids: List[int]) -> List[Query]:\n",
    "        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n",
    "\n",
    "        return [q for q in self.query_graph if q.id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper\n",
    "\n",
    "def wrapper(system_prompt: str | None = None, user_context: str | list = None, response_model: BaseModel | None = None, max_retries: int = 3):\n",
    "    \"\"\"Wrapper function to generate LLM completion\"\"\"\n",
    "    messages = []\n",
    "    if system_prompt is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    if user_context is not None:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_context})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        response_model=response_model,\n",
    "        max_retries=max_retries,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised wrapper\n",
    "\n",
    "def wrapper(\n",
    "    system_prompt: str | None = None, \n",
    "    user_context: Union[str, list] | None = None, \n",
    "    response_model: BaseModel | None = None, \n",
    "    max_retries: int = 3, \n",
    "    additional_messages: List[dict] | None = None\n",
    "):\n",
    "    \"\"\"Wrapper function to generate LLM completion\"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # Add system prompt if provided\n",
    "    if system_prompt is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    # Add user context if provided\n",
    "    if user_context is not None:\n",
    "        if isinstance(user_context, list):\n",
    "            for context in user_context:\n",
    "                messages.append({\"role\": \"user\", \"content\": context})\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_context})\n",
    "\n",
    "    # Add additional messages if provided\n",
    "    if additional_messages is not None:\n",
    "        messages.extend(additional_messages)\n",
    "\n",
    "    # Generate the completion\n",
    "    completion = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        response_model=response_model,\n",
    "        max_retries=max_retries,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_system_prompt = \"\"\"\n",
    "You are a world class query planning algorithm capable of breaking apart questions into its dependency queries such that the answers can be used to inform the parent question. \n",
    "Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies.\n",
    "Your compute graph should both decompose and recompose queries in order to answer the parent question.\n",
    "Before you call the function, think step-by-step to get a better understanding of the problem.\n",
    "\"\"\"\n",
    "\n",
    "optimise_system_prompt = \"\"\"\n",
    "You are a highly sophisticated query optimization algorithm designed to enhance the quality of query plans.\n",
    "Your primary function is not to answer the queries but to meticulously improve the query graph. \n",
    "This involves ensuring each step is clearly defined and leverages detailed dependencies to construct a comprehensive and precise query sequence.\n",
    "Before making any modifications, thoroughly evaluate the existing query plan to identify opportunities where additional or more detailed questions or steps could enhance clarity or add value.  \n",
    "Your output should detail an optimised query graph where each question is well-defined and logically sequenced based on its dependencies.\n",
    "Ensure no loss of necessary detail and increase the questions or steps where it adds to the robustness and precision of the information-gathering process.\n",
    "\"\"\"\n",
    "\n",
    "question = \"I want to use AI to generate a exceptionally high quality science fiction and fantasy short story\"\n",
    "\n",
    "plan_user_context = f\"Consider: {question}\\nGenerate the correct, complete query plan to answer the parent question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_response = wrapper(plan_system_prompt, plan_user_context, QueryPlan)\n",
    "plan_response_json= plan_response.model_dump_json()\n",
    "plan_response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuggestionList(BaseModel):\n",
    "    \"\"\"A detailed list of suggestions to optimise the query plan\"\"\"\n",
    "\n",
    "    suggestions: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_plan_list = wrapper(optimise_system_prompt, plan_response_json, SuggestionList)\n",
    "suggestions = optimised_plan_list.model_dump_json()\n",
    "for suggestion in optimised_plan_list.suggestions:\n",
    "    print(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_plan_user_context = f\"\"\"\n",
    "Consider the original parent question : {question}\n",
    "Consider the list of suggestions for optimising the query plan: {suggestions}.\n",
    "Apply them to improve the original query plan : {plan_response_json}.\n",
    "Generate a revised, optimised query plan that answers the parent question.\n",
    "Query Ids and dependencies should be listed sequentially.\n",
    "\"\"\"\n",
    "optimised_plan = wrapper(optimise_system_prompt, optimised_plan_user_context, QueryPlan)\n",
    "optimised_plan_dump= optimised_plan.model_dump()\n",
    "optimised_plan_dump"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
