#Creating Safe Systems

### Discuss How to Regulate Deployed Systems Instead of Released Models to Ensure Safety

Focusing regulations on deployed systems rather than released models ensures that AI technologies are used safely without hindering development.

- **Regulating Deployed Systems**: Laws should mandate safety checks and transparency for systems using AI models. This includes evaluating their behavior in practice, requiring oversight and monitoring.

  Example: AI systems should have human oversight, appeal processes, and automated monitoring for harmful outputs.
  
  ```mermaid
  graph TD
    A[Deploy System] --> B[Safety Check]
    B --> C[Human Oversight]
    B --> D[Automated Monitoring]
    D --> |Problem Detected| E[Immediate Action]
  ```

- **Safety of AI Systems**: Ensuring safety involves continuous evaluation and the ability to shut down harmful applications quickly.

  Example: Continuous deployment in training should be regulated to prevent harmful interactions.

> **Callout:** "But if deployment is regulated, including internal deployment, then no special additional regulation is required for the training process itself."