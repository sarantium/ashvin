{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL https://www.kraftful.com/robots.txt: HTTPSConnectionPool(host='www.kraftful.com', port=443): Max retries exceeded with url: /robots.txt (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)')))\n",
      "No nested indexes found. Adding URLs to list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Storage\\python_projects\\ashvin\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 36/36 [00:01<00:00, 19.15it/s]\n",
      "Generating embeddings: 100%|██████████| 386/386 [06:48<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This product provides AI tools for product managers and UX researchers to analyze user feedback and data, allowing them to gain insights and make informed decisions. It also helps in summarizing user feedback quickly and efficiently, reducing the time it takes to answer user queries.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "\n",
    "# Function to extract URLs from a sitemap\n",
    "def extract_urls_from_sitemap(sitemap_url):\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {sitemap_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if 'xml' not in response.headers.get('Content-Type', ''):\n",
    "        print(f\"Unsupported content type for URL {sitemap_url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "    urls = []\n",
    "\n",
    "    if soup.find('sitemapindex'):\n",
    "        sitemaps = soup.find_all('sitemap')\n",
    "        for sitemap in sitemaps:\n",
    "            sitemap_url = sitemap.find('loc').text.strip()\n",
    "            print(f'Found sitemap index: {sitemap_url}. Adding URLs to list')\n",
    "            urls.extend(extract_urls_from_sitemap(sitemap_url))\n",
    "    elif soup.find('urlset'):\n",
    "        print(f'No nested indexes found. Adding URLs to list')\n",
    "        locs = soup.find_all('loc')\n",
    "        urls = [loc.text.strip() for loc in locs]\n",
    "\n",
    "    return urls\n",
    "\n",
    "# Function to create a DataFrame from a list of URLs\n",
    "def create_dataframe(urls):\n",
    "    df = pd.DataFrame(urls, columns=['URL'])\n",
    "    return df\n",
    "\n",
    "# Function to get URLs from robots.txt or sitemap\n",
    "def get_urls_from_robots_or_sitemap(url):\n",
    "    if url.endswith('robots.txt'):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL {url}: {e}\")\n",
    "            return pd.DataFrame(columns=['URL'])\n",
    "\n",
    "        sitemap_urls = [line.split(': ')[1].strip() for line in response.text.split('\\n') if line.startswith('Sitemap:')]\n",
    "\n",
    "        all_urls = []\n",
    "        for sitemap_url in sitemap_urls:\n",
    "            all_urls.extend(extract_urls_from_sitemap(sitemap_url))\n",
    "\n",
    "        return create_dataframe(all_urls)\n",
    "\n",
    "    elif url.endswith('sitemap.xml'):\n",
    "        return create_dataframe(extract_urls_from_sitemap(url))\n",
    "\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['URL'])\n",
    "\n",
    "# Function to check and extract from a domain\n",
    "def check_and_extract_from_domain(url):\n",
    "    robots_url = url.rstrip('/') + '/robots.txt'\n",
    "    sitemap_url = url.rstrip('/') + '/sitemap.xml'\n",
    "    urls = []\n",
    "\n",
    "    robots_df = get_urls_from_robots_or_sitemap(robots_url)\n",
    "    if not robots_df.empty:\n",
    "        urls.extend(robots_df['URL'].tolist())\n",
    "\n",
    "    sitemap_urls = extract_urls_from_sitemap(sitemap_url)\n",
    "    if sitemap_urls:\n",
    "        urls.extend(sitemap_urls)\n",
    "\n",
    "    if not urls:\n",
    "        urls = [url]\n",
    "\n",
    "    return urls\n",
    "\n",
    "# Load API keys\n",
    "dotenv_path = Path(r\"C:\\Storage\\python_projects\\ashvin\\.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "GPT_MODEL = \"gpt-4-1106-preview\"\n",
    "\n",
    "# Example usage\n",
    "base_url = 'https://www.kraftful.com'\n",
    "list_of_urls = check_and_extract_from_domain(base_url)\n",
    "\n",
    "# RAG process\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
    "loader = SimpleWebPageReader()\n",
    "documents = loader.load_data(urls=list_of_urls)\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Example query\n",
    "response = query_engine.query(\"What does this product do?\")\n",
    "print(response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot provide a list of competitors for the product mentioned in the context information. My responses are based solely on the given context and not on any external knowledge or information.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_chat_engine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
