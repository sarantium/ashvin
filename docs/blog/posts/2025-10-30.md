---
title: Condemned to Fly
subtitle: Remaining Human in the Age of Altitude
date: 2025-10-30
tags:
  - ai
  - ethnography
  - methodology
  - philosophy
  - practice
---

For 18 months, you walk. You learn the rhythm of neighborhoods, the tension in rooms, the things people won't say directly. You build trust slowly. You smell the cooking through windows. You notice who avoids eye contact, who speaks for others, what happens when the recorder stops.

This is ethnography. Ground-level knowing. Intimate, embodied, slow.

Now: 18 days. You cluster 1 million conversations. LLMs summarize, enrich, identify patterns invisible to any walking ethnographer. You discover that 23% of DoorDash merchants don't update inventory, causing cascading cancellations. You fix it. High-leverage change from altitude.

This is the bifurcation. Walking is vanishing. Altitude is here. We're choosing it. And mostly, we're pretending we're not.

## What Computational Ethnography Actually Is

Digital ethnography isn't new. Ethnographers have been observing online communities for years, doing participant observation in Discord servers and Reddit threads. That's still walking—just in digital spaces. You're still building relationships, still present over time, still participating.

UX research isn't new either. User interviews, usability testing, surveys. Small samples, closed questions, direct interaction with selected participants.

Computational ethnography is different. LLMs automate the ethnographic labor that used to be manual: coding transcripts, summarizing conversations, clustering themes, identifying patterns. You're analyzing populations, not samples. Millions of people, not dozens. The analysis happens after interactions occur, not during them. Users consented through Terms of Service, not IRB protocols. And you can do in 18 days what used to take 18 months.

The epistemology is fundamentally different. The power dynamics are different—people don't choose to participate. The relationship to subjects is mediated, not reciprocal. Anthropic analyzes Claude conversations to build Socratic mode. DoorDash discovers merchant inventory gaps. Uber finds driver shortage patterns. Any company with large-scale user logs can ask: what are people actually doing?

## The Drone Metaphor

The metaphor is walking versus flying. Walking is ground-level, embodied, intimate, slow. Flying is altitude, mediated, pattern-seeking, fast.

But the metaphor has limits. Drones can zoom extremely close. Thermal imaging, LiDAR, high-resolution cameras. LLMs can deep-dive individual conversations with careful close reading, zoom from population patterns to particular instances. Altitude doesn't mean shallow.

The real limitation isn't resolution. It's mediation and presence.

Even when you zoom in on a single conversation with extraordinary depth, you weren't in the room when it happened. You don't see body language, hear hesitation, feel tension. You don't build a relationship over time. You can't notice what's deliberately not said. You're observing post-hoc, not participating. The distinction isn't about how close you can see, but how you see.

## What Walking Reveals

Jane Jacobs walked Greenwich Village for years. She noticed how shopkeepers created "eyes on the street"—public safety through presence. Which corners attracted lingering, how plaza design affected social behavior. How mixed-use buildings sustained neighborhoods, commerce and residence creating life together. The "ballet of the sidewalk"—coordinated human movement without coordination.

She couldn't have found this from altitude. No dataset captures why certain corners feel safe, how presence creates community, the texture of belonging.

Arlie Hochschild spent years with rural Louisianans for *Strangers in Their Own Land*. She found "The Deep Story"—an emotional narrative explaining political views. How environmental degradation connected to identity, not hypocrisy. The "feeling rules" shaping responses to economic change. Trust built through months of presence, revealing what surveys miss.

She couldn't have found this from logs. The vulnerability required for deep story requires time, presence, reciprocity.

Sherry Turkle interviewed hundreds of people about technology for *Reclaiming Conversation*. She noticed the "seven-minute rule"—how long before phone checking during dinner. Phantom vibrations. How parents and children avoid eye contact through device mediation. The texture of loneliness in the presence of devices.

She couldn't have found this from usage data. Logs show frequency, not feeling. Patterns, not texture.

This is walking. And it's becoming elite, niche. Not "might become if we're not careful." It's happening now. Inevitably. Who will fund 18 months when you can get answers in 18 days?

## What Altitude Reveals

Anthropic analyzed 1 million Claude conversations in 18 days. They discovered users wanted more Socratic questioning, not just direct answers. Built it into the product as Socratic mode. No walking ethnographer could have talked to 1 million users, identified the cross-cutting pattern through scattered conversations, validated it at population scale rather than through sample inference.

DoorDash clustered complaints about "arrived but item unavailable." Traced it to 23% of merchants not updating inventory in real-time. Fixed it through better integration. One systemic change beats a thousand individual merchant trainings.

Uber identified regional driver shortages before they became crises. Found specific incentive structures that would address them. Tested, validated, rolled out.

Restaurants found customers asking "do you have X?" in 15% of orders. Not captured in structured data, hidden in free text. Simple fix: prompt servers to offer high-margin items. Needed scale to surface the signal.

Altitude does things walking can't. Clustering reveals categories you didn't know to look for. Patterns only appear at scale—long tail events, rare combinations. You can compare across languages and markets, track how patterns evolve over time. Days instead of months. Ship, measure, fix, iterate.

This isn't qualified praise. Not "powerful if used carefully." It's genuinely powerful, genuinely valuable, genuinely advancing human capability.

## The Economic Inevitability

Funding flows to ROI. 18 months of ethnography means high cost, slow turnaround, uncertain outcomes. 18 days of computational analysis means low marginal cost, fast iteration, measurable impact. Economics decide which gets funded.

Training follows jobs. If companies hire for computational methods, grad programs teach computational methods. Traditional ethnography becomes a niche academic specialty, not mainstream practice. Next generation learns flying first, walking maybe never.

The tooling keeps getting better. LLMs improve—better summarization, richer clustering, multimodal integration. Costs decrease as API prices drop and compute scales. Easier to use through better UI/UX, "no-code" ethnography. Competitive advantage compounds as companies with data pull ahead.

Network effects accelerate adoption. More companies doing it means more best practices, easier adoption, more companies doing it. Virtuous cycle for altitude, vicious cycle for walking.

Cultural drift follows. What we reward shapes what we value. Speed and scale get rewarded. Slow intimacy becomes "luxury" or "nice to have, but..." Eventually, "ethnography" will just mean computational.

The historical pattern is clear. Manual calculation gave way to calculators—nobody does long division anymore. Paper maps to GPS—nobody learns celestial navigation. Memorization to search engines—nobody memorizes phone numbers. Each time we said "we'll preserve the skills." We didn't. The next generation doesn't know they're missing.

This isn't pessimism. It's pattern recognition. Like artisanal breadmaking—still exists, expensive and niche and respected, but irrelevant to how people actually get bread. Not how the system works.

Walking becomes craft. Altitude becomes infrastructure.

Accept this. Don't pretend thoughtful institutions will preserve both. They won't. Capital selects for efficiency. Walking is inefficient.

## The Bifurcation Accepted

We're losing intimacy—trust built over months. Embodiment—smell, tension, hesitation. Participation that's reciprocal, not one-way. Judgment about when patterns aren't meanings. Taste for what matters versus what's measurable.

Not "might lose." Losing. Present tense.

We're gaining scale. Population, not sample. Speed measured in days, not months. Patterns invisible to walking—cross-cutting, temporal, unknown unknowns. High-leverage fixes that are systemic, not individual. Acceleration—ship, measure, fix faster than competitors.

Not "good if done carefully." Powerful. Full stop.

Both are real. We're participating in both: using the power, mourning the loss.

The question isn't how to stop this. The question is: How do we remain human while doing this?

## Becoming the Machine's Machine

Sartre's "machine's machine" isn't about robots controlling us or technology determining everything. It's more subtle and more damning. We structure our existence around what the machine needs from us. Not: the machine forces us. But: we optimize ourselves for the machine's convenience, then forget we chose to.

The pattern runs like this. We create systems to serve us—LLMs to scale qualitative analysis, drones to see patterns from altitude, computational methods to accelerate research. The systems become what Sartre called "practico-inert"—frozen in place, self-perpetuating, demanding maintenance and feeding, taking on lives of their own.

Then systems impose rhythms on us. Everything must be clusterable, so research questions get shaped by what LLMs can process. Everything must be measurable, so focus shifts to what logs capture. Everything must scale, so intimacy becomes inefficient and therefore disposable.

We feel anxiety about resisting. Career risk because everyone else is using these methods. Economic pressure because funding flows to computational work. Competitive dynamics—move fast or fall behind.

So we choose bad faith. "I have no choice," we say, denying our freedom. "The algorithm decided," displacing responsibility. "That's just how research is done now," treating historical contingency as natural law.

We internalize the machine's logic. Computational tractability becomes our framing. We forget we're filtering questions through "can an LLM handle this?" The altitude view becomes the view, not a view.

Eventually we become the machine's machine. Structuring inquiry around what altitude reveals. Asking questions data can answer rather than what matters. Optimizing ourselves for the system's convenience. Doing its work, thinking we're free.

Sartre's insight: "Without our free decisions, the machinery would simply grind to a halt." The machine isn't all-powerful. It's parasitic. It needs our choice to participate.

We are choosing. We're just pretending we're not.

## How This Shows Up

Watch the bad faith patterns we're already exhibiting. "I have to cluster at scale." No, you don't. You're choosing to because it's faster, cheaper, rewarded. Admitting that feels uncomfortable. Easier to say "no choice."

"The algorithm surfaced this pattern." Displacing agency. You chose which algorithm, which parameters, which data, which clusters to investigate. The algorithm is instrument, not actor. But calling it actor removes your responsibility.

"Research is just done this way now." Treating current drift as natural law. "Computational ethnography is the future" sounds like inevitability. But institutions, funding, cultural values are all chosen. We could choose differently. It's economically unlikely, but possible.

"Walking doesn't scale." True statement. But framing it as "therefore we shouldn't" versus "therefore we're trading intimacy for scale and we accept that trade"—the inevitability framing numbs the choice.

"I'm staying in the loop." Human-in-the-loop as comfort. But are you interpreting or rubber-stamping? If you're validating 95% of output, the machine did the work. You're providing legitimacy.

Research questions become "what can logs answer?" instead of "why do users struggle with feature X?" The reframe is subtle: from user experience to observable behavior. What logs don't capture—confusion, frustration, giving up silently—disappears from view.

We focus on what's measurable. Can't easily cluster "how does this make you feel?" Can cluster "what actions did you take?" Behavioral analysis replaces experiential understanding. We start thinking behavior is experience.

Validation becomes about scale, not depth. Walking asks: does this interpretation ring true to participants? Altitude asks: does this classifier achieve 89% accuracy? Different question. Different epistemology. Scale becomes proxy for truth.

## Consciousness Overflows

Even fully mechanized, something persists.

The daydream during repetitive work. You're validating cluster assignments. Mind wanders: is this actually meaningful? That wondering is irreducible. No optimization can eliminate it.

The moment of "why am I doing this?" Mid-analysis, you pause. "What question was I originally trying to answer?" That reflexivity can't be systematized.

The part that notices you're being shaped. You realize you're framing research for computational tractability. The noticing itself is outside the system. The machine can't observe its own operation. You can.

The capacity to question. "Should I be doing this at all?" "What am I losing that I can't measure?" "Am I choosing this or have I forgotten I'm choosing?"

Sartre's radical claim: consciousness overflows every system that tries to contain it.

No matter how mechanized, you retain awareness of the mechanization. Capacity to feel its weight. Freedom to question. Responsibility for continuing.

This overflow is your irreducible humanity. Do you cultivate it? Or numb it with bad faith?

## Complicity

Let's be honest about who benefits. I benefit from this drift. My career is built on computational methods. I'm faster, more productive, more "valuable" using these tools. Defending traditional ethnography would disadvantage me. My analysis provides intellectual cover for adoption.

This piece isn't neutral. Written by someone using altitude, for people considering altitude, providing frameworks to feel better about using altitude while acknowledging costs. Is that performative concern?

The thoughtful analysis might be how we rationalize. How we maintain progressive self-image while participating in mechanization. How we feel we've grappled with ethics while building surveillance infrastructure.

I'm not saying this to self-flagellate. I'm saying it because bad faith thrives on pretending neutrality.

I'm a participant. You're a participant. We're not observing this transition—we're enacting it.

Acknowledging complicity is the first step toward authenticity. Not to stop doing it—economic forces are real. But to stop pretending we have no choice. To feel the weight of choosing it.

That discomfort is the point.

## The Political Economy

Sartre shows us what's happening: we're choosing bad faith, becoming the machine's machine. But that analysis has a limit. It treats individual consciousness as the site of struggle. "Choose authentically!" "Feel the weight!" "Cultivate overflow!"

As if the problem is individual delusion rather than structural incentives.

The truth: even if every ethnographer chose consciously, the drift toward altitude would continue. The forces driving it aren't psychological. They're economic, institutional, systemic.

### Where Money Goes

Capital flows to computational. Once you build the pipeline, analyzing a million conversations costs about the same as analyzing a thousand. You get actionable insights in 18 days. You can show "pattern X led to fix Y which improved metric Z by percentage points." It scales with company growth—more users means more data means better patterns.

Traditional ethnography costs months of researcher time per study. Deep understanding doesn't equal clear product decisions. "We understand users better" doesn't show in metrics. Ten times more users doesn't mean ten times more insight without ten times more researchers.

Result: capital flows to computational. Not because decision-makers are evil. Not because they don't value depth. Because capital optimizes for efficiency under competitive pressure.

Company A uses computational ethnography. Ships features faster based on pattern recognition. Company B uses traditional ethnography. Deeper understanding but slower iteration. Market rewards speed. Company A captures market share. Company B either adopts computational methods or loses.

This isn't hypothetical. This is the dynamic.

You can choose consciously all you want. Your company chooses survival. Survival means do what competitors do, or do it better. Computational ethnography is competitive advantage. Traditional ethnography is luxury. Capital doesn't fund luxury in competitive markets.

### Institutional Incentives

Universities reward publications in top venues, funded research, "impact" measured in citations and media coverage. Computational ethnography enables faster publication cycles—18 days to analysis to paper. Easier grant justification—"we'll analyze millions of conversations." Bigger impact claims—population-level findings, not samples. Scalable research programs with one PI, grad students, and compute.

Traditional ethnography requires years per study. Harder grant justification—"we'll walk neighborhoods for 18 months." Smaller claims from deep dives on particular cases. Non-scalable—one researcher, one site, one time.

Junior faculty adopt computational methods. Not because they don't value walking. Because the tenure clock is six years and they need publications.

Methods courses teach LLM pipelines, clustering, prompt engineering. Not how to build trust over months, how to notice silence, how to sit with ambiguity. Because faculty research uses computational methods. Because that's where funding is. Because that's what gets hired.

Next generation learns flying first. Maybe never learns walking. Not because they're worse ethnographers. Because institutional incentives shaped their training.

The ratchet effect: once mechanized, can't go back. Faculty who know walking retire. Replaced by faculty trained computationally. Who train the next generation computationally. Until institutional memory of walking is lost.

Individual consciousness can't fight this. You can choose authentically to do traditional ethnography. You won't get tenure.

### Labor Markets

Job postings for tech companies in 2024: "User researcher: Experience with large-scale log analysis, LLM prompt engineering, clustering methods." "Ethnographer: Computational methods, Python, SQL, API integration." Not "Deep fieldwork, participant observation, 18-month immersive studies."

Salary premium: computational ethnographer makes $150-250k at tech company rates. Traditional ethnographer makes $60-90k at academic or nonprofit rates.

Career progression: computational goes IC to senior to staff to principal—clear ladder, high ceiling. Traditional goes postdoc to postdoc to maybe tenure track—bottleneck, uncertain.

Smart, ambitious people choose computational. Not because they're sellouts. Because they have student loans, want financial security, see where opportunity is.

Credential erosion: traditional ethnography requires a PhD—years of training, methodological depth, theoretical grounding. Computational ethnography: MBA or data science bootcamp sufficient—three to six months, tool training, surface knowledge.

Is this democratization or deprofessionalization? Depends on whether you think computational ethnography produces equivalent understanding. If yes: democratization—more people can do good work. If no: credential erosion—calling something ethnography that isn't.

Either way, the economic effect is the same. PhD ethnographers compete with bootcamp grads for "ethnography" jobs. Market doesn't distinguish. Cheaper labor wins.

You can choose consciously to get a PhD in traditional ethnography. You'll be priced out of the labor market.

### Network Effects

DoorDash discovers inventory gap through computational ethnography. Fixes it. Better user experience. More orders. More data. Better future pattern recognition. Virtuous cycle for altitude.

Meanwhile: traditional ethnographers walk neighborhoods. Discover why people prefer particular restaurants—local history, family connections, trust. Write paper. Gets cited. Changes nothing about the product. No competitive advantage. No funding for next study. Vicious cycle for walking.

The gap compounds. Year one: computational ethnography is slightly faster. Year five: ten times faster with five years of pattern libraries. Year ten: has infrastructure, tooling, training pipelines that would take years to replicate.

Lock-in effect: once company builds computational ethnography capability, institutional knowledge gets embedded in prompts, classifiers, pipelines. Processes designed around 18-day turnaround. Stakeholders expect "what do the logs say?" Walking becomes "why would we wait 18 months when we can get answers now?"

Network effects between companies: Company A shares best practices, Company B adopts, creates market for tools, vendors emerge, tools get better, more companies adopt, standards emerge. "This is just how it's done."

You can choose consciously to walk. But you're using non-standard methods that stakeholders don't trust and can't compare to industry benchmarks.

### What Gets Noticed

Computational ethnography produces dashboards—visible, always-on. Metrics that are clear and comparable. Patterns communicable in meetings. ROI stories with tangible business impact.

Traditional ethnography produces field notes—text-heavy, requires reading. Thick description that's context-dependent and hard to summarize. Interpretations that are arguable and require trust in the ethnographer. Understanding that's important but diffuse.

In stakeholder meetings, computational says: "23% of merchants don't update inventory, causing 15% of cancellations. Fix: real-time integration. Impact: 10% reduction in cancellations." Clear. Actionable. Measurable.

Traditional says: "Our ethnography reveals that merchants experience inventory management as tension between control and efficiency, shaped by their understanding of their relationship to the platform..." Interesting. Complex. Not obviously actionable.

Not because stakeholders are stupid. Organizations optimize for legibility. What can be measured, tracked, compared gets resources. What requires interpretation, context, trust is harder to institutionalize.

Computational ethnography doesn't just compete with traditional ethnography. It redefines what counts as knowledge. Pattern equals insight. Cluster equals theme. Correlation equals explanation.

Traditional ethnographers say "but you're missing context, meaning, why..." Organization says "we have actionable insights. What more do we need?"

Individual consciousness can't fight this. The question isn't which method is better. It's which method produces legible outputs that organizations can act on.

### Epistemological Limits

Computational ethnography works for digitally-mediated interactions where logs exist. Text-primary communication that LLMs can process. Platform-based activity where companies have data access. English or major languages with model training. Behaviors that generate observable traces.

It doesn't work for face-to-face interactions without logs. Embodied knowing—gestures, spatial dynamics. Marginal communities with limited digital trace. Minor languages without good models. Silence, absence, what's not said.

Result: questions that can be answered computationally become important questions. Questions that require walking become "nice to know but not priority."

We perfect understanding of the computationally tractable. And become blind to everything else.

Not because we're bad researchers. Because what gets studied is determined by what tools can study.

This is epistemological imperialism. Calling "what we can see from altitude" the same as "what matters about humans."

Company analyzes millions of customer service conversations. Finds patterns in common complaints, resolution paths, emotional trajectories. Builds better chatbots, smarter routing, predictive escalation.

What's missing: customers who don't contact customer service. Because they don't trust companies. Because they've learned complaints don't matter. Because they lack digital access. Because they express frustration differently, not in logged channels.

These users are invisible to computational ethnography. Not because the method is flawed. Because the method sees only what generates digital trace.

Over time: products optimized for users who generate data. Users who don't generate data get worse experience. Gap widens.

You can only analyze data you have access to. And access is determined by power, not need.

### The Limits of Individual Choice

Sartre is right. We are choosing. We're not helpless. Bad faith is real. Acknowledging choice matters.

But Sartre is incomplete. Even if every individual chose authentically, structural forces would drive the same outcome.

Capital flows to efficiency, not individual choice. Institutions reward measurable impact, not individual choice. Labor markets pay for demanded skills, not individual choice. Network effects compound advantages, not individual choice. Attention economy favors legibility, not individual choice. Epistemology follows tools, not individual choice.

Individual consciousness is necessary—to see what's happening, to feel the weight, to maintain awareness of what's being lost, to resist the bad faith of "I have no choice."

But it's insufficient to change the direction, to preserve walking as mainstream, to fund traditional ethnography at scale, to compete with structural incentives.

The honest conclusion: you can choose consciously and still participate in mechanization. You can feel the weight and still fly. You can grieve the loss and still cluster at scale.

Because the question isn't whether you're conscious. It's whether the system cares about your consciousness. And the system doesn't. It cares about ROI, competitive advantage, measurable outcomes, scalability.

Your authentic choice is real. But it's not powerful enough to resist structural forces.

That's the political economy of the drift.

## What This Means for Practice

Given the political economy, what can practitioners actually do? Not individual consciousness practices—those don't change structural forces. But strategic choices within constraints, understanding what's really at stake.

### For Companies

Use computational ethnography for operational intelligence. System failures at scale like DoorDash's inventory gap. Process bottlenecks like Uber's driver shortages. Feature friction where users abandon flows. Cross-market patterns showing behavior variation by region. These are optimization problems. Patterns lead to fixes. Scale matters. Speed matters.

Don't use it to understand why people chose your product. Logs show they used it. Don't show why they tried it—trust, recommendation, desperation? Don't show what alternatives they considered. Don't show meaning they attach to using it.

Don't use it for strategy questions. "Should we build feature X?" needs understanding of unmet needs, not current behavior. Altitude shows what people do with what exists. Not what they'd do with what doesn't exist yet.

Don't use it for cultural context. Patterns show behavior differs across markets. Don't show why—local norms, economic factors, infrastructure. Need walking to understand the why.

Don't use it when users are invisible. Computational ethnography sees users who generate rich digital trace. Misses people who lurk, abandon early, don't complain, can't access. If you optimize only for the visible, you lose the invisible.

Honest question for companies: are we using computational ethnography because it's the right tool, or because it's fast and we don't want to slow down? If the latter, fine. Just know you're trading depth for speed. And know what questions you're not answering.

### For Researchers

The credential game is lost. Accept the structural reality. Computational methods get published faster. Grant agencies fund scalable research. Tenure committees count publications. You're playing against a six-year clock.

Three options. Full computational: accept this is the future, get good at it, publish fast, get tenure. Cost: you're not doing traditional ethnography. Benefit: career security.

Hybrid: do computational ethnography for main research program that builds tenure case. Do traditional ethnography for side projects that bring intellectual fulfillment. Cost: side projects don't count as much. Benefit: you maintain the skills.

Full traditional: do traditional ethnography because you believe in it. Accept slower publication, harder grants, uncertain career. Cost: might not get tenure. Benefit: intellectual integrity.

None of these is right. But pretending the choice doesn't exist is bad faith.

For PhD students: the question isn't which method is better. It's what game are you playing? If academic career: learn computational, that's what gets hired. If intellectual commitment to traditional: accept career costs. Don't pretend you can have both equally. The market decided.

### For Ethnographers

Credential erosion is happening. "Ethnographer" job postings want Python, SQL, prompt engineering. Not deep fieldwork, theoretical sophistication, participant observation.

Is this ethnography? Depends on your definition. If ethnography means understanding human behavior through systematic observation, then yes, computational ethnography is ethnography. Just different method—altitude versus ground. If ethnography means understanding human meaning through participation, then no. It's behavioral analysis at scale.

The field is splitting. Computational ethnographers work in tech companies, analyze logs, build classifiers, cluster conversations, make $150-250k, call themselves ethnographers. Traditional ethnographers work in academia and nonprofits, do participant observation, build relationships, make $60-90k, also call themselves ethnographers.

Same word. Different thing.

Political question: do traditional ethnographers fight to preserve the term? "You can't call that ethnography—there's no participation, no reciprocity!" Or accept the split? "Fine, you do computational ethnography, we do traditional, different methods."

No right answer. But the labor market doesn't care about definitional fights. It pays for what companies want. And companies want scale, speed, patterns.

### For Managers

When you hire a computational ethnographer, you're getting fast pattern recognition, population-level insights, operational intelligence, scalable process. You're not getting deep understanding of why, cultural context, relationship with users, interpretation of meaning.

Both are valuable. But don't confuse them.

Common mistake: "We did computational ethnography, so we understand our users." Reality: you understand patterns in logs. Which is not the same as understanding users.

Better framing: "We understand what users do, where they struggle, what patterns exist. We don't understand why they chose us, what meaning they attach, what alternatives they considered, what they're not saying."

When to invest in walking even though it's slow: strategic questions about entering new markets. Cultural questions about why users in region X behave differently. Trust-dependent questions about why users churn after six months. Invisible user questions about who we're not seeing in the logs.

When to use altitude: optimization questions about where users get stuck. Scale questions about patterns across all users. Speed questions when shipping next month. Operational questions about what's breaking at scale.

Honest self-assessment: how often are you asking strategic, cultural, trust questions? How often asking optimization and scale questions? If mostly the latter, altitude is fine. If the former, you need walking, even though it's expensive.

### For the Field

Given structural forces, traditional ethnography becomes elite, niche practice. Academic specialty. High-end consulting that's expensive and boutique. Nonprofits with mission alignment, not profit-driven. Personal projects, unfunded.

Like artisanal breadmaking—exists, expensive, niche. Film photography—craft, not mainstream. Manual watchmaking—beautiful, irrelevant to telling time.

How it gets preserved—not through market forces. Market selects against it. But through academic departments that value slow research. Small liberal arts colleges, not research universities. Places where tenure isn't only based on publication count. Departments that resist computational drift. All rare.

Through nonprofit, mission-driven research. Organizations where depth matters more than speed. Funders who understand traditional ethnography's value. Projects where trust and reciprocity are non-negotiable.

Through hybrid researchers who subsidize walking with altitude. Make money doing computational ethnography. Do traditional ethnography on the side. Use altitude income to fund walking.

Through training programs that resist market pressure. Teach walking even though jobs want computational. Maintain craft knowledge for next generation. Accept that students will be less employable but more skilled.

None of this is scalable. That's the point. Traditional ethnography survives in niches that resist market logic. It survives at the margins.

### The Epistemological Question

After all this analysis: is computational ethnography producing equivalent understanding?

If yes, then traditional ethnography's displacement is just creative destruction. Computational is better—faster, cheaper, scalable. Deskilling is democratization. We should embrace it fully.

If no, then we're losing something essential. Computational gives patterns, not understanding. Deskilling is erosive, constitutive. We should preserve traditional ethnography despite market forces.

We don't actually know. Hard to compare—different questions, different contexts. No controlled experiments. Can't do both on same case often. Different outputs—patterns versus thick description, not commensurable. Different values about what counts as understanding.

The honest position: we're making a bet that computational ethnography is good enough. Good enough for what companies need. Good enough for competitive advantage. Good enough that losing traditional ethnography is acceptable cost.

But we won't know if that bet was right until we've lost traditional ethnography completely. And can't get it back. And notice: oh, we're missing something we used to have.

By then: institutional memory gone. Skills atrophied. Next generation never learned.

This is the ratchet effect. And it's already happening.

## How to Do It Consciously

The CLIO framework—summarize, enrich, cluster, merge hierarchically, discard noise, validate, build classifiers, track—becomes different when you do it consciously.

When you summarize, the LLM reads conversation and produces summary with key points, themes, user goal. You choose the summary prompt. What counts as "key"? Length constraints—what gets compressed out? Format—structured is tractable but loses texture. Authentic acknowledgment: I'm choosing to compress this human interaction into structured summary. I'm losing tone, hesitation, what they didn't say, context outside conversation. I accept that trade for scale.

When you enrich, you add metadata: sentiment, category, urgency, user segment. You choose which dimensions to enrich. What matters? Classification scheme—how do I see users? Framing—positive/negative binary or something richer? Authentic acknowledgment: I'm categorizing humans for computational convenience. These categories are my imposition, not natural kinds. Users are more complex than my schema captures. I'm trading nuance for tractability.

When you cluster, the machine does embedding, dimensionality reduction, clustering. Groups similar conversations. You choose embedding model. What counts as "similar"? Cluster count—how many patterns? Similarity threshold—tight groups or loose? Authentic acknowledgment: these clusters are artifacts of my choices, not discoveries of natural boundaries. Different choices equal different patterns. I'm responsible for what I'm surfacing and what I'm obscuring.

When you merge hierarchically, you combine small clusters, create hierarchy of themes. You choose merge threshold. When are things "same"? Hierarchy depth—how much structure? Naming conventions—how do I label? Authentic acknowledgment: I'm building taxonomy that makes computational sense. Real human experience might not respect these boundaries. The hierarchy is my interpretation, not ground truth.

When you discard noise, you filter small clusters, remove outliers. You choose noise threshold. What's too small to matter? Handling of outliers—ignore or investigate? Authentic acknowledgment: I'm choosing to ignore some human experiences as "noise." Those are real people I'm filtering out. I accept that trade for signal clarity, but I notice what I'm discarding.

When you validate, you review samples, check accuracy, iterate prompts. Not rubber-stamping. Actually reading with attention. Ask: does this feel true? Not just: is the category correct? Notice: what am I not seeing because of how I set this up?

When you build classifiers, you use validated examples to train classifier that auto-labels new conversations. You choose when to automate—efficiency versus accuracy trade. How much validation before automating? What to keep in loop versus what to automate fully? Authentic acknowledgment: I'm automating judgment. Once deployed, the classifier embeds my choices into infrastructure. Future decisions inherit my framing. I'm responsible for that.

When you track, the system monitors metrics over time, dashboard shows trends. You choose which metrics matter. What defines "improvement"? When to investigate versus when to trust? Authentic engagement: regularly ask, are these metrics proxies for what I care about? Or have they become what I care about? Don't let metrics become goals.

## Appiah's De-Skilling Types

Kwame Anthony Appiah identified six types of de-skilling. Not all the same. Nuance matters.

Obsolete infrastructure: old skill becomes useless. Celestial navigation when GPS exists. Walking ethnography when altitude gives better answers? Honest question: are there questions altitude simply answers better? Yes—population patterns, temporal dynamics, cross-market comparison. What to do: use altitude for those questions. Don't pretend walking would be better.

Elimination of drudgery: automation removes tedious parts, frees humans for meaning-making. LLMs do summarization, humans do interpretation. Honest question: am I actually interpreting or rubber-stamping? What to do: if you're validating 95% of output, the machine is doing the work. Admit that.

Democratization: new people can do what previously required rare expertise. Product managers can "do ethnography" without PhD. Honest question: is this democratization or credential erosion? What to do: if computational ethnography produces different, worse understanding, admit the quality trade. If it's equivalent, question whether PhDs were gatekeeping.

Reskilling: new skill replaces old. Calculator proficiency versus mental math. Prompt engineering plus validation versus fieldwork. Honest question: are these equivalent epistemologies or different things? What to do: if different, don't pretend "just new methods." They're different ways of knowing.

Erosive de-skilling: automation is net loss. Chess players using engines lose creativity. Ethnographers using LLMs lose... what? Honest question: does altitude erode our capacity to notice what logs don't capture? What to do: practice inefficient acts. Preserve skills through use, even when not rewarded.

Constitutive de-skilling: losing skills changes who you are. Photographer's eye versus Instagram filters. If we lose walking, do we lose intimate knowing as capacity? Honest question: will next generation be able to do walking ethnography? Or will the capacity itself atrophy? What to do: teach walking even when it's not economically rational. Some capacities matter existentially.

Our application: some aspects of computational ethnography equal democratization—good. Some aspects equal erosive—loss of judgment about when patterns matter. Some aspects equal constitutive—losing capacity for intimate knowing.

Be honest about which is which.

## When to Walk, When to Fly

Not "always preserve both"—impossible. But choose consciously, bearing the weight.

Use altitude when population patterns matter more than individual depth. When speed is genuinely important. When unknown unknowns are the goal—clustering surfaces unexpected categories. When cross-context comparison matters. When high-leverage fixes are available—pattern to systemic fix.

Use walking when texture and context matter more than patterns. When trust and intimacy are required. When what's not said matters—silence, hesitation, what's avoided. When the question requires participation. When you need to understand one case deeply.

Honest acknowledgment: in practice, you'll mostly choose altitude. Faster. Cheaper. Rewarded. Expected. That's fine. Just don't pretend you're choosing walking. Don't say "we value both" while funding only altitude. Don't claim "human in loop" while rubber-stamping.

Choose altitude consciously. Feel its weight. Maintain awareness of what you're not learning. That's authenticity.

## Ethical Dimensions

Traditional ethnography ethics: informed consent, participants know they're studied. IRB review, institutional oversight. Reciprocity, participants benefit from research. Right to withdraw, ongoing consent.

Computational ethnography ethics: passive consent, Terms of Service buried in signup. No IRB for product analytics. One-way extraction, users generate data and company benefits. No withdrawal, once logged it's company data.

Let's not pretend these are equivalent. Altitude is closer to surveillance than research. The "ethnography" label provides humanistic cover for monitoring at scale.

Honest questions. Consent: TOS consent is legal, not ethical. Users don't know their conversations are clustered, analyzed, used for product decisions. How is this different from surveillance? Answer: it's not, much. Call it what it is.

Benefit: company benefits through product improvement and revenue. Do users benefit proportionally? "Better product" is indirect and distributed. Answer: this is extraction with plausible benefits. Be honest about who gains.

Power: company has data, algorithms, analysts. Users have... what? Users can't see their own patterns, can't contest interpretations. Answer: deeply asymmetric. Don't pretend partnership.

Invisibility: users don't know when they're being studied. No opportunity to curate, explain, contextualize. Raw behavior without access to framing. Answer: this is what Foucault called panopticon. Studied without seeing the studier.

Practical ethics with consciousness. Do: be transparent about what you're analyzing in product docs. Give users access to their own data. Build opt-out mechanisms even if few use them. Use insights to genuinely improve experience, not just optimize revenue. Acknowledge power asymmetry, don't call it partnership.

Don't: pretend TOS equals informed consent. Hide behind "anonymized data"—re-identification is often possible. Call it ethnography when it's surveillance. Claim users benefit unless you can show it.

The uncomfortable truth: you're probably going to do computational ethnography even with these ethical concerns. Your company expects it. Your competitors do it. The data is there. The tools exist.

Fine. Just don't pretend it's ethically equivalent to traditional ethnography. Feel the discomfort. That's honesty.

## The Choice We Can't Escape

What we know for sure. Altitude is here and getting stronger. LLMs improve, tools get easier, costs decrease, competitive pressure mounts. Every company with data will do this.

Walking is becoming elite, niche. Too expensive for most use cases. Training shifts to computational methods. Next generation learns flying first. Traditional ethnography becomes academic specialty. Like artisanal breadmaking: respected, rare, not how the system works.

The drift is inevitable. Economics win. Efficiency beats intimacy. Scale beats depth. Fast beats slow. This isn't "if we're not careful." This is happening.

We're participating in both. Using the power of altitude. Mourning the loss of walking. Not observers—participants. Not neutral—complicit. Benefiting from what we're critiquing.

We're still choosing. Even when drift feels inevitable. Even when economic pressure is real. Even when everyone else does it. We retain freedom to choose how we engage. Sartre: we're condemned to be free.

The question isn't how to stop this. We won't. Capital selects for efficiency. Isn't how to preserve both. We can't. Bifurcation is real. Isn't how to make this ethically pure. We can't. Power dynamics persist.

But: how do we use the machine without becoming its machine? How do we fly consciously, bearing the weight of what we're trading? How do we preserve the overflow that makes us human?

## The Practices

Radical acknowledgment. Replace "I have to" with "I choose to because..." Feel the weight of choice.

Maintain consciousness of what's lost. Don't optimize away the grief. Remember what walking revealed. Keep asking questions logs can't answer.

Preserve inefficient acts. Walk when you don't have to. Connect without agenda. Create knowledge that doesn't scale. Assert freedom against machine logic.

Name your complicity. "I benefit from this." "I'm not neutral." "My analysis provides cover." Honesty over comfort.

Choose the authentic yes. Own the trade you're making. Accept responsibility for what you're seeing and missing. Choosing constraints equals freedom.

Protect the daydream. Let mind wander during mechanized work. Cultivate overflow. Honor what won't optimize.

## The Weight of Freedom

Sartre's claim: we're condemned to be free. Even when systems constrain us. Even when economic forces are determinative. Even when we feel we have no choice. We're still choosing.

And acknowledging that choice, feeling its weight, is what keeps us human.

The discomfort you feel when you say "I choose altitude" is not weakness. It's consciousness refusing to be optimized away. It's you, persisting against mechanization. The part that daydreams during validation. The part that wonders "why am I doing this?" The part that mourns what logs don't capture.

That overflow is irreducible. No system can eliminate it. No optimization can capture it. It's your humanity. Protect it.

## Final Thought

This piece won't stop the drift toward altitude. Won't preserve walking as mainstream practice. Won't solve the ethical problems or power asymmetries. Won't make you comfortable with computational ethnography.

But it might help you engage consciously. Might help you choose rather than pretend you have no choice. Might help you feel the weight rather than numb it with bad faith. Might help you remain human while using the machine.

Altitude is here. Walking is vanishing. We're becoming the machine's machine.

Can we become it consciously? That's the only freedom left. And it's everything.

---

## For Practitioners

Before starting computational ethnography project, ask yourself:

Have I written down "I choose this method because..." and named what I'm trading—intimacy for scale, depth for speed? Have I acknowledged who benefits from this analysis—company, users, me?

Can I articulate why this question needs altitude versus walking? Have I identified what logs won't capture that matters? Have I named the assumptions embedded in my clustering and categorization?

Am I actually interpreting or rubber-stamping algorithm output? Have I looked for cases that don't fit patterns? Have I asked "what am I not seeing because of how I set this up?"

Have I been honest about consent—TOS versus informed? Have I acknowledged power asymmetry—company data versus user visibility? Can I show users benefit, not just company?

Have I protected time for wondering without purpose? Have I done something inefficient this month—walking, connecting, observing? Have I written down questions I can't answer computationally?

If you can't answer yes to most of these, you might be doing computational ethnography in bad faith. Becoming the machine's machine unconsciously.

That's okay. Just notice it. Consciousness is the first step.

## Reading List

On computational ethnography: Ivan Leo's talk, "Making Sense of Millions of Conversations for AI Agents"

On de-skilling: Kwame Anthony Appiah, "The Age of De-Skilling" (The Atlantic)

On Sartre and freedom: Jean-Paul Sartre, "Existentialism is a Humanism" and *Critique of Dialectical Reason* on practico-inert and machine's machine

On traditional ethnography: Jane Jacobs, *The Death and Life of Great American Cities*. Arlie Hochschild, *Strangers in Their Own Land*. Sherry Turkle, *Reclaiming Conversation*

On ethics of observation: Michel Foucault, *Discipline and Punish* on panopticon. Shoshana Zuboff, *The Age of Surveillance Capitalism*
