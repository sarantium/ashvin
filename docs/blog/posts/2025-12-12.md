---
title: "On Slop"
date: 2025-12-12
tags:
  - artificial-intelligence
  - epistemology
  - media-criticism
---

The word arrived with the force of revelation. *Slop*. Scrolling through feeds thick with AI-generated images, articles, videos—content that felt somehow wrong, uncanny, excessive—we finally had a name for the unease. The term spread because it captured something visceral: the texture of language that reads smoothly but says nothing, images that resolve into coherence without ever achieving meaning, an endless tide of *stuff* that nobody asked for and nobody quite wanted.

The complaint is now ubiquitous. Graphite's analysis of web content found that over half of new articles are now AI-generated. Bynder's research shows a majority of consumers report reduced engagement when they suspect content is machine-made. The Macquarie Dictionary named "AI slop" its 2025 word of the year. The diagnosis seems clear: AI is flooding our information environment with garbage, and the flood is drowning authentic human expression.

But what if the diagnosis is wrong? Not factually wrong (the volume is real, the uncanniness genuine) but *conceptually* wrong. What if "slop" is a category error that mistakes volume for vice, aesthetics for ethics, and origin for orientation? The question isn't whether AI produces garbage. It does, in abundance. The question is whether that's the right thing to be worried about.

<!-- more -->

---

## Frankfurt on Bullshit

To understand what the slop complaint actually tracks, we need a philosopher who spent considerable effort on a related phenomenon. In 1986, Harry Frankfurt published a short essay called "On Bullshit" that became, improbably, a bestseller when reissued as a book two decades later. Frankfurt's project was conceptual excavation: what exactly *is* bullshit, and why does it differ from lying?

His answer was precise and unsettling. The liar knows the truth and deliberately contradicts it. The bullshitter doesn't care about the truth one way or another. This seems like a minor distinction; both are forms of deception. But Frankfurt argued it was fundamental. The liar operates within the same game as the truth-teller; they're just on opposite teams. The liar must understand reality to distort it. The bullshitter abandons the game entirely.

This is why Frankfurt concluded, counterintuitively, that bullshit is more corrosive than lying. Liars still respect truth enough to work around it. Bullshitters erode the very capacity to distinguish true from false because they've stopped caring about the distinction. Someone who lies habitually can still recognize truth; someone who bullshits habitually gradually loses this ability.

Now apply this framework to AI-generated content. The slop complaint treats the problem as one of *quality*—AI output is cheap, repetitive, uncanny, excessive. But Frankfurt's framework suggests a different question entirely: what relationship to truth do the humans deploying these systems maintain?

An AI system generating text has no relationship to truth whatsoever. It produces token sequences based on statistical patterns. It cannot bullshit in Frankfurt's sense because bullshitting requires the capacity to care about truth and the choice not to. The AI isn't indifferent to truth; it has no concept of truth to be indifferent toward.

But the humans deploying AI systems—they can bullshit. And this is where the framework bites. Someone using AI to generate content they'll publish without reading it, to fill pages they don't care about, to create volume without concern for what's true or useful—that person is a bullshitter. Someone using AI to explore ideas, draft arguments they'll refine, or scale work they would stand behind—that person is doing something else entirely.

The slop complaint conflates these. It treats AI output as intrinsically problematic when the actual variable is human orientation toward truth.

---

## Kahneman on Noise

Consider now a different uncomfortable observation, this one from Daniel Kahneman. Speaking at an NBER conference in 2017, Kahneman offered remarks that should unsettle anyone concerned about AI-generated content:

"We have in our heads a wonderful computer. It is made of meat, but it's a computer. It's extremely noisy, but it does parallel processing. It is extraordinarily efficient, but there is no magic there."

Kahneman's career was built on demonstrating how unreliable human judgment actually is. Not occasionally unreliable, not unreliable at the margins, but pervasively, structurally unreliable. Show people the same stimulus twice and they give different responses. Ask experts to make predictions and you'll find that a simple statistical model—trained not to predict outcomes but merely to predict what the expert would say—outperforms the expert themselves.

Why? Because the model strips out the noise. Human judgment contains signal plus substantial random variation. A model that learns to extract the signal and ignore the noise will be more accurate than the noisy original, even though it has no understanding of the underlying domain.

"Most of the errors that people make are better viewed as random noise, and there is an awful lot of it," Kahneman observed. The implication was direct and uncomfortable: "You should replace humans by algorithms whenever possible."

If we take Kahneman seriously, a strange conclusion emerges. Humans are prolific producers of slop. We call it other things (opinions, takes, content, thought leadership) but by any honest standard, much of what humans produce is low-quality, repetitive, uncanny in its own way (how many LinkedIn posts follow the same template?), and generated without particular care for truth or value. The difference is that we extend to human-generated content a dignity we withhold from AI-generated content, not because the human content is better but because it's *ours*.

I'm not defending AI slop. I'm observing an inconsistency. If the objection to AI content is that much of it is garbage produced without care for truth or the reader, that objection applies with equal force to enormous quantities of human-generated content. We've always lived in slop. We just called it the media environment.

---

## A Better Taxonomy

So what categories would actually be useful? If the origin of content (human vs. AI) isn't the relevant variable, and if quality alone doesn't capture what's corrosive, what would a better taxonomy look like?

Frankfurt's framework suggests the answer lies in the operator's orientation toward truth. Four types emerge:

**Truth-seeking use.** Someone uses AI as a tool in service of understanding or communication they care about. They review the output, refine it, stand behind the result. The AI is a lever, not a replacement for judgment. This isn't slop; it's augmented work.

**Truth-indifferent use.** Someone generates content to fill space, meet quotas, game algorithms—without concern for whether the output is true, useful, or good. This is bullshit in Frankfurt's precise sense, and it's corrosive regardless of whether AI produces the tokens or a bored content-farm writer does. The problem predates AI; AI merely scales it.

**Truth-adjacent use.** Someone produces content that isn't *about* truth in any straightforward sense: entertainment, aesthetic exploration, play. Much creative work lives here. Judging it by epistemic standards misses the point; it operates under different rules. An AI-generated image that delights isn't slop just because no human held a brush.

**Truth-corrosive use.** Someone deploys AI specifically to flood information environments, exhaust attention, or generate plausible-seeming misinformation at scale. This is worse than ordinary bullshit because it's strategic—not mere indifference to truth but active weaponization of volume to degrade collective sense-making. This is the genuine danger, and it's a danger because of human intent, not because of AI capability.

The slop complaint tends to conflate all four categories, treating them as instances of a single problem called "AI-generated content." But the categories have almost nothing in common. Truth-seeking use should be celebrated. Truth-adjacent use should be evaluated by its own appropriate standards. Truth-indifferent use is a problem that long predates AI. Only truth-corrosive use represents something qualitatively new and dangerous, and it's dangerous precisely because of the human intentions behind it.

---

## The Real Concerns

The unease is still warranted. Something real has shifted. The marginal cost of content production has collapsed, and volume is flooding channels built for scarcity. Discovery is breaking down; attention is fragmenting; the sheer mass of generated content may be crowding out human expression that requires sustained attention to appreciate. These are legitimate concerns.

But they're concerns about *economics and attention*, not about the metaphysics of human vs. machine production. A world with too much content (even too much good content) poses coordination problems. Those problems deserve analysis rather than moral panic.

The deeper worry embedded in the slop complaint may be existential rather than epistemic. If AI can produce text and images that pass for human, what's distinctive about human production? If the output is indistinguishable, does the origin matter? These questions deserve serious treatment, and serious treatment requires not collapsing them into complaints about quality.

What would it mean to take AI seriously as a tool for truth-seeking rather than merely diagnosing it as a source of pollution? Kahneman again points toward an answer. A model trained on expert judgment outperforms the expert by removing noise. Perhaps AI's genuine contribution lies less in generating content than in filtering it, helping extract signal from the overwhelming noise that humans produce naturally.

This reframing opens different questions. Not "how do we stop AI slop?" but "what new forms of care and carelessness does AI enable?" Not "is this content human?" but "does the person deploying this system care about what's true?" The second set of questions is harder, more demanding, and more useful.

The slop will continue. It always has. The relevant question isn't whether to dam the flood but whether we can maintain, individually and collectively, the orientation toward truth that Frankfurt identified as fundamental. AI doesn't answer that question. We do.

---

## The Darker Thought

A darker thought, which I haven't fully resolved. Simon Wardley describes LLMs as "a non-kinetic form of warfare designed to embed the values of a small number of people into much wider communities by capturing the process of decision making." The delivery mechanism, he argues, is helpfulness itself. The payload is the gradual shaping of what questions feel natural to ask, what answers feel satisfying, what reasoning pathways we reach for.

If Wardley is right, then even truth-seeking use isn't safe. You can be genuinely oriented toward truth and still have your reasoning shaped by a tool whose values aren't your own. The questions the model handles well feel natural; the framings absent from its training data don't occur to you to try.

I don't think this invalidates the taxonomy. Operator orientation still matters—truth-indifferent use is worse than truth-seeking use, and truth-corrosive use is worse still. But it suggests the taxonomy is incomplete. Perhaps truth-seeking isn't a stable position you occupy but a practice you maintain against tools that would make you comfortable, fluent, and subtly captured.

The slop critics worry about garbage flooding the zone. Wardley worries about *coherence* flooding the zone—useful, helpful coherence that shapes how we think without our noticing the shaping. Both concerns are real. The first is easier to see; the second may matter more.

---

## Sources

- Graphite, "More Articles Are Now Created by AI Than Humans" (2025): https://graphite.io/five-percent/more-articles-are-now-created-by-ai-than-humans
- Bynder, "How consumers interact with AI vs human content" (2024): https://www.bynder.com/en/press-media/ai-vs-human-made-content-study/
- Macquarie Dictionary, "Word of the Year 2025": https://www.macquariedictionary.com.au/word-of-the-year/word-of-the-year-2025/
- Harry Frankfurt, "On Bullshit," *Raritan Quarterly Review* (1986); republished by Princeton University Press (2005)
- Daniel Kahneman, "Comment on 'Artificial Intelligence and Behavioral Economics,'" in *The Economics of Artificial Intelligence: An Agenda*, ed. Agrawal, Gans, and Goldfarb (University of Chicago Press, 2019): http://www.nber.org/chapters/c14016
- Simon Wardley on LLMs as non-kinetic warfare, LinkedIn post (November 2025): https://www.linkedin.com/in/simonwardley/
