---
title: "The Political Economy of Fog"
date: 2026-01-01
tags:
  - ai-platforms
  - political-economy
  - options-theory
  - uncertainty
  - phenomenology
draft: false
---

# The Political Economy of Fog

## Prologue: The Point of the Game

I keep returning to something the philosopher James Carse wrote nearly forty years ago, and that Sangeet Paul Choudhary put more sharply in a recent post: the point of an infinite game is to keep playing.

This sounds like a platitude until you watch people forget it. You stay in the infinite game by winning finite games: the funding round, the product launch, the quarterly target, the acquisition. These finite games have clear winners and losers. They feel urgent. They come with metrics and deadlines and congratulations when you close them. But they are not the point. They are what you do to remain in the arena where the actual game unfolds.

The pathology (and it is a pathology, not a mistake) is when we optimize so hard for the next finite win that we sacrifice the capacity to keep winning. When we confuse the battle for the war. When every decision serves next quarter at the expense of next decade. I've watched this happen to people I respect, and I've caught myself doing it more than I'd like to admit.

What makes the AI platform market worth examining is that it has industrialized this confusion. Meta pays $2 billion for Manus in ten days. Cursor raises at $29 billion. The valuations make no sense as prices for things that exist; they make complete sense as prices for options on things that might. Everyone is playing finite games (the demo, the deal, the markup) and almost no one can tell whether these finite wins are building something durable or consuming the conditions for durability.

The fog prevents the knowing. What follows is an attempt to trace its contours.

By fog I mean something specific: a market condition where participants cannot evaluate their own productivity. Buyers cannot distinguish platforms that work from platforms that perform. Capital flows toward stories rather than outcomes; outcomes resist measurement. The fog isn't a temporary inconvenience that better tools will disperse. The fog is structural. It's produced by the same dynamics that produce the market itself.

This essay moves in concentric circles through that fog. It starts with the individual developer who feels productive while actually getting slower, a perception gap documented in studies that should have caused more alarm than they did. It moves outward to markets that cannot learn from their participants, capital structures that reward performance over verification, and political battles over who gets to define what "working" even means. At the center is a question I find genuinely unsettling: what kind of people do we become when we work in conditions of structural unknowability? What happens to judgment, to attention, to the capacity for honest self-assessment, when the tools we use are optimized to make us feel effective regardless of whether we are?

I don't have comfortable answers. But I've become convinced that the question (which game are you actually playing?) is the one that matters. The fog makes it difficult to answer, and that difficulty is itself the subject.

<!-- more -->

---

## Circle One: The Developer in the Fog

### I.

In a converted warehouse in San Francisco, a developer named Marcus is having the most productive day of his career. He's been using an AI coding assistant for three months, and the code is flowing. Functions appear almost as fast as he can describe them. The boilerplate that used to consume his mornings materializes in seconds. He's touching more files, closing more tickets, shipping more features than he ever has before.

Marcus feels like a god. His manager's Slack messages have shifted from check-ins to congratulations. The sprint velocity charts show his team pulling ahead. He's started mentoring junior developers on "AI-native workflows," and they look at him the way he once looked at senior engineers who seemed to conjure solutions from thin air.

What Marcus doesn't know (what he has no way of knowing from inside his experience) is that his production code is accumulating debt faster than he's shipping features. The AI-generated functions work in isolation but create subtle integration issues. The edge cases aren't handled because Marcus never learned to see edge cases; the assistant generated the happy path, and the happy path is what he tested. The refactoring that should have happened in month two was deferred because the velocity was so good, and now the codebase has calcified around patterns that will take six months to unwind.

Marcus will discover this in eight months, when a production incident cascades through systems that looked fine in every review. By then, he'll have been promoted. By then, he'll be leading a team of developers who learned to code the way he taught them. By then, the debt will be structural.

### II.

In early 2025, METR (an AI safety research organization) published a study that illustrates a structure worth examining. Experienced developers using AI coding assistants completed tasks 19% slower than developers without AI assistance, while believing they had worked 20% faster.

One study proves nothing, and this one had specific conditions: experienced developers, their own codebases, particular task types. The precise numbers may not generalize. But the structure they reveal is interesting regardless of the exact percentages. A 39-percentage-point gap between perceived and actual performance. The developers weren't just wrong about the magnitude of the effect; they were wrong about its direction. They experienced productivity gains that were, in fact, productivity losses.

Daniel Kahneman would recognize this immediately. The developers' System 1 (fast, intuitive, pattern-matching) registered "code appearing on screen" as progress. The dopamine hit of watching the AI write was neurologically real. But System 2 (slow, analytical, the part that would notice the debugging took longer, that the edge cases weren't handled, that the total time had increased) wasn't activated. Why would it be? The feeling was so good. The velocity metrics were so impressive. Everything that could be measured was measuring improvement.

This perception gap is not a bug in the METR study. It's a feature of how AI assistance works at the phenomenological level. The experience of using these tools is designed to feel productive. The interfaces reward prompting and generation. The metrics track output, not outcome. The developer sits in a fog of their own experience, accelerating confidently in a direction they cannot verify.

### III.

The temptation here is to moralize: the developers should have known better, should have measured more carefully, should have distrusted their intuitions. But this misses what makes the perception gap structurally interesting.

This gap exists because the tools work at the level of phenomenology. The developer's experience of productivity is not a side effect of using AI assistants; it is what the tools produce. The code generation is secondary to the feeling of generation. The interfaces are optimized for engagement, which means they're optimized for the subjective experience of progress, which means they're optimized for producing the very gap that makes evaluation impossible.

Philip K. Dick built a career exploring this structure. His characters live in worlds where reality is negotiable, where memories can be implanted, where the authentic cannot prove its authenticity because any proof could also be simulated. The horror in Dick's fiction is never the unreality itself; it's the impossibility of verification. You can never be certain you're not in the fake version.

The METR study suggests something Dickian about AI assistance. The developer who feels productive but isn't faces an epistemological condition, not a measurement problem. The test that would reveal the truth (actual productivity measurement) requires standing outside the experience. But there is no outside. You're always experiencing your own experience. The feeling of productivity is itself a product, manufactured by interfaces optimized to produce that feeling.

The authentic cannot prove its authenticity. The problem is structural, not individual.

### IV.

Simone Weil, the French philosopher and mystic, wrote about attention as a form of generosity: the capacity to wait, to not-act, to let the object reveal itself rather than projecting onto it. Attention for Weil is not focus in the modern productivity sense. It's something closer to receptivity: the willingness to sit with difficulty until understanding emerges.

AI coding assistants invert this. They generate before you've fully attended. They answer before you've understood the question. They fill space that should be empty. The developer's experience of productivity is the experience of attention being colonized; the quiet space where understanding forms is now occupied by generation.

This explains why the METR study found experienced developers more harmed than novices. The experienced developer had cultivated something over years of practice: a habit of attention, a learned capacity to sit with uncertainty until the problem resolved itself. They had developed what programmers sometimes call "taste": an intuition for code quality that emerges from countless hours of making mistakes and noticing their shapes.

The AI "helps" by removing the waiting, which is the part where understanding happens.

The novice, with no such practice to disrupt, experiences only the velocity. They haven't learned what they're losing because they never had it. For them, AI-assisted coding isn't a disruption of a practice; it's the practice itself. They are learning to code in conditions where the attention that would have produced understanding is never required.

The counterargument: taste isn't only about output. It's about the capacity to evaluate output, to know when something is good, to recognize quality you couldn't produce yourself. If AI assistance produces code while atrophying the capacity to evaluate code, you end up with developers who can generate endlessly but cannot tell what they've generated. The perception gap becomes permanent. You've produced achievement without understanding, velocity without direction.

### V.

Gregory Bateson, the anthropologist and cyberneticist, described a pattern he called the double bind: a situation where someone receives two contradictory messages with no way to comment on the contradiction. The child told to be spontaneous, which is impossible because spontaneity cannot be commanded. The employee praised for initiative and punished for not following procedures. The messages conflict, but you can't say they conflict, because saying so violates the rules of the interaction.

Double binds, Bateson observed, are pathogenic. They create what he called schismogenesis: escalating cycles where each attempt to resolve the contradiction makes it worse.

Watch Marcus, the developer caught in the perception gap. He feels productive. The metrics say he's productive. But his codebase is degrading; the deadlines are slipping, the technical debt is mounting, the production incidents are accumulating. So he leans harder into AI assistance. More prompting, more generation, more of the thing that feels like it's working. The productivity feeling intensifies. The actual productivity deteriorates further. Each turn of the cycle digs the hole deeper.

Marcus can't comment on the contradiction because he can't perceive it. The gap between felt productivity and real productivity is invisible from inside the felt productivity. The double bind is perfect: you cannot solve the problem because solving it requires seeing it, and the problem is precisely that you cannot see.

Bateson studied double binds in families, in institutions, in communication patterns that drove people toward breakdown. He didn't live to see them encoded in software interfaces. But the structure he identified (contradictory messages without meta-commentary) describes exactly what happens when a tool that feels like progress isn't. The developer receives two signals: *you're doing great* from the interface, the velocity metrics, the dopamine; and *you're falling behind* from the actual outcomes that no one is measuring. The signals conflict. The conflict cannot be named.

The schismogenesis runs until something external breaks it. A production incident. A burnout. A layoff. The system doesn't self-correct because the system is structured to prevent the perception that would enable correction.

Marcus is winning finite games. Every velocity metric, every closed ticket, every sprint completion is a win. He's optimizing brilliantly for the game in front of him.

But coding skill is an infinite game. The point is to keep developing judgment, to maintain the capacity to evaluate code, to stay in the arena where quality can be recognized. Marcus is winning sprints while losing the ability to know what winning means. He's optimized so hard for finite victories that he's sacrificing the infinite game they were supposed to serve.

The fog doesn't just hide whether you're productive. It hides whether your finite wins are serving your infinite game or consuming it.

---

## Circle Two: The Market That Cannot Learn

### I.

At the individual level, the perception gap is bad enough. At the market level, it compounds into something that should concern anyone trying to allocate capital rationally.

George Akerlof won a Nobel Prize for explaining how markets collapse when buyers can't verify quality. In his 1970 paper "The Market for Lemons," he showed that information asymmetry creates a death spiral. If buyers can't distinguish good cars from bad ones, they pay the average price. But at the average price, sellers of good cars lose money and exit. The average quality drops. Buyers adjust expectations downward. Prices drop further. More quality sellers exit. The market doesn't stabilize at medium quality; it races toward the bottom, stabilizing only when quality is so low that even uninformed buyers can tell what they're getting.

The AI agent market has all the conditions for a lemons problem.

Every vendor claims "enterprise-ready AI." The demos are impressive; they're designed to be, optimized for the moments when someone is watching. Case studies are curated for persuasion, stripped of failures and context. Testimonials come from users caught in the same perception gap as Marcus, genuinely believing they're more productive while they're not.

Buyers can't verify the claims. How would they? The product is cognitive augmentation. The outcome is supposed to be better decisions, faster work, higher quality. But the METR finding suggests that enthusiasm is a lagging indicator of actual performance. The users who love the tool most are the ones it's hurting most. The testimonial proves nothing except that the person giving it feels good.

### II.

The opposing view isn't obviously wrong.

Markets have dealt with quality uncertainty before. Brands exist because they solve information problems: the buyer doesn't know if this particular product is good, but they know that products with this brand are usually good. Warranties and guarantees shift risk from buyers to sellers, incentivizing quality. Third-party certifications create trusted intermediaries. Reputation systems aggregate individual experiences into collective signals.

Perhaps the AI platform market will develop these mechanisms. Perhaps we're simply early; the brands haven't formed, the certifications haven't emerged, the reputation systems haven't matured. Perhaps in five years there will be a clear hierarchy: platforms with track records of reliability, certification bodies that verify claims, enterprise buyers sophisticated enough to run rigorous evaluations. The lemons problem might be a transitional condition, not a terminal one.

This is a reasonable position, and history provides some support. The early days of most technology markets feature exactly this kind of uncertainty. The software industry in the 1980s was full of lemons: products that crashed, vendors that disappeared, claims that evaporated on contact with reality. Over time, quality signals emerged. Microsoft became a brand you could trust (for certain values of trust). Enterprise software developed evaluation methodologies. The market sorted itself out.

Medicine and financial services developed verification mechanisms for products at least as complex as AI platforms. Drug trials, clinical outcomes, audited returns, regulatory oversight. These markets aren't perfectly transparent, but they're not pure fog either. What enabled them to develop quality signals?

Three things, mostly. First, regulatory forcing functions: the FDA, the SEC, bodies with teeth that could compel disclosure and punish fraud. Second, catastrophic failures with clear causation (thalidomide, Enron), events dramatic enough to create political will for transparency. Third, long enough feedback loops that outcomes eventually surfaced. A drug either works or it doesn't, a fund either returns or it doesn't, and given enough time the truth emerges.

Does the AI platform market have these enabling conditions? Not obviously. No regulatory body has jurisdiction over "does this AI actually help." No catastrophic failure has yet been clearly traced to AI assistance rather than human error. And the feedback loops are precisely what the perception gap corrupts; the outcomes don't surface because the users can't perceive them.

But there's a reason to think AI platforms are different, and it has to do with the nature of what's being purchased.

### III.

Software, for all its complexity, eventually reveals itself. Install a word processor and use it for a month; you'll know whether it works. Deploy an ERP system and run your business on it; the quality becomes evident in the running. The feedback loop between use and evaluation is tight enough that quality eventually surfaces.

AI platforms short-circuit this feedback loop.

The product is cognitive augmentation: making decisions, generating content, automating judgment. The feedback loop between using the tool and knowing whether the tool helped requires evaluating the counterfactual: what would have happened without the tool? This is exactly what the perception gap prevents. The developer feels faster, the manager sees higher velocity, the metrics improve. The fact that the code is worse, that the technical debt is mounting, that the real productivity declined: this lives in a counterfactual that no one is positioned to evaluate.

Worse: the tool's output becomes the baseline. Once you've been using AI assistance for six months, you don't remember what your unassisted work looked like. You can't compare current output to a previous version of yourself that no longer exists. The counterfactual has been erased by the intervention.

In Akerlof's used car market, the buyer eventually learns whether they got a lemon. They drive it home, and it breaks down or it doesn't. The learning happens. In the AI platform market, the equivalent learning never happens because the tool has altered the conditions under which learning would occur.

### IV.

In a glass-walled conference room in Manhattan, Sarah is preparing to recommend an AI platform to her firm's partners. She's spent three months evaluating options: sitting through demos, running pilots, talking to references. Every vendor showed her the same thing: impressive generation, smooth interfaces, metrics that went up and to the right. The references were uniformly positive. Of course they were. No one gives a reference for a product they think failed them.

Sarah knows something is wrong. The pilots were too short to measure real outcomes. The metrics tracked activity, not results. The references couldn't articulate what "working" meant except that it "felt faster." But Sarah has to make a recommendation. The partners expect a decision. The firm's competitors are already using these tools, or claim to be.

She's going to recommend the platform with the best demo. Not because she believes it's the best platform (she has no way of knowing) but because the demo is the only signal she has, and the demo was very good. The vendor understood what she needed to see. The vendor always understands what buyers need to see.

This is not a failure of Sarah's diligence. It's the rational response to an information environment where quality signals don't exist. When you can't tell good from bad, you optimize on what you can tell: presentation quality, brand recognition, the confidence of the sales team, the sophistication of the demo. These proxies have no reliable connection to actual quality, but they're the only game in town.

### V.

Akerlof's lemons dynamic has a stable endpoint, and it's not zero quality. His insight was that markets converge on whatever level of quality buyers can verify. If you can tell a car is a complete wreck, you won't pay for it, so complete wrecks exit the market. The market stabilizes at the lowest quality level that buyers can't detect.

For AI platforms, this level is: "produces output that looks plausible and feels helpful, even if it doesn't actually help." Platforms that clear this bar survive and grow. Platforms that fall below it (producing obviously bad output) get filtered. Platforms that exceed it (producing genuinely good output) can't charge premium prices because buyers can't detect the premium.

This is a depressing equilibrium. The market converges on tools that feel helpful but aren't transformative: good enough to justify adoption, not good enough to deliver real value, reliable enough to avoid embarrassment but not reliable enough to trust with anything critical. The AI assistant as mediocre colleague: pleasant, competent-seeming, ultimately not adding much.

The fog is not uniform. Some AI tools clearly work. Copilot has retention data suggesting real value. ChatGPT has 200 million users who keep returning. The question "does this tool generate plausible code?" is answerable. The question "does this tool make developers more productive over twelve months?" is foggier. The question "does this platform justify a $29 billion valuation?" is maximally foggy. The fog thickens as claims escalate, from utility to productivity to transformation to world-historical importance. The interesting question isn't whether any quality signal exists, but whether quality signals exist where the capital flows. Copilot might work; that tells you nothing about whether Manus justified $2 billion.

Some readers will recognize this description. They've been using AI tools for a year and have a nagging sense that the tools are less useful than the enthusiasm suggests. They can't quite articulate what's wrong. The tools work, kind of. The output is acceptable, sort of. But the transformation that was promised hasn't materialized. Maybe they're using the tools wrong. Maybe they just need more practice. Maybe the next version will be the breakthrough.

Or they're living in the lemons equilibrium, mistaking mediocrity for "not yet good enough" because they have no baseline to compare against.

### VI.

Consider a counter-narrative: perhaps the lemons dynamic is exactly what competitive markets need to function.

Consider: if buyers could perfectly evaluate quality, there would be no competition on price. Everyone would buy the best product. The best product would capture the entire market. We'd have monopolies everywhere. Information asymmetry, on this view, is what keeps markets competitive. Buyers who can't tell good from bad spread their purchases across multiple vendors, keeping multiple vendors in business, preserving competition that eventually produces innovation.

This is economically coherent, even if it feels perverse. Some level of buyer ignorance is systemically functional, even as it's individually costly. The market as a whole benefits from the diversity that ignorance preserves, even as individual buyers pay the cost of purchasing lemons.

The question then becomes not "how do we fix the market" but "do we want this kind of market, given what it produces?"

That's a political question, not a technical one. And we'll return to it.

But notice the game structure. Every participant in the lemons market is playing finite games. The vendor plays "close this deal." The buyer plays "make a defensible recommendation." The reference plays "maintain my relationship with the vendor." Each finite game has a winner. Each winner advances.

The infinite game is different: a market that can learn. A market where quality signals emerge, where buyers get better at evaluation, where vendors who deliver value capture it. A market that improves its own capacity for judgment over time.

The lemons dynamic is what happens when finite games devour the infinite game. Each participant optimizes for their immediate win. The aggregate effect destroys the conditions for sustainable competition. Everyone advances; the arena crumbles.

Sarah, recommending the platform with the best demo, is playing rationally. She'll win her finite game (the recommendation will be made, the decision will be defensible). But she's contributing to a market that cannot learn from her participation. Her finite win subtracts from the infinite game.

---

## Circle Three: Capital's Installation Phase

### I.

If markets have information problems, capital markets are supposed to solve them. Investors do due diligence. They verify claims. They price risk appropriately. Smart money finds quality; quality gets funded; the market corrects itself through the discipline of return-seeking.

In practice, the capital structure of the AI platform market is making things worse.

Carlota Perez, the economist who studies technological revolutions, distinguishes two phases: installation and deployment. During installation, financial capital dominates. Money floods into new infrastructure, creating bubbles and frenzies. Valuations detach from fundamentals because the fundamentals haven't emerged yet. Everyone is pricing options, buying variance, purchasing possibility. The dot-com boom, the railway mania, the canal speculation: all were installation-phase phenomena.

During deployment, production capital dominates. The surviving infrastructure gets integrated into the real economy. The speculation burns off. What remains generates actual value, captured in revenue and profit rather than possibility and hope.

Between the phases: a crash. The excesses of installation get corrected. The companies that built real infrastructure survive; the rest don't. Perez has called this transition the "turning point": the moment when financial capital gives way to production capital, when the evaluation criteria shift from "what could this become?" to "what does this actually produce?"

We are deep in the installation phase for AI platforms. The $29 billion valuations, the ten-day acquisitions, the capital flooding faster than production value can absorb: all are installation-phase signatures. This observation should not be read as criticism. Installation phases look like this. The people making these bets aren't fools; they're pricing options in a market where variance is enormous.

But installation-phase capital has a specific pathology: it rewards performance of progress over verification of progress.

### II.

In a venture capital office in San Francisco, David is preparing a term sheet for an AI agent startup. He's seen the pitch deck. The metrics are impressive: ARR growing 40% month-over-month, logos from recognizable companies, a founder who demos well. The product might work. David can't tell. The diligence would take six months, and by then some other fund will have closed the round.

David knows the game. He's been playing it for fifteen years. You invest in the next round, not in the fundamentals. You bet on the story, because the story is what raises the next round, which is what validates your entry. The music is playing. Everyone is dancing.

The term sheet David sends values the company at $800 million. The metrics suggest $100 million would be aggressive. But $100 million doesn't clear the round; other funds are offering higher. The price isn't about what the company is worth. It's about what other funds are willing to pay, which is about what they think still other funds will pay at the next round.

David's fund will mark this investment up 3x in eighteen months when the Series C closes at $2.4 billion. The markup will justify the fund's returns to its LPs. The returns will justify the fund's next raise. The cycle will continue until it doesn't.

This is not irrational. Within the game as it's structured, David is playing optimally. The returns are real. The markups are real. The fund performance is real. What's questionable is the relationship between all this reality and anything that could be called fundamental value. The game is internally coherent and externally unmoored.

### III.

Hyman Minsky, the economist who spent his career explaining why capitalist economies are inherently unstable, traced a progression that applies disturbingly well to what we're seeing.

During stable periods, Minsky observed, financing moves through three stages:

*Hedge financing*: the borrower can cover interest and principal from cash flows. This is old-fashioned, boring, sustainable financing. You lend money, the business generates revenue, the revenue covers the debt.

*Speculative financing*: the borrower can cover interest but must roll over principal. This depends on continuous access to credit markets. You're fine as long as you can keep refinancing. If credit tightens, you're exposed.

*Ponzi financing*: the borrower can't cover interest or principal from operations. The only way to service debt is through asset appreciation. You're betting that the thing you bought will be worth more tomorrow than you paid for it today. If appreciation stops, everything unwinds.

Minsky's disturbing insight: stability itself creates instability. Success encourages risk-taking. Risk-taking moves the market from hedge to speculative to Ponzi. The longer the stability continues, the more leverage accumulates, the more participants depend on appreciation rather than fundamentals. Then something triggers a correction (any shock will do) and the Minsky Moment arrives.

The AI platform market is in speculative/Ponzi territory by any honest accounting. Manus's $100 million ARR didn't justify a $2 billion acquisition by hedge-financing logic. You don't pay twenty times revenue for an eight-month-old company because you expect the cash flows to service the investment. You pay because you expect the strategic value, the option value, the synergies that the spreadsheet can't model. All bets on appreciation rather than production.

When the underlying uncertainty compounds (perception gap plus lemons market plus unknowable exercise conditions), the financing structure doesn't correct the problem. The financing structure accelerates it. Capital flows toward the options with the best story, regardless of whether those options can be exercised.

### IV.

But the opposing view matters.

Installation-phase financing might be exactly what technological revolutions need. The money flooding into AI platforms isn't stupid; it's functional. Someone has to fund the exploration of possibility space. Someone has to pay for the experiments that fail. Someone has to absorb the losses that make the eventual winners possible.

The railways of the 19th century were funded by speculators who lost everything. Their capital built the tracks that transformed economies. The fiber-optic cables of the dot-com boom were laid by companies that went bankrupt. Their cables carry the internet we depend on today. The AI platform funding, for all its apparent irrationality, is building infrastructure that will matter long after the current crop of investors have marked their losses.

On this view, the Minsky dynamic isn't a bug; it's how capitalist economies explore new technological frontiers. The installation-phase frenzy is waste in one sense and investment in another. The question isn't whether there will be a correction (there will be) but whether the correction will leave behind infrastructure that justifies the cost.

This is Brian Eno's perspective on generative systems, applied to capital markets. You don't specify outcomes; you create conditions for emergence. The waste is productive. The failures are substrate. Something grows in the chaos that couldn't have been planned.

I find this view genuinely persuasive, which is why I want to name its limits. The claim that installation-phase waste is functional assumes that what gets built during the frenzy has durable value. Railways and fiber-optic cables are physical infrastructure; once built, they persist. Software platforms are not physical infrastructure. They depreciate quickly, require continuous investment, and can become worthless overnight when something better emerges.

The AI platforms funded today might not become the infrastructure of tomorrow. They might become nothing: abandoned code, dispersed teams, forgotten pitch decks. The waste might just be waste.

We won't know which interpretation is right until we're through the transition. Living in the installation phase means the meaning of what's happening is underdetermined by the present.

The Carse frame sharpens this. Installation-phase capital is finite-game capital. It plays for the next round, the next markup, the next exit. These are games with clear winners, and the winners are winning.

The question is whether these finite wins serve an infinite game: the creation of durable infrastructure, the exploration of possibility space, the accumulation of capabilities that will matter after the crash.

David, sending the term sheet at $800 million, is winning his finite game. His fund will mark up. His LPs will be satisfied. His next raise will succeed. But is he contributing to infrastructure that survives the transition? Or is he funding a performance that evaporates when the music stops?

The fog makes this unanswerable. The finite games are visible; David can see his returns. The infinite game is invisible; no one can see whether durable value is accumulating or being consumed. You can optimize for what you can see. What you can see is the finite game.

Minsky's cycle is the collective result. Everyone winning finite games, no one tracking the infinite game, the system accumulating fragility until something breaks.

---

## Circle Four: The Political Economy of Categories

### I.

So far, we've moved from individual phenomenology (the developer in the fog) through market dynamics (the lemons problem) to capital structures (installation-phase financing). Each circle has revealed how the fog is produced and reproduced at different levels of scale. But there's a dimension we haven't examined: politics.

The fog is a political arena, not only an epistemological condition. Who gets to define what "working" means? Who sets the standards for "enterprise-ready"? Who determines when an AI platform has crossed from experimental to reliable?

These are not technical questions. They're questions about power.

Mary Douglas, the anthropologist who spent her career studying how societies create categories, would see immediately what's at stake. In *Purity and Danger*, she showed that what counts as "clean" or "dirty," "safe" or "dangerous," is not an objective property of things. It's a social construction. Communities draw boundaries. Institutions enforce categories. What counts as pollution in one culture is sacred in another. The distinction isn't discovered; it's produced, and producing it is an exercise of power.

"Enterprise-ready AI" is this kind of category. It doesn't name a natural kind, like "hydrogen" or "mammal." It names a social boundary that institutions are currently fighting to draw. The fight matters because whoever wins gets to define the game everyone else plays.

### II.

Consider the stakeholders and what they want.

*Vendors* want broad categories. If "enterprise-ready" means "has SSO and an SLA," then every vendor is enterprise-ready. The term becomes marketing, applied wherever it helps close deals. The fog serves vendors; it lets them make claims without accountability.

*Regulators* want narrow categories. If "enterprise-ready" means "approved for specific use cases after extensive review," then regulators control the gate. Every new application requires their blessing. The fog alarms regulators; they want to disperse it through standards and oversight.

*Enterprise buyers* want verifiable categories. If "enterprise-ready" means "can prove its claims through independent evaluation," then buyers can make informed decisions. They want the fog lifted so they can see what they're purchasing.

*AI safety researchers* want cautious categories. If "enterprise-ready" means "demonstrated safe under adversarial conditions," then most platforms don't qualify. They want the fog treated as dangerous until proven otherwise.

Each stakeholder has a different relationship to uncertainty, and each would benefit from a different resolution of the category contest. The vendors are winning so far. "Enterprise-ready" means whatever vendors say it means, and buyers lack the tools to challenge the claim. But this could change.

### III.

Category battles have clocks tied to events.

A major AI platform failure, something dramatic enough to attract regulatory attention, could shift the contest overnight. Suddenly "enterprise-ready" becomes a compliance category with legal weight. The vendors who anticipated regulation (building audit trails, human oversight, certification infrastructure) have structural advantage. The vendors who didn't scramble to retrofit, or die.

Alternatively, industry self-regulation could preempt government action. The major players agree on standards, create certification bodies, define what "enterprise-ready" means before regulators do. This happened in other industries: accounting standards, credit ratings, organic food certification. It creates a different power dynamic: the incumbents who wrote the standards advantage themselves.

Or the category fragments. Instead of one definition of "enterprise-ready," each vertical develops its own. "Healthcare AI" gets defined by healthcare regulators. "Financial AI" gets defined by financial regulators. The general-purpose category dissolves into specialized niches, each with its own rules.

The point is not to predict which outcome occurs (prediction is exactly what the fog prevents) but to recognize that the outcome is underdetermined by technology. The same AI capabilities could live in radically different category structures depending on how the political contest resolves. And the stakes are high: the winning definition shapes which companies thrive, which business models work, what kinds of AI get built and deployed.

### IV.

Douglas's framework helps surface a deeper politics.

Categories don't only sort things that already exist. They call things into being. Once "organic food" became a category, farmers started producing for the category. Once "enterprise software" became a category, companies started building for the category. The category shapes what gets created, not only what gets classified.

If "enterprise-ready AI" gets defined as "deterministic, auditable, controlled," the platforms that survive will be deterministic, auditable, controlled. The more generative, emergent, surprising possibilities of AI (the ones that might be most valuable but are least predictable) will be selected against. The category will produce a certain kind of AI by excluding alternatives.

This is James C. Scott's critique of legibility, applied to the AI market. In *Seeing Like a State*, Scott documented how high-modernist projects (planned cities, scientific forestry, collectivized agriculture) failed because they optimized for what central planners could see and measure, destroying the tacit knowledge and local adaptation that made systems actually work.

The push to make AI "enterprise-ready" might be the same kind of legibility project. The aspects of AI that can be standardized, measured, and certified are not necessarily the aspects that create value. The pressure to make AI legible to institutions might kill exactly what makes it useful.

### V.

The category battle connects back to the lemons problem in a way worth making explicit.

Akerlof's lemons market assumes buyers can't verify quality. But what counts as quality is itself defined by categories. If the category for AI platforms emphasizes "feels helpful" (vendor preference), the lemons market stabilizes around platforms that feel helpful. If the category emphasizes "verifiably improves outcomes" (buyer preference), the market stabilizes around platforms that can prove improvement. If the category emphasizes "poses no unacceptable risks" (regulator preference), the market stabilizes around platforms that can demonstrate safety.

The lemons problem doesn't have a fixed endpoint. It has multiple possible endpoints, and which one we reach depends on political contests over categorical definition.

This is the political economy of fog: the fog isn't just there, naturally occurring, waiting to be measured or dispersed. The fog is produced by conflicting interests, maintained by power imbalances, and shaped by categorical battles that determine what would even count as clarity.

And each stakeholder is playing finite games within the fog. The vendor plays "capture this category." The regulator plays "assert jurisdiction." The buyer plays "demonstrate due diligence." Each game has tactical winners.

The infinite game (categories that actually track quality, standards that enable market learning, definitions that help rather than obscure) requires coordination that finite-game competition prevents. Everyone is so busy winning their categorical battle that no one is building categories worth fighting over.

The deepest political question isn't who wins the category contest. It's whether anyone is playing for categories that serve the infinite game of market learning, or whether the contest itself is consuming the possibility of categories that work.

---

## Circle Five: The Achievement-Subject All the Way Down

### I.

We've moved from individual phenomenology through market dynamics through capital structures through political economy. Each circle has revealed the fog at a different level of scale. But there's one more level, the deepest and most uncomfortable: what the fog does to the people in it.

Byung-Chul Han, the German-Korean philosopher, describes the contemporary condition as a shift from discipline to achievement. In the disciplinary society Foucault described, external authorities told you what to do and punished deviation. The factory whistle. The prison guard. The school principal. Power was visible, localized, exercised from outside.

In the achievement society, Han argues, constraint has moved inside. You tell yourself what to do. You punish yourself for falling short. The achievement-subject is both exploiter and exploited, both the demanding boss and the overworked employee. There's no one else to blame because there's no one else. The discipline is self-generated.

Marcus, the developer in the productivity spiral, isn't being exploited by his AI assistant. He's exploiting himself through the AI assistant. He chases velocity because he's internalized the imperative to chase velocity. The spiral isn't imposed from outside; it's self-generated. The AI is just the tool that lets him exploit himself more efficiently.

### II.

This framing explains why knowing about the perception gap doesn't help.

You can show Marcus the METR study. You can explain the 39-percentage-point divergence between perceived and actual performance. You can demonstrate, with data, that his felt productivity is disconnected from his real productivity. He might even believe you. But his behavior won't change, because the behavior isn't driven by belief about productivity. It's driven by the internalized imperative to achieve.

The achievement-subject doesn't optimize for outcomes; they optimize for the feeling of achievement. The feeling is the goal. The velocity metric, the completed tickets, the flowing code: these are not instrumental to some further end. They are the end. Producing the feeling of progress is what the achievement-subject is for.

This is why the AI assistant fits so perfectly. It produces the feeling of progress with maximum efficiency. Every prompt generates code. Every generation feels like accomplishment. The interface is optimized for achievement-feeling, and the achievement-subject is optimized for consuming achievement-feeling. The match is perfect.

The perception gap, from this perspective, isn't a problem to be solved. It's a feature of how achievement-subjects relate to their work. Closing the gap would require caring about something other than the feeling of achievement: actual outcomes, long-term quality, the sustainable cultivation of skill. But caring about those things is precisely what the achievement-subject has been conditioned not to do.

### III.

The same analysis applies at every level we've examined.

David the investor isn't being exploited by the Minsky cycle. He's exploiting himself through the Minsky cycle. He chases returns because he's internalized the imperative to chase returns. The markup, the next fund, the portfolio performance: these aren't instrumental to some further end. They're the end. The investor is an achievement-subject optimizing for investor-achievement-feeling.

Sarah the buyer isn't being deceived by the lemons market. She's participating in a game whose rules she's internalized. The recommendation, the decision, the appearance of due diligence: these produce the feeling of having done her job. Whether the platform works is less important than having selected a platform defensibly. The buyer is an achievement-subject optimizing for buyer-achievement-feeling.

The vendors, the regulators, the researchers: all are achievement-subjects optimizing for their respective achievement-feelings. No one is forcing the ten-day acquisitions. No one is forcing the inflated valuations. No one is forcing the confident demos that prove nothing. Everyone is doing it to themselves because they've internalized the games that make these behaviors feel like achievement.

Carse would recognize this immediately. Achievement-subjectivity is finite-game subjectivity. The achievement-subject can only see finite games: the next win, the next feeling of progress, the next dopamine hit of completion. The infinite game is structurally invisible because it doesn't produce achievement-feeling. Continuation doesn't feel like victory. Optionality doesn't feel like winning.

Marcus sprints because sprinting feels like achievement. The infinite game (sustainable skill development, the capacity to evaluate code, judgment that compounds over years) doesn't feel like anything in the moment. It's what you have when you're not optimizing for feeling. It's what you lose when you are.

This is why the fog is so hard to escape. The finite games are phenomenologically vivid. They produce feelings. The infinite game is phenomenologically silent. It doesn't produce feelings; it produces capacity. Achievement-subjects are structurally tuned to what produces feelings. The infinite game is invisible to them by design.

### IV.

Bateson's schismogenesis reaches its deepest register here.

The escalating spirals we've examined (productivity spirals, lemons spirals, Minsky cycles) aren't imposed by some external structure. They're the collective product of achievement-subjects competing to achieve. Each participant creates the conditions for others' participation. The developer's enthusiasm becomes the buyer's testimonial. The buyer's purchase becomes the vendor's revenue. The vendor's revenue becomes the investor's return. The investor's return becomes the next company's funding. Each achievement creates the conditions for the next achievement, and the system accelerates because each participant is optimizing for their own achievement-feeling.

The system has no outside. Every participant is inside the achievement-subjectivity that produces and reproduces the fog. Even the critics, even this essay, are achievement-activities. I'm optimizing for the feeling of having understood something. You're optimizing for the feeling of having read something insightful. We're all inside the structure we're examining.

This is why Bateson was pessimistic about escaping schismogenesis. You can't solve a double bind by trying harder within it. You'd need to step to a different level where the bind doesn't operate. But where would that level be? Where do you stand outside a market you're participating in? Where do you stand outside an achievement-subjectivity you've internalized?

### V.

Simone Weil would say: contemplation.

Not thinking about things, not analyzing or critiquing. Something closer to waiting. Attention without agenda, receptivity without production. Time that isn't productive and space that isn't optimized. The willingness to be present without needing to achieve.

This sounds like mystical nonsense in a business context. But notice what Weil is pointing at: a mode of being that isn't achievement-subjectivity. A way of relating to work that doesn't optimize for the feeling of progress. A capacity to be present without generating, to wait without filling the space.

AI assistance, whatever else it does, forecloses this possibility. It fills space. It generates before you've waited. It provides achievement-feeling on demand, which means you never have to sit with the discomfort that precedes understanding.

The capacity for contemplation is upstream of everything else: the capacity to evaluate outcomes, to recognize quality, to know when something is working and when it isn't. Destroying contemplation destroys the foundation on which rational markets depend.

### VI.

The uncomfortable conclusion: market corrections don't fix this.

The Minsky moment will come. The installation phase will crash into deployment. The lemons market will shake out. Valuations will compress. Companies will fail. Money will be lost.

And then the same people will rebuild the same structures.

They'll rebuild them because the problem isn't the structures. The problem is what they've become. Achievement-subjects produce achievement-subject conditions. The fog isn't something that descended on the market; the fog is what achievement-subjects generate when they compete to achieve. Clear the fog and they'll generate more fog, because fog-generation is what they do.

This is how markets under capitalism work. Achievement-subjectivity is functional for capital accumulation. The installation-phase frenzies fund exploration, the crashes clear deadwood, the rebuilding creates new opportunities. The whole thing lurches forward, generating more output than any alternative system has managed, at the cost of producing subjects who cannot stop generating.

Whether this is good or bad depends on what you think humans are for. Economics can't answer that question.

---

## The Spiral Back: Two Ways Through the Fog

### I.

Elena may not exist yet, not as a single person anyway. She's the archetype of what would have to exist for the market to correct itself.

Elena builds an AI agent platform. She's different from most founders in the space. Fifteen years of enterprise software, regulated industries, tools that had to work and could be audited. She watched the AI platform market with fascination and horror. The demos were impressive. The adoption was real. The valuations were insane. And no one, as far as she could tell, was building the infrastructure to verify whether any of this actually worked.

So she built it. Her platform publishes failure rates.

Not hidden in documentation. Not buried in settings. On the main dashboard, in real time: "This platform succeeds 85% of the time on task type X. 60% on task type Y. We don't recommend it for task type Z."

Her investors hate it. Her advisors think she's committing commercial suicide. Every competitor hides their failures; why would she advertise hers? The sales conversations are harder. Prospects see the 60% number and flinch. Some walk. Her ARR is a fraction of what her competitors claim.

But something else is happening too.

### II.

Read Elena through each circle.

*Through Circle One (the developer in the fog).* Elena is trying to break the perception gap. By publishing failure rates, she gives users a baseline to evaluate against. "I expected 85% success and I'm experiencing 70%something's wrong" is a different mental state than "I feel productive and therefore I must be productive." She's trying to create the conditions for attention that Weil describes, the capacity to see what's actually happening rather than what the interface wants you to feel.

*Through Circle Two (the lemons market).* Elena is betting that quality signals can exist. Her failure rates are an attempt at credibility, a costly signal that only confident vendors can afford. Advertising your failures is commercial suicide unless your failures are better than competitors' hidden failures. She's hoping that buyers will eventually learn to read the signal, that the lemons dynamic isn't terminal.

*Through Circle Three (installation-phase capital).* Elena is positioning for the crash. She's raised $12 million while competitors have raised $400 million, which means she needs less capital to survive, which means she can weather a funding winter that would kill the Ponzi-financed competition. She's betting that Perez's turning point will come, and that she'll be standing when it does.

*Through Circle Four (category politics).* Elena is trying to shape the category. By defining success rates by task type, she's creating the taxonomy that could become the standard. If "enterprise-ready" comes to mean "publishes audited failure rates," she wins. She's not waiting for regulators or industry bodies to define the category; she's defining it herself, hoping others will follow.

*Through Circle Five (achievement-subjectivity).* And here's where it gets complicated. Is Elena outside the achievement-subjectivity? Or is she an achievement-subject achieving differently?

### III.

The honest answer: both.

Elena is still optimizing for achievement-feeling. She's betting, fundraising, competing, trying to win. The fact that her strategy is different doesn't mean she's escaped the game. She's playing for different stakes (reputation for quality instead of revenue growth, survival through crash instead of growth until crash) but she's still playing.

At the same time, there's something in Elena's approach that gestures toward an alternative. Publishing failure rates requires a certain willingness to not achieve. It means accepting that your numbers will look worse than competitors' numbers. It means giving prospects reasons not to buy. It means sitting with the discomfort of visible limitation rather than performing confidence.

This is closer to Weil's contemplation than it might appear. Not full contemplation; Elena is still a founder, still competing, still optimizing. But there's a seed of something else: the willingness to let reality be visible rather than managing perception. The willingness to wait for buyers who can see rather than performing for buyers who can't.

### IV.

Elena understands which game she's playing.

She's playing finite games. Every founder does. The fundraise, the sale, the feature ship. These have winners and losers. She wants to win them.

But she hasn't mistaken them for the point. The point is staying in the arena. The point is building the capacity to keep building. The point is the infinite game.

Her finite games are selected for their service to the infinite game. Publishing failure rates loses finite games: deals walk, investors flinch, competitors mock. But it builds something that compounds: credibility, infrastructure for verification, a category position that strengthens with time.

Most founders do the opposite. They win finite games that consume the infinite game. They juice the metrics, perform the demo, close the round, and each win leaves them more dependent on the next win, more fragile, less capable of surviving the crash.

Elena is betting that finite-game maximizers will burn out before the fog lifts. She's betting on continuation.

### V.

But Elena isn't the only archetype. There's another path through the fog, and it looks nothing like hers.

In early 2025, a Singapore-based startup called Manus launched what they called "the first general-purpose agent." Eight months later, Meta acquired them for north of $2 billion. The speed was disorienting (term sheet to close in ten days) but what's more interesting than the outcome is the path that led to it.

Manus didn't win because the product was perfect. Early users on Chinese social media described an unstable product, burned credits, slow support. The criticism was real. And yet the acquisition happened anyway, at a price that made no sense by any conventional metric.

What Manus understood was the structure of the game they were playing.

### VI.

The Manus playbook moved through four phases, each a finite game deliberately sequenced to serve infinite positioning.

*Phase Zero: Timing and framing.* Manus didn't invent agents, but they went public at the precise moment when agents were emerging from technical demos into market consciousness. The category "general-purpose agent" didn't exist until they named it. They positioned slightly ahead of consensus: not so far ahead that the market couldn't follow, not so aligned with consensus that they'd be lost in the crowd. The timing wasn't controllable, but the framing was. They chose a launch format that matched the current norm while naming a future step.

*Phase One: Scarcity as time-buying.* At launch, Manus didn't try to prove product superiority. They controlled who could judge it. Invite-only access wasn't about manufactured hype; it was about delaying mass criticism while curiosity built. On Reddit, the dominant emotion wasn't rejection but desire"I want access" rather than "this doesn't work." Scarcity reframed early users as participants in something exclusive. While people were trying to get in, they weren't yet dissecting flaws. The finite game was access control; the infinite game was buying time for the product to mature.

*Phase Two: Public trust through presence.* When issues surfaced (credits burning too fast, pricing complaints, refund delays), Manus didn't retreat into private support channels. They showed up publicly on Reddit, replying directly inside complaint threads. The frustration was real, but so was the team's visibility. Global users tolerate imperfect products; what they don't tolerate is silence. Each public reply became a trust asset, a signal that the team was alive and accountable. The finite game was resolving individual complaints; the infinite game was building the credibility that would matter when larger players came looking.

*Phase Three: Product currency as proof engine.* Manus treated credits (the currency users spent to run agents) as a marketing budget, not just a cost center. Instead of traditional advertising, they rewarded users with credits for sharing detailed use cases. The campaigns were tightly aligned with AI economics: agent products are compute-hungry, credits let users explore deeply, and credits-for-stories generated exactly the credible, specific case studies that a lemons market cannot otherwise produce. By December, Reddit was full of long, concrete user stories: self-updating landing pages that signaled real adoption to partners and acquirers evaluating the company.

The sequence matters. Scarcity only works after positioning is set. Public trust only matters after scarcity has created an audience. The proof engine only generates credible stories after trust has established that the stories are worth reading. Each phase is a finite game with clear tactics; the sequence is an infinite-game strategy.

### VII.

Read Manus through the same circles we read Elena.

*Through Circle One (the developer in the fog).* Manus didn't try to close the perception gap; they delayed it. By controlling access, they ensured that early users were self-selected enthusiasts more likely to attribute problems to their own inexperience than to the product. The fog served them. They bought time for the product to improve before mass judgment crystallized.

*Through Circle Two (the lemons market).* Manus's credits-for-stories program was a direct response to the quality-signal problem. In a market where testimonials prove nothing because everyone caught in the perception gap gives glowing reviews, Manus generated a different kind of signal: detailed, specific use cases with enough technical depth to be evaluated. Not "I love this product" but "I built this application, here's how it worked, here's where it failed." The finite game was user-generated content; the infinite game was creating quality signals the market couldn't otherwise produce.

*Through Circle Three (installation-phase capital).* Manus understood they were an option, not an answer. Their GTM wasn't designed to prove the product worked at scale (that question remains unanswerable). It was designed to position them as the option worth buying. The narrative, the timing, the credibility infrastructure: all of it made Manus legible to acquirers as a strategic asset. Meta didn't buy proof that general-purpose agents work; they bought the option on finding out, packaged in a company that had demonstrated it could navigate the fog.

*Through Circle Four (category politics).* Manus named the category before anyone else could. "General-purpose agent" is now a thing because Manus said it was, at a moment when the market was ready to hear it. They didn't wait for industry bodies or regulators to define the category; they defined it themselves, then let competitors position relative to their frame. The finite game was the launch; the infinite game was category ownership.

*Through Circle Five (achievement-subjectivity).* The Manus founders are achievement-subjects like everyone else. They optimized, competed, won. But their GTM reveals something most achievement-subjects lack: awareness of the game's structure. They didn't just chase the next win; they sequenced wins to serve positioning. They understood that Phase One enables Phase Two enables Phase Three, that the sequence compounds in ways that isolated wins don't.

### VIII.

Elena and Manus represent two different paths through the same fog.

Elena's strategy is transparency. She publishes failure rates, accepts finite-game losses, and positions for a future where verification matters. She's betting that the fog will lift, that buyers will eventually learn to read quality signals, that her credibility infrastructure will compound while competitors' Ponzi structures collapse. Her finite games sacrifice short-term wins for infinite positioning.

Manus's strategy is timing. They control when judgment happens, sequence finite wins to build narrative momentum, and position for a future where strategic value matters more than product proof. They're betting that acquirers will pay for options before the fog lifts, that credibility can be manufactured through structure rather than transparency, that the right sequence of wins creates exit velocity before verification becomes necessary. Their finite games accumulate into positioning that's valuable regardless of whether the underlying questions ever get answered.

Both strategies work. Both navigate the fog rather than trying to dispel it. Neither requires the fog to lift for the strategy to succeed.

The difference isn't that one is honest and the other isn't (both involve genuine capability and real execution). The difference is what they're optimizing for. Elena optimizes for being right when the fog clears. Manus optimizes for being positioned before it does. Elena bets on verification. Manus bets on velocity.

And both understand which game they're playing.

### IX.

What separates Elena and Manus from Marcus and Sarah and David isn't success; all of them are winning, by some measure. What separates them is awareness.

Marcus wins sprints while losing skill. Sarah makes defensible recommendations while contributing to a market that cannot learn. David marks up portfolios while feeding a Minsky cycle. Each wins finite games that consume infinite positioning. They optimize for the feeling of winning without tracking what the winning costs.

Elena and Manus also win finite games. But their wins are selected for service to something larger. Elena's losses purchase credibility that compounds. Manus's wins sequence into positioning that attracts strategic capital. Neither is playing for the next quarter. Both are playing for continuation.

The fog makes this distinction invisible to most participants. The finite games are vivid: you can see the closed deal, the marked-up investment, the shipped feature, the completed sprint. The infinite game is silent. It doesn't produce feelings; it produces capacity. Achievement-subjects, tuned to what produces feelings, can't perceive it.

Elena and Manus perceive it because they've built their strategies around it. Their finite-game tactics only make sense if you understand the infinite game they serve. Publishing failure rates is commercial suicide unless you're playing for credibility that compounds. Scarcity at launch is artificial constraint unless you're playing for judgment timing. The tactics reveal the strategy; the strategy reveals the game awareness.

---

## Coda: The Game You're Actually Playing

### I.

Return to Meta's conference room, ten days after the term sheet arrived.

Someone is writing the integration plan. Call her Ana. Manus's technology will merge with Meta's existing AI infrastructure. The 147 trillion tokens processed, the 80 million virtual computers created, the $100 million ARR: all of this will be absorbed into an organization two thousand times larger.

Ana can't answer the question that justified the acquisition: will general-purpose AI agents work at scale? The question is unanswerable from inside the fog. Meta bought an option, not an answer.

But Ana might notice something else. Manus navigated to this conference room through a specific sequence of moves: positioning before the category crystallized, scarcity while the product matured, public presence when trust mattered, proof generation when credibility needed to compound. Each move was a finite game. The sequence was an infinite-game strategy. Manus understood which game they were playing.

Did Meta?

### II.

The finite game is clear: acquire Manus, integrate the technology, ship features, capture market share, win this round of the AI platform competition. Clear objectives, measurable outcomes, definite winners.

The infinite game is different: maintain the capacity to keep playing. Preserve optionality. Build infrastructure that survives the crash. Develop judgment about what works. Stay in the arena long enough for the fog to lift.

The fog obscures the relationship between these games. You can win finite games that serve infinite positioning: Elena publishing failure rates that compound into credibility, Manus sequencing moves that compound into strategic value. Or you can win finite games that consume infinite positioning: Marcus sprinting away his skill, Sarah recommending platforms she can't evaluate, David marking up portfolios in a Minsky cycle.

From inside the fog, both kinds of winning feel the same. The metrics rise, the deals close, the sprints complete. The difference only becomes visible later, when the fog lifts or the crash comes, when it turns out that some wins were building something and others were borrowing against it.

Ana, writing the integration plan, is playing a finite game. She has to; that's what she was hired to do. But somewhere in Meta's organization, someone should be asking whether their finite games serve their infinite game or consume it. Manus knew. Does Meta?

### III.

This is what the fog costs: the inability to know which game you're actually playing.

Every participant in the AI platform market is winning finite games. The developers are closing tickets. The buyers are making recommendations. The investors are marking up portfolios. The vendors are shipping features. The metrics all point up and to the right.

And no one knows whether these wins are accumulating into something durable or consuming the conditions for durability. No one knows whether the finite games serve the infinite game or sacrifice it. No one knows because the fog is precisely the condition of not-knowing, and the fog is produced by the same dynamics that produce the finite wins.

The perception gap means developers can't tell if their productivity serves long-term skill development. The lemons market means buyers can't tell if their purchases serve market learning. The Minsky dynamic means investors can't tell if their returns serve durable value creation. The achievement-subjectivity means no one can tell if their wins serve anything beyond the feeling of winning.

This is not a temporary condition that better measurement will solve. The fog is structural. It's produced by the same forces that produce the market. Disperse it in one place and it regenerates in another.

### IV.

So what do you do?

Start by recognizing which game you're actually playing. Most people think they're playing the infinite game while optimizing for next quarter. They talk about "long-term value" while chasing metrics that sacrifice sustainability. Be honest: which game are your actual decisions serving?

Then notice when finite games start consuming infinite games. The developer who stops debugging because velocity is high. The buyer who stops evaluating because decisions are defensible. The investor who stops questioning because markups are strong. These are the moments when finite wins eat infinite optionality.

Find the people who understand the game structure. Elena, publishing failure rates, betting on verification. Manus, sequencing finite wins, betting on positioning. Their strategies are different; their awareness is the same. Both have built their tactics around an infinite game that most participants can't perceive. Study their moves. The tactics only make sense if you see the larger game they serve.

Cultivate what the fog destroys: attention, patience, the willingness to wait without generating, the ability to evaluate quality you didn't produce. These capacities atrophy under AI assistance and achievement pressure. They're also what you need to perceive the infinite game at all.

And remember that the fog is political. The categories aren't natural; they're contested. "Enterprise-ready" means what the winners of the category battle say it means. Manus won that battle early by naming "general-purpose agent" before anyone else could. You can participate in that contest. You can shape what "working" means. You don't have to accept the definitions that serve the fog.

### V.

The sprint is not the point. The deal is not the point. The markup is not the point.

These are finite games you play to stay in the arena. The point is staying there, preserving the capacity to keep playing.

The fog makes this hard to see. Everyone around you is optimizing for finite wins. The metrics reward finite wins. The interfaces produce the feeling of finite wins. The pressure is always toward the next win, the next quarter, the next round.

The people who will matter when the fog lifts are the ones who never forgot which game they were playing.

### VI.

Ana finishes the integration plan.

Somewhere in Singapore, the Manus founders prepare for their new reality, the reality their four-phase playbook was designed to reach. Somewhere in San Francisco, Marcus pushes another AI-generated commit, winning a sprint he'll pay for later. Somewhere in Manhattan, Sarah's recommendation lands on partners' desks, defensible but not verified. Somewhere in a seed-stage office, Elena publishes another failure rate, losing a deal that would have cost her more than it paid.

Each is playing finite games. Each is winning, by some measure.

What separates them is whether they know which game they're actually playing. Manus knew. They sequenced their finite wins deliberately, built narrative momentum that served strategic positioning, navigated to an exit that validated the sequence. Elena knows. She accepts finite losses that purchase infinite credibility, builds verification infrastructure while competitors build castles on sand.

Marcus doesn't know. Sarah doesn't know. David doesn't know. They're winning finite games while the infinite game slips away, and the fog prevents them from seeing the difference.

Ten days was plenty of time to close an acquisition. It was not enough time to know whether the acquisition served Meta's infinite game or consumed it.

Knowing takes patience, attention, the willingness to step back from the finite game long enough to see the larger structure. There's more than one path through the fog: Elena's transparency, Manus's timing, and probably others we haven't seen yet. But all the paths start with the same awareness: the sprint is not the point.

Most people never figure it out. The fog is designed to prevent it.

Elena figured it out. Manus figured it out.

---

## Sources

- Akerlof, George. "The Market for 'Lemons': Quality Uncertainty and the Market Mechanism" (1970)
- Bateson, Gregory. *Steps to an Ecology of Mind* (University of Chicago Press, 1972)
- Carse, James P. *Finite and Infinite Games* (Free Press, 1986)
- Douglas, Mary. *Purity and Danger* (Routledge, 1966)
- Han, Byung-Chul. *The Burnout Society* (Stanford University Press, 2015)
- Kahneman, Daniel. *Thinking, Fast and Slow* (Farrar, Straus and Giroux, 2011)
- METR. "Measuring the Impact of Early Exposure to AI on Experienced Open-Source Developer Productivity" (2025)
- Minsky, Hyman. *Stabilizing an Unstable Economy* (Yale University Press, 1986)
- Perez, Carlota. *Technological Revolutions and Financial Capital* (Edward Elgar, 2002)
- Scott, James C. *Seeing Like a State* (Yale University Press, 1998)
- Weil, Simone. *Gravity and Grace* (Routledge, 1952)
- Bloomberg: "Meta acquires startup Manus to bolster AI business" (December 29, 2025)
- Choudhary, Sangeet Paul. ["A healthy infinite game optimizes for continuation with optionality"](https://www.linkedin.com/posts/sangeetpaul_jan-1-2026-a-healthy-infinite-game-optimizes-activity-7412336678117314560-4cU9) (LinkedIn, January 1, 2026)
- Keec, Coni. ["The Fog of Code"](https://conikeec.substack.com/p/the-fog-of-code) (Substack)
- Mao, Jaye. "When 'This Product Is Dead' Turns Into a $2B Exit" (AI Product GTM, 2026)
