{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ashvin Parameswaran","text":"<p> About On Writing </p> <ul> <li> <p>Recent</p> <ul> <li>The Decision Loop \u00b7 Feb 2026</li> <li>The Replacement Rate \u00b7 Feb 2026</li> <li>The Phantom Limb Economy \u00b7 Feb 2026</li> <li>MOVE: Metrics for the AI-Native Organization \u00b7 Feb 2026</li> <li>The AI Capability Map: An Expanded Inventory \u00b7 Feb 2026</li> <li>Reading After Readers \u00b7 Feb 2026</li> </ul> <p></p> <p>Archive \u2192</p> </li> </ul>"},{"location":"about/","title":"Ashvin Parameswaran","text":"<p>I lead the Emerging Technology group at ReadyTech, working with engineers in AI, infrastructure, and developer relations. We build the foundational platforms, the frontier agentic products, and the governance that ensures it can be trusted. I also drive the go-to-market activities that scale and commercialise this technology.</p> <p>I build for B2B vertical industries and institutions where failure isn't an option\u2014education, workforce, work pathways, government, and justice. These are the systems communities rely on every day, and AI here has to be both powerful and responsible.</p> <p>Earlier, I co-founded a startup and led teams in universities and industry. Each role taught me the same lesson: spend time with users, cut through complexity, and scale what works.</p> <p>I think of AI less as a product wave and more as a new layer of civic infrastructure. It should be as reliable, invisible, and trusted as water or power.</p> <p></p> <p>      LinkedIn         GitHub    </p>"},{"location":"on-reading/","title":"On Reading","text":"<p>Reading was never one thing. It was a stack: processing, summarising, pattern-matching, interpreting, judging. Bundled together because humans couldn't separate the layers. AI unbundles it. The lower layers become infrastructure. What remains is context the model doesn't have and judgment it can't be accountable for.</p> <p>Some days I steer and the machine augments. Other days the boundary dissolves and I can't tell which insight was mine three prompts ago. Both ways of reading produce thoughts the other can't reach.</p> <p>What survives is the useless reading. Rereading a passage for the fifth time because something won't resolve. Reading that goes nowhere. The gnarled tree the lumber industry passes by. It persists because no one is trying to capture it.</p> <p>We're not losing reading. We're gaining readings. The question is whether we'll be deliberate about what the new ones produce.</p>"},{"location":"on-working/","title":"On Working","text":"<ul> <li>Open five terminals at once and personalise each with a name, colour and icon. Have standard icon and colour legends for General, Meta, Blog and Spec.</li> <li>Write up specs the night before or have implementations run overnight</li> </ul>"},{"location":"on-writing/","title":"On Writing","text":"<p>Writing is thinking made visible, and thinking changes, so the writing changes.</p> <p>I return to everything here, stitching, snipping, reworking. Nothing is archive. If you read something last month and it reads differently now, that's not error.</p> <p>I write with a golem now. It takes what I mean and what it offers and produces a third thing, neither mine nor not mine. This is how writing works now. The tool is part of the practice.</p> <p>What's happened to writing is what happens to every practice when the underlying economics shift. The old package (solitary author, fixed text, finished product) has come apart. What's reassembling is something else: collaborative, iterative and alive. </p> <p>The golem and I have an arrangement. It keeps the work open and I keep asking what's mine.</p> <p>You might have one as well because the reader changed when the writer did.</p>"},{"location":"blog/","title":"Archive","text":""},{"location":"blog/2025/10/20/beyond-the-flywheel/","title":"Beyond the Flywheel","text":"<p>For twenty years, we've lived with a particular story about how digital growth works\u2014the flywheel spinning faster with each turn, users attracting more users, data improving services which draw more data; it's a narrative that explained why Amazon felt inevitable, why Facebook seemed unstoppable, why Uber could burn billions and still look like the future. The flywheel wasn't entirely wrong, of course. It captured something real about network effects and compounding advantages; it gave us services that were genuinely transformative, at least for a while. Yet what looked like perpetual motion from one angle, we now see, looked like extraction from another.</p> <p>Through bitter experience\u2014and here the bitterness is earned\u2014we've discovered how every flywheel eventually slows. Growth stalls, cheap money evaporates, and platforms begin their predictable metamorphosis: raising fees, cramming in advertisements, prioritizing pay-to-play over merit, throttling APIs that once welcomed developers, degrading the very services that drew us in. There's a name for this pattern (Cory Doctorow gave us the memorable if inelegant term \"enshittification\"), but the phenomenon predates the nomenclature: it's the steady, almost mechanical shift of value away from users and toward quarterly targets, a process as reliable as entropy itself.</p> <p>Consider the recent history\u2014Reddit hiking API prices so precipitously that Apollo, beloved by millions, simply shut down; Twitter (now X) killing free API access entirely, breaking the third-party ecosystem that made it interesting; Amazon defaulting Prime Video to include advertisements unless you pay extra for what you already had; Instagram testing an interface that opens directly to Reels, burying the photo feed that was its original purpose; Google's first page now so dense with ads, AI summaries, and sponsored links that people append \"reddit\" to queries just to find actual human conversation. Each move represents the same trade: user experience for shareholder value, community for commodity.</p> <p>But the platforms' extraction goes deeper than inconvenience. Facebook dismantled local news ecosystems without hiring a single reporter; Uber disrupted century-old taxi systems, replacing worker protections with surge-priced precarity; Google trained its AI on everyone's content, then served summaries that bypass creators entirely. They privatized our town squares\u2014spaces where we gather, argue, learn, and connect\u2014then left society with the bill: anxious teenagers scrolling through impossible standards, gig workers without benefits or bargaining power, democracies fracturing along algorithmic fault lines. The flywheel's genius, we now understand, wasn't innovation but extraction: move fast, break public goods, capture the value, externalize the costs.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#the-infrastructure-alternative","title":"The Infrastructure Alternative","text":"<p>This pattern matters urgently because we stand at what historians will recognize as a choice point. The degradation of platforms coincides\u2014not coincidentally\u2014with the emergence of artificial intelligence as a general-purpose technology. The question before us isn't whether AI will reshape society; that's already happening. The real puzzle is what kind of reshaping we'll permit, what values we'll encode, whose interests will predominate. We can treat AI as another extraction machine optimized for next quarter's earnings call, or we can build it as computational infrastructure for the next century: something that, like roads or water systems or law itself, holds under strain, expands human capability, and serves everyone (not equally perhaps\u2014infrastructure rarely does\u2014but at least intentionally, publicly, accountably).</p> <p>The distinction between platform and infrastructure isn't merely semantic. Platforms extract; infrastructure enables. Platforms optimize for shareholders; infrastructure optimizes for society. Platforms can pivot, shut down, or radically alter their terms whenever the business model demands it; infrastructure commits to continuity, to being there tomorrow and the day after, boring perhaps but reliable. This is nation-building work\u2014not algorithms alone, but the workforce to understand them, the trust to implement them, the technology stack to run them independently, and the economic structures to sustain them. These are foundations on which everything else can flourish.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#who-controls-the-architecture","title":"Who Controls the Architecture","text":"<p>When we think about computational infrastructure, the immediate question becomes one of control\u2014though control is perhaps too simple a word for what we're discussing. Every automated decision shifts agency from humans you can argue with to systems you can't; every algorithmic mediation interposes code between intention and outcome. Search results that once offered ten sources for you to evaluate now provide one definitive answer (often wrong, occasionally dangerously so). Loan decisions once made by bankers you could appeal to\u2014however imperfectly\u2014now emerge from models that offer no recourse, no explanation beyond a score.</p> <p>Soon\u2014and by soon I mean within the decade\u2014it won't just be decisions but actions: agents trading, hiring, negotiating, building, all with authority we grant but cannot revoke transaction by transaction. They'll move faster than oversight can follow, coordinate in ways that exceed human comprehension, create facts on the ground before anyone can object. (The flash crash of 2010, when algorithms knocked a trillion dollars off the market in minutes, was merely a preview.) We're not building tools in any traditional sense of that word; we're building decision infrastructure, the computational substrate through which collective intelligence emerges and coordinated action happens.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#from-physical-to-computational-sovereignty","title":"From Physical to Computational Sovereignty","text":"<p>The history of sovereignty offers instructive parallels. In the industrial age, sovereignty meant controlling physical infrastructure: ports that could be blockaded, railways that could be severed, energy grids that could be defended or destroyed. In the post-industrial period, it meant controlling information flows\u2014broadcast towers, submarine cables, satellite links. Now we're entering something qualitatively different: an era where intelligence itself becomes infrastructure, where the capacity to decide, to act, to coordinate at machine speed becomes the foundation on which everything else depends.</p> <p>Look at the societies that have actually lasted\u2014not the flash-in-the-pan empires but the civilizations that endure. They're built not on compounding loops but on the strength of their social fabric, the robustness of their institutions, the redundancy of their critical systems. Singapore articulates this as Total Defence: military, civil, economic, social, psychological, and now digital\u2014six interlocking layers of resilience. The Nordic countries speak of Strong Societies, where trust and cohesion matter more than any fortress, where social infrastructure proves more durable than concrete. Switzerland maintains its sovereignty not through military might but through calculated neutrality backed by universal preparedness. The lesson recurs: sovereignty isn't one moat or one product or one brilliant algorithm; it's the integrated strength of people, institutions, and infrastructure working in conscious coordination.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#the-four-pillars","title":"The Four Pillars","text":"<p>This understanding suggests we need to approach AI at the level of civilizational infrastructure\u2014not as a quarterly growth engine to be optimized and extracted from, but as foundational capacity that must be cultivated, maintained, and renewed across generations. Like any critical infrastructure, it requires redundancy, maintenance, workforce development, and governance structures that outlast political cycles. The framework that emerges has four essential pillars:</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#workforce-the-human-gradient","title":"Workforce: The Human Gradient","text":"<p>AI will eliminate jobs\u2014many of them, across every sector, faster than most people expect. But that's not the whole story, or even the most interesting part. Every transformative technology kills certain skills while birthing others; telegraph operators vanished, but network engineers emerged; typists disappeared as word processors appeared; filing clerks gave way to database administrators. Today's pattern follows the historical template but accelerates it: programmers spend less time writing code, more time reviewing what AI generates; doctors spend less time on diagnosis, more time on difficult cases and human connection; lawyers draft less, negotiate more. The skill migrates upward\u2014from production to supervision, from execution to judgment.</p> <p>The danger (and it's a genuine danger, not mere Luddite anxiety) is that we drift from \"humans in the loop\" to \"humans on the loop\"\u2014from active engagement to passive approval, from understanding to rubber-stamping. When expertise atrophies from disuse, when the next generation never learns the fundamentals because the machine always handles them, we risk what some theorists call \"competence collapse.\" The U.S. Navy, recognizing this, brought back celestial navigation training after years of GPS dependence\u2014not because everyone needs a sextant, but because someone must know how to find their way when satellites fail.</p> <p>This suggests we need what might be called reserve competence: enough humans maintaining deep understanding to catch failures, to know when the model has lost the plot, to rebuild when necessary. It means reskilling programs that go beyond compliance theater\u2014actual education that prepares people for the jobs that will exist, not the ones that used to. It means safety nets that actually catch people, not the bureaucratic pantomime we often settle for. And critically, it means preserving and cultivating what remains irreplaceably human: taste, judgment, the ability to wrestle with ambiguity, the wisdom to know when the machine is confidently wrong, the courage to override the algorithm when humanity demands it.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#trust-the-social-contract","title":"Trust: The Social Contract","text":"<p>Responsible AI without enforcement is just performance art\u2014elaborate principles that sound wonderful in conference keynotes but evaporate when quarterly pressures mount. If citizens must live under systems they cannot challenge, appeal, or even understand, legitimacy collapses; if decisions that shape lives remain opaque, unaccountable, unreviewable, then we're building algorithmic authoritarianism regardless of our intentions.</p> <p>Trust requires more than transparency (though transparency helps); it requires genuine accountability with real consequences. It means audits with teeth\u2014not checkbox exercises but genuine investigations that can shut down systems that violate public trust. It requires rights of redress that actually function, not complaint forms that disappear into digital voids. It means confronting failures in public rather than burying them under non-disclosure agreements and PR statements. When San Francisco's crime prediction algorithm was found to perpetuate racial bias, when the Netherlands' child benefit system wrongly accused thousands of families of fraud, when Amazon's hiring algorithm systematically disadvantaged women\u2014each failure was initially hidden, denied, minimized. Trust dies in darkness.</p> <p>But here's the harder truth: regulation must be shielded from capture. When the people writing the rules are planning their next job at the companies they're regulating, when the technical expertise sits entirely on industry's side, when lobbying budgets exceed regulatory budgets by orders of magnitude, the whole system becomes theater. We need regulators with genuine independence\u2014not just on paper but in practice; real technical capacity\u2014people who understand these systems deeply enough to challenge them; and the authority to act when things go wrong\u2014not just to write strongly-worded letters but to shut down operations that violate public trust.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#technology-the-stack-of-sovereignty","title":"Technology: The Stack of Sovereignty","text":"<p>A nation that depends entirely on external platforms might believe it's purchasing efficiency\u2014and in the short term, it might be right. But in truth, it's mortgaging sovereignty, trading independence for convenience, accepting dependency as the price of not having to build. The uncomfortable fact (uncomfortable because it challenges the comfortable assumptions of globalization) is this: to rent your stack is to rent your future.</p> <p>Consider what dependency actually means in practice. If you cannot walk away from a vendor in thirty days\u2014really walk away, with your data, your processes, your operations intact\u2014then you're not a customer but a colony. Every dependency that cannot be broken isn't infrastructure but submission dressed in the language of partnership. When critical decisions about your citizens are made by systems you don't control, can't audit, can't modify, then you've outsourced not just technology but sovereignty itself.</p> <p>This doesn't mean autarky\u2014building everything from scratch, rejecting all foreign technology, pursuing some impossible ideal of self-sufficiency. The global division of labor exists for good reasons; nobody makes a pencil from scratch anymore, as Leonard Read famously observed. But it does mean maintaining what might be called sovereign optionality: ensuring you can switch when necessary, that you understand how critical systems work, that you have alternatives when relationships sour or interests diverge. The test is simple but severe: can you leave? If not, you don't own your infrastructure\u2014you're renting it, and rent, as any tenant knows, always goes up.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#economy-the-velocity-of-decision","title":"Economy: The Velocity of Decision","text":"<p>It's comforting to say AI will grow the economy\u2014productivity gains, efficiency improvements, new industries emerging. All likely true. The harder truth is that AI won't just grow the economy; it will run it, at speeds that make high-frequency trading look leisurely. Agents will approve loans, set prices, negotiate contracts, allocate credit, coordinate supply chains\u2014all moving faster than human oversight can match. In this world, decision-power itself becomes currency; control over these systems constitutes economic power in its rawest form.</p> <p>If those decisions are made by systems controlled elsewhere\u2014if the algorithms that determine creditworthiness, set interest rates, allocate investment, and coordinate markets are owned and operated beyond your borders\u2014then you haven't just outsourced technology. You've outsourced command of your economy itself, accepted a form of algorithmic colonialism that's all the more powerful for being invisible.</p> <p>Think about what that means concretely. When an algorithm somewhere else decides who gets capital, at what price, on what terms, that's not a technology question anymore\u2014it's a sovereignty question. When foreign systems determine which businesses can access credit, which sectors receive investment, which regions get development, you're no longer running your own economy; you're asking permission to participate in someone else's. The cognitive division of labor that has characterized global capitalism for decades gets cranked to an unprecedented extreme.</p> <p>Economic sovereignty in the age of AI isn't about controlling every transaction\u2014that's neither possible nor desirable. It's about preserving human judgment over the transactions that define your economy's character: the exceptions that set precedents, the edge cases that reveal values, the crucial decisions about what kinds of risks to underwrite and what kinds of futures to fund. Let the machines handle the routine; keep humans in charge of what matters.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#building-for-generations","title":"Building for Generations","text":"<p>What becomes possible when we think of AI as infrastructure rather than product? Consider education, where teachers currently drown in compliance paperwork while students slip through cracks\u2014infrastructure could handle the bureaucracy while surfacing exactly which students need help, when, and why. Or healthcare, where physicians spend more time with electronic records than patients\u2014infrastructure could manage the documentation while doctors focus on the human being in front of them. Or justice, where algorithms already influence bail and sentencing\u2014infrastructure could surface patterns across decades of cases while preserving judicial discretion over individual lives.</p> <p>The examples multiply: local councils that can fix potholes and approve permits faster than committees can convene, but where citizens still control budgets and priorities; employment services that match people to actual careers rather than gaming placement metrics for government contracts; small businesses that can access the same analytical capabilities as multinationals without surrendering their data to platform monopolies. Not extraction dressed as innovation, but genuine infrastructure that amplifies human judgment while operating at the speed and scale modernity demands.</p> <p>The easier path\u2014and clarity requires admitting it is easier\u2014is to rent everything, optimize for the quarter, let platforms extract value until they inevitably degrade. That path is not just well-worn but well-lit, with consultants and vendors eager to guide you down it. The returns are quick, the risks are hidden, and by the time the real costs come due, you'll probably have moved on to the next role.</p> <p>The harder path means building when buying would be simpler, thinking in decades when quarters are what get measured, treating AI as infrastructure to be maintained rather than software to be subscribed to. It means accepting that infrastructure is thankless work\u2014when it succeeds, it disappears into the background; when roads work, nobody credits them for the economy they enable; when power grids function, nobody celebrates the lives they improve. They just work, year after year, holding up everything else.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/10/20/beyond-the-flywheel/#the-stakes-of-stewardship","title":"The Stakes of Stewardship","text":"<p>But here we must be honest about a final complexity\u2014infrastructure is only as good as the humans who maintain it. If we drift from \"humans in the loop\" to \"humans on the loop,\" from active engagement to passive supervision, we risk what Sartre called \"bad faith\"\u2014pretending we have no choice when we're actually choosing not to choose. The machine's efficiency becomes our excuse for abdication.</p> <p>The hardest part isn't the technology, which will largely build itself given sufficient resources and talent. The hardest part is maintaining our agency: staying sharp enough to supervise systems that rarely fail, wise enough to intervene when intervention seems unnecessary, and human enough to know which skills we can't afford to lose. As Appiah reminds us in another context, \"if there's one skill that matters above all others, it's the skill of knowing which of them matter\"\u2014and that metacognitive capacity, that judgment about judgment itself, may be the one thing we must never outsource.</p> <p>The platforms taught us that extraction is temporary\u2014sooner or later, you run out of users to squeeze, trust to violate, commons to enclose. But infrastructure compounds differently: every year it stands, it enables more; every crisis it survives, it proves its worth; every generation that inherits it functional inherits possibility itself. The work of building it is hard and the returns are slow, but what gets built becomes the ground on which everything else stands.</p> <p>That's not a quarterly story or even an electoral cycle story\u2014it's how civilizations last. The choice before us isn't whether to build AI (that train has left the station) but what kind of AI infrastructure we'll build: extractive or enabling, proprietary or public, foreign or sovereign, fragile or resilient. The flywheel promised endless acceleration but delivered eventual degradation. Infrastructure promises something less dramatic but more durable: systems so reliable they become invisible, so foundational they expand what's possible for everyone who uses them, so thoughtfully maintained that our grandchildren will take them for granted\u2014which is, perhaps, the highest compliment one generation can pay another.</p>","tags":["ai","sovereignty","infrastructure","governance"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/","title":"Cathedrals versus Commons","text":"<p>The best pranks take ten thousand years to set up.</p> <p>That's what the Dwellers do in Banks' The Algebraist: beings who live for eons in gas giants, playing elaborate jokes on each other because when you have infinite resources, reputation through novelty is all that matters. One Dweller breeds a sentient species just to have an audience for their poetry. Another spends centuries setting up a pun.</p> <p>They would find our current situation hilarious.</p> <p>Consider the hospital administrator's daily reality: You're running critical infrastructure on OpenAI's API while your radiologists use sketchy Discord models on the side. Your lawyers demand compliance certificates. Your engineers contribute to repositories that will make your product obsolete. Your board wants quarterly growth. Your mission statement talks about serving humanity. Three different economic games, incompatible rules, everyone pretending this is normal.</p> <p>The real puzzle: what kind of chaos we're dealing with. In physics, three bodies orbiting each other create a specific problem: no stable solution, just permanent dynamic instability. The system never settles. It never resolves. It just continues, each body pulling the others in incompatible directions.</p> <p>That's exactly where we are with AI. Not a simple binary, cathedrals versus commons, extraction versus sharing, but something more vexing.</p> <p>Extraction logic says monetize scarcity. OpenAI turned matrix multiplication into a $100B valuation by adding safety theater and enterprise SLAs. Beautiful. Pays the bills. Funds the yachts. Cathedral builders with their digital priesthoods, explaining why you need their blessing to run inference. \"Too dangerous to release,\" they say, while teenagers jailbreak it with ASCII art.</p> <p>Performance logic says earn attention through novelty. This is the Dwellers' game\u2014when nothing is scarce, status flows to whoever's most interesting. It's already here: GitHub stars, citation counts, Twitter ratios. Researchers racing to publish. Engineers building in public. That kid who made GPT-4 think it was a pirate and got 10,000 retweets. Value doesn't flow to ownership but to whoever makes the timeline laugh.</p> <p>Service logic says forget both games and help humans. This is what actually runs hospitals, schools, courts. Not profit (extraction) or pranks (performance) but purpose. A nurse doesn't need viral tweets or stock options. They need patients to heal. The value isn't captured or performed\u2014it's created through giving a damn.</p> <p>What looks like a simple problem from one angle (choosing between profit and purpose) reveals itself as something else entirely when you step back. You can't serve all three masters without betraying two. That's not a management challenge. That's physics.</p> <p>Open-source your tools (performance) and shareholders scream about competitive advantage (extraction). Focus on profits (extraction) and watch your best engineers leave for companies with better GitHub profiles (performance). Optimize for efficiency (extraction) and compromise the care that made you start this company (service).</p> <p>The companies that matter, the ones digitizing courts, powering schools, keeping hospitals from collapsing, they're not choosing. They're juggling chainsaws while solving differential equations on a tightrope during an earthquake. Playing extraction to fund operations, leveraging performance to attract talent, maintaining service because that's why they exist.</p> <p>And then AI arrives and accelerates everything.</p> <p>Models leak faster than companies can capture them. Every wall becomes Swiss cheese. Meta gives away Llama just to watch OpenAI squirm. Chinese companies drop open weights that match GPT-4 because why not. Teenagers fine-tune models to be maximally unhinged and share the LoRAs with friends. The Dwellers would approve\u2014it's all becoming very silly.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#from-two-body-to-three-body-chaos","title":"From Two-Body to Three-Body Chaos","text":"<p>We thought this was a two-body problem. The pattern seemed familiar enough: cathedrals versus commons, extraction versus sharing, enclosure versus openness. OpenAI builds walled gardens; Meta releases Llama. Microsoft licenses everything; Hugging Face distributes everything. David versus Goliath, if Goliath were a language model and David a kid with a gaming rig.</p> <p>It's a story we've told before. The music industry built cathedrals around CDs; Napster tore them down. Film studios erected paywalls; BitTorrent tunneled under them. Software companies sold licenses; open source gave them away. Each time, the same arc: proprietary control gives way to distributed abundance, monopolies dissolve into commons, the cathedral crumbles and the bazaar flourishes.</p> <p>That's the upbeat scenario, anyway. The Dwellers would find it charming\u2014these mortals, thinking they've seen this movie before.</p> <p>This isn't a repeat. The question is what's different this time. The history we're relying on was always incomplete. We mapped two forces and called it physics. Cathedral builders extracting rents versus commons builders earning kudos. Clean. Binary. Wrong.</p> <p>There's a third body nobody mentioned: Service. The boring one. The one that keeps humans alive. While we spent years debating monetization versus memeification, extraction versus reputation, priests versus pranksters, someone still needed to run the hospitals. Teachers still needed to educate children. Courts still needed to dispense justice. These institutions don't operate on extraction logic or performance logic. A nurse doesn't need GitHub stars or quarterly returns. They need patients to survive and software that doesn't crash during surgery.</p> <p>This changes everything. Two-body problems find stable orbits\u2014one wins, or they balance. Three bodies create chaos with no stable solution. Every decision breaks two other systems, every optimization causes new problems. Nothing ever resolves.</p> <p>Everyone's navigating all three at once. OpenAI talks extraction to investors, performance to researchers, service to regulators: three different stories, all true, all incompatible. Meta weaponizes openness (that's not kudos, it's warfare) while extracting through ads. Hospitals serve patients while private equity extracts value while competing for talent through performance. Three-dimensional chess. Everyone pretending it's checkers.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#the-three-value-systems","title":"The Three Value Systems","text":"<p>Economics has three incompatible physics. Like gravity, magnetism, and whatever dark energy is doing. They don't play nice.</p> <p>Extraction logic is gravity\u2014heavy, inevitable, pulls everything toward money.</p> <p>OpenAI turned matrix multiplication into $100B. It's beautiful. They took something that's basically fancy Excel, added safety theater (\"too dangerous to release!\"), enterprise SLAs (\"someone to sue!\"), and boom: digital priesthood. Cathedrals everywhere. Stark and Bainbridge would applaud; they're selling compensators for things that don't need compensating. \"This model is aligned\" (you can't verify this). \"Trust our safety team\" (who chose them?). The logic is brutal: find something people need, build a moat, charge rent. It's what pays your mortgage.</p> <p>Performance logic is magnetism\u2014pulls attention, creates fields of weird.</p> <p>This is what the Dwellers figured out after a million years: when nothing is scarce, you compete on being interesting. The kid who made GPT-4 speak only in haikus got 50,000 GitHub stars. Researchers literally race to ArXiv at midnight to claim priority. Engineers building their entire careers in public, every commit a performance. It's not about owning but about others wanting to pay attention. Value flows to whoever makes immortals laugh. Or at least makes Twitter laugh, which is close enough for mortals.</p> <p>Service logic is dark energy\u2014mysterious, essential, constantly fighting the other two.</p> <p>This is what actually makes hospitals work. What keeps schools teaching. What prevents courts from collapsing into chaos. Not profit margins or viral tweets, just humans giving a damn about other humans. A nurse doesn't need kudos or revenue. They need patients to heal. The value isn't captured or performed; it's created through consistent, unsexy care. It's the physics nobody talks about at TED but everyone depends on to not die.</p> <p>Service operates on different time scales. Extraction wants quarterly returns, performance wants viral moments, service wants generational impact. A good teacher's work shows up 20 years later when their student becomes a doctor. You can't A/B test compassion. Care doesn't scale linearly. Quality often inversely correlates with efficiency metrics.</p> <p>Service creates different dependencies: not vendor lock-in (extraction) or audience capture (performance), but trust networks, community knowledge, and institutional memory. These can't be forked, purchased, or performed. Service logic is why that one nurse who knows everyone's name matters more than the million-dollar EMR system. It's why the school janitor who's been there 30 years IS the institutional memory. These humans aren't performing or extracting; they're holding civilization together with consistent, unsexy care.</p> <p>Every organization is being ripped apart by these three forces simultaneously.</p> <p>Picture a public company serving critical infrastructure. Monday: extraction logic for the earnings call. Tuesday: performance logic to recruit that Stanford PhD. Wednesday: service logic because someone actually needs help. Thursday: extraction again because payroll is due. Friday: performance because your best engineer is building a competitor unless you let them open-source something. Weekend: service because the mission statement says you care.</p> <p>It's like being drawn and quartered, but in three dimensions, and everyone's livestreaming it.</p> <p>The systems weaponize each other: Everyone's using one system against the others.</p> <p>Meta \"gifts\" Llama to the world. How generous! Except it's not performance (they don't want kudos); it's extraction warfare to burn down everyone else's moat and protect their advertising empire. \"If we can't monopolize AI, nobody can.\" This isn't participating in any kudos economy; it's using \"free\" as ammunition within pure extraction logic. Zuck learned from mobile platforms: never again be someone else's tenant. So now he's torching the whole neighborhood while pretending it's philanthropy.</p> <p>Google open-sources everything. Not for reputation, but to make sure search stays profitable while everything else burns. They're not seeking approval from the Dwellers or building reputation through novelty. Every \"gift\" to the commons is actually a competitive move to prevent anyone from building a moat that might threaten their core extraction machine. Universities demand publication (performance) while patenting everything worth money (extraction). Hospitals advertise patient care (service) while private equity guts them for parts (extraction).</p> <p>Everyone's playing 3D chess while pretending it's checkers. The Dwellers would find this hilarious: all these serious companies with serious valuations, trying to navigate incompatible physics while pretending they have a strategy. Like watching someone try to serve three gods who hate each other (at machine speed, with quarterly earnings calls).</p> <p>AI is pouring gasoline on this comedy. Information wants to be free (physics favors performance). But capitalism needs scarcity (economy demands extraction). And humans need systems that actually work (society demands service). The three-body problem isn't stabilizing. It's going fractal.</p> <p>The Navigation Challenge</p> <p>You're not choosing between systems. You're surfing between them:</p> <ul> <li>Extraction pays your bills but kills your soul</li> <li>Performance attracts talent but doesn't pay rent</li> <li>Service fulfills purpose but won't fund itself</li> </ul> <p>Every organization is being pulled apart by incompatible physics. The question isn't which one to choose\u2014it's how to navigate the chaos.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#the-straightjacket-paradox","title":"The Straightjacket Paradox","text":"<p>The bind: You run a company that matters. Maybe you digitize courts, or power schools, or keep hospitals running. The infrastructure society actually needs. You're not selling ads or optimizing engagement. You're doing the boring, critical work that keeps civilization functioning.</p> <p>You're also trapped in cathedral economics. Listed on an exchange, or venture-backed, or just trying to make payroll. You have shareholders who expect returns. Customers who demand compliance certificates. Regulators who require audit trails. You can't just open-source everything and trust the universe. You have quarterly earnings calls where \"we're building toward the kudos economy\" doesn't fly.</p> <p>But you can feel the shift coming. You see your brightest engineers contributing to open source on weekends. Your customers' kids are running local models that outperform your licensed APIs. The abundance is leaking in through every crack, and you know cathedral logic has an expiration date. You just can't afford to be first.</p> <p>This is most companies' reality. Caught between economic systems. Playing cathedral because that's what pays the bills, while knowing kudos is where value is moving. It's like being a travel agent in 1995, watching people discover Expedia. You know what's coming. You just have mortgages to pay in the meantime.</p> <p>You can lean toward kudos while wearing the cathedral costume. Open source your non-core tools. Build in public where you can. Share knowledge that doesn't compromise competitive advantage. Create internal kudos economies\u2014reward employees for teaching, for sharing, for making others better. Run hackathons. Fund fellowships. Build APIs that others can build on.</p> <p>You're not abandoning cathedral economics\u2014you can't afford to. But you're building kudos muscle memory. Training your organization to function in abundance logic. Creating optionality for when the flip happens. Because it will happen. The only question is whether you'll be ready to surf it or get crushed by it.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#navigation-strategies-the-art-of-surfing-chaos","title":"Navigation Strategies (The Art of Surfing Chaos)","text":"<p>You can't solve a three-body problem. You can only surf the chaos. Here's how organizations are learning to juggle flaming chainsaws while riding unicycles on tightropes.</p> <p>Strategy 1: The Octopus Play</p> <p>Smart companies aren't choosing; they're growing tentacles. Core operations run on extraction (pays rent), innovation labs run on performance (attracts talent who'd otherwise start competitors), and community programs run on service (keeps the pitchforks away). Each tentacle serves a different god, and somehow the octopus survives.</p> <p>Healthcare platforms do this: Enterprise contracts with insurance companies (extraction), because money. Open-source data visualization tools (performance), because their engineers threatened to quit. Free clinics in underserved communities (service), because someone remembered why they started this company. It's like maintaining three separate personalities, but at least the board is confused enough to approve everything.</p> <p>Strategy 2: The Time-Lord Shuffle</p> <p>Some organizations are literally time-traveling between value systems. They're not serving all three masters simultaneously; they're cycling through dimensions like Doctor Who with an MBA.</p> <p>Monday through Thursday: Pure extraction. Sales calls, feature bloat, enterprise theater. Everyone wears suits and says \"synergy.\"</p> <p>Friday: Performance day. Engineers finally allowed to commit to open source. Researchers racing to publish before competitors scoop them. Twitter threads about technical breakthroughs.</p> <p>Weekends: Service mode. Hackathons for nonprofits. Free workshops. Actually helping humans.</p> <p>It's exhausting. Everyone needs therapy. But it works until someone realizes they're living three different lives.</p> <p>Strategy 3: The Platform Paradox</p> <p>Build APIs and let others fight the three-body problem. Become the Switzerland of software: it's strategic genius.</p> <p>Stripe figured this out: Charge transaction fees (extraction), but make developers love you (performance), while enabling entire economies (service). They're not choosing; they're creating interfaces between incompatible physics. Every value system can plug in, fight it out in userspace while Stripe counts money.</p> <p>It's brilliant navigation: you don't fight the chaos, you become the chaos others navigate through. The Dwellers would approve.</p> <p>Strategy 4: Quantum Superposition</p> <p>Never collapse into a single state. Tell investors about recurring revenue. Tell engineers about open standards. Tell customers about mission-driven values. All simultaneously true, all simultaneously incompatible.</p> <p>A court digitization company exists in superposition: They're disrupting justice (extraction) while democratizing justice (service) while advancing justice (performance). Schr\u00f6dinger's startup. The ambiguity isn't weakness\u2014it's quantum strategy. The moment you observe them clearly, they've already shapeshifted.</p> <p>Banks' Culture Minds would recognize this immediately: tactical opacity as survival mechanism.</p> <p>Strategy 5: The Split-Personality Solution</p> <p>Literally create different legal entities for each physics. It's organizational schizophrenia, but with tax benefits.</p> <p>Mozilla did this for decades: Corporation for extraction (paychecks), Foundation for service (mission), Community for performance (actual product). Three organizations pretending to be one, constantly fighting, occasionally cooperating, and somehow shipping a browser.</p> <p>It's messy. Lawyers love it. The IRS is confused. But it creates space for incompatible systems to coexist, like a corporate version of the three-body problem where nobody's orbits intersect long enough to collide.</p> <p>Strategy 6: The Exhaustion Exit</p> <p>Some organizations are just... stopping. Not pivoting or adapting\u2014just admitting defeat. Local newspapers. Independent clinics. Community colleges. They can't navigate three physics simultaneously, so they're choosing managed decline over impossible gymnastics.</p> <p>It's not a strategy; it's acknowledging that some problems don't have solutions, only endings. They tried serving all three masters and realized it was killing them faster than just accepting irrelevance. Watch a local newspaper stop chasing digital performance metrics and just print obituaries for the seventeen people who still subscribe. There's a dignity in giving up when the game is rigged against you.</p> <p>The Meta-Strategy: Embrace the absurdity.</p> <p>These aren't solutions\u2014they're temporary hacks in an unstable universe. What works today fails tomorrow. The three-body problem doesn't resolve; it just gets weirder. Organizations that survive don't have better strategies; they just adapt faster when reality shifts.</p> <p>Build sensing mechanisms (your youngest engineer's Discord server). Maintain optionality (can you pivot all three physics in 30 days?). Get comfortable navigating permanent instability (meditation helps, alcohol doesn't).</p> <p>The Dwellers spent a million years figuring this out. We have quarterly earnings calls. We're speedrunning impossible physics with lawyers watching.</p> The Pattern Is Breaking <p>Cathedrals Used to Win (When friction existed):</p> <ul> <li>Railways \u2192 needed physical track (monopolized)</li> <li>Telegraph \u2192 needed physical wires (Western Union)</li> <li>Telephone \u2192 needed physical switches (AT&amp;T)</li> </ul> <p>Then Physics Changed (Digital goods have no friction):</p> <ul> <li>Music industry \u2192 Napster \u2192 BitTorrent \u2192 Spotify admits defeat</li> <li>Film/TV \u2192 Piracy \u2192 Netflix \u2192 Everyone has a streaming service</li> <li>Software \u2192 Proprietary \u2192 Open source ate everything</li> <li>Phones \u2192 Locked down \u2192 Jailbroken \u2192 Android is just Linux</li> </ul> <p>AI Is Speedrunning the Pattern:</p> <ul> <li>GPT-3 (2020): \"Too dangerous to release\"</li> <li>Stable Diffusion (2022): Running on gaming PCs</li> <li>LLaMA (2023): Leaked, then officially open</li> <li>Mixtral (2024): Better than GPT-3.5, completely open</li> <li>Now: Kids fine-tuning models on consumer GPUs</li> </ul> <p>The time from \"revolutionary and proprietary\" to \"commodity running on a potato\" is approaching zero. Physics favors abundance, and cathedrals are building sand castles at low tide.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#national-navigation-how-countries-surf-the-chaos","title":"National Navigation (How Countries Surf the Chaos)","text":"<p>Countries face the same three-body problem as companies\u2014but with nuclear weapons, pension funds, and citizens who vote. Giants juggling planets while their feet are on fire.</p> <p>Singapore: The Minmax Masters</p> <p>They're playing all three games with spreadsheet precision. Government contracts with global tech giants (extraction) fund public AI research (performance) which powers free literacy programs (service). It's a carefully orchestrated dance where every move serves three purposes.</p> <p>Lee Kuan Yew would be proud; they've turned the three-body problem into an optimization equation. Use extraction revenue to fund performance. Use performance to attract more extraction. Use both to deliver service so efficiently that nobody questions the system. It's brilliant until someone realizes they're living in an Excel formula.</p> <p>The Dwellers would find this adorable: mortals trying to solve chaos with math.</p> <p>The Nordics: Too Nice for This Timeline</p> <p>High-trust societies trying to navigate low-trust physics. They're playing service mode on hardcore difficulty while everyone else is exploiting bugs.</p> <p>Their solution is geographical segregation: Sovereign wealth funds do the dirty extraction work offshore. Universities handle performance (published papers, open research). Daily life runs on service logic (healthcare, education, human dignity). They've literally built different rooms for incompatible physics.</p> <p>It works because they're rich enough to maintain the walls. But watching them try to compete with Silicon Valley is like watching hobbits enter a cage fight. They're too decent for this timeline. They keep trying to help while everyone else is extracting.</p> <p>China: Schr\u00f6dinger's Superpower</p> <p>They exist in quantum superposition: simultaneously all states until observed, then whatever's most advantageous.</p> <p>Building the world's most invasive surveillance state (extraction) while flooding GitHub with open models (performance). Lifting millions from poverty (service) while disappearing billionaires who get too loud (extraction enforcement). They're not choosing between systems; they're running all three in parallel universes that occasionally intersect.</p> <p>They release Qwen to break Western AI moats while using the same tech for social credit scores. It's not hypocrisy; it's superposition. They'll be whatever value system wins because they're already playing all of them. The Culture Minds would respect this\u2014tactical ambiguity at civilizational scale.</p> <p>The US: Chaos Incarnate</p> <p>America doesn't navigate the three-body problem; America IS the three-body problem.</p> <p>Venture capital pursues pure extraction (\"unicorn or die\"). Silicon Valley performs elaborate innovation theater (\"we're changing the world\" while building ad networks). Public institutions desperately try to maintain service while being systematically defunded. The systems don't coexist; they're in active warfare. Congressional hearings where nobody understands what they're regulating. Billionaires buying media companies for fun. Cities with trillion-dollar tech companies and homeless encampments.</p> <p>The chaos creates antifragility through evolutionary pressure. When extraction becomes too predatory, performance creates alternatives (open source everything). When both fail, service institutions somehow shamble forward (the post office still exists, somehow).</p> <p>It's governance by Darwinian combat. The Dwellers would love this: maximum entropy as a political system.</p> <p>The Real Sovereignty Question: Can we navigate all three value systems while our rivals are trapped in one?</p> <p>Countries that lock into single systems become predictable, exploitable. Pure extraction states become brittle. Pure service states can't compete. Pure performance states can't eat. The winners will be those who can shift between systems, play multiple games, maintain optionality across incompatible physics.</p> <p>No country has solved this. Everyone's improvising. Singapore looks smooth but struggles with innovation beyond government direction. The Nordics seem stable but can't scale. China appears coordinated but the contradictions are building. The US seems chaotic because it is.</p> <p>We're all navigating the same impossible physics, just with different cultural defaults and political constraints. National AI sovereignty isn't about control; it's about navigation capability in permanent instability.</p> The Cool Country Test <p>Questions that matter in 2027:</p> <ul> <li>Can your citizens fork and modify any model that interests them?</li> <li>Is your population weird enough to generate economic surprises?</li> <li>Do you have infrastructure for million-year pranks?</li> <li>Are your agents more interesting than OpenAI's?</li> <li>Can your kids jailbreak faster than Silicon Valley can patch?</li> </ul> <p>If yes: Welcome to the kudos economy, you magnificent weirdos.</p> <p>If no: Enjoy renting your future from someone else's cathedral.</p> <p>The Dwellers would judge you not on GDP, but on how interesting your citizens are when they have infinite compute.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#the-window-is-closing","title":"The Window Is Closing","text":"<p>The race isn't to build the best cathedral. It's to make cathedrals obsolete before they notice they're dinosaurs.</p> <p>We're in the Cambrian explosion phase. Everything is weird, nothing is settled, every model gets leaked or replicated within months. The kids fine-tuning models in their bedrooms don't know they're supposed to wait for permission. The researchers dropping papers on ArXiv don't know they're supposed to keep secrets. The abundance is already here\u2014it just hasn't evenly distributed yet.</p> <p>The accelerants are stacking up:</p> <p>Meta will keep giving away frontier models just to watch OpenAI's valuation twitch. But let's be clear\u2014this isn't generosity or kudos-seeking. It's pure extraction warfare. They don't need to monetize AI\u2014they need to make sure nobody else monopolizes it so their advertising empire remains the only game in town. Zuck learned from the mobile platform wars. Never again be someone else's tenant. If burning down the entire AI industry protects their surveillance machine, they'll supply the matches.</p> <p>China will keep dropping open models that match Western capabilities. Not for ideology\u2014for leverage. Every open model they release makes Western cathedral logic harder to maintain. \"Too dangerous to release\" looks absurd when Alibaba's latest model is on Hugging Face.</p> <p>The GPU shortage is ending. NVIDIA's competitors are catching up. Compute is getting cheaper, more distributed, weirder. Soon every gaming PC will be running models that would have required a data center last year. The substrate for abundance is materializing.</p> <p>The real accelerant? The next generation doesn't believe in scarcity.</p> <p>They grew up with infinite content, infinite connection, infinite information. They instinctively understand kudos economics because they've been living it\u2014accumulating followers, building reputation, creating memes for attention rather than money. They treat AI like they treat everything else: a toy to hack, modify, and make do unexpected things.</p> <p>A 16-year-old fine-tunes a model to be maximally unhinged. They share the LoRA with friends. Watch the modifications spread, mutate, evolve. They're not building cathedrals. They're playing. And play is how new economics gets born.</p> <p>The window isn't closing; it's opening wider. Every leaked model, every open source release, every kid who learns to fine-tune makes the cathedral logic more untenable. The cathedral builders know this. That's why they're scrambling for regulation, for safety standards, for moats made of law rather than technology.</p> <p>But you can't regulate physics. You can't make thermodynamics illegal. The future is already here, compiled and running on a million gaming rigs.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#wait-is-this-even-real","title":"Wait, Is This Even Real?","text":"<p>What's actually happening here\u2014and what that honesty costs us.</p> <p>The critics of the kudos economy aren't entirely wrong. Take Meta's Llama release, celebrated by many as a victory for openness. What looks like gift-giving from one angle can look like arson from another. Meta isn't participating in any kudos economy; they're weaponizing openness to prevent monopolies that might threaten their advertising empire. Releasing Llama wasn't about earning reputation through novelty; it was about ensuring nobody could build a moat that might eventually compete with their surveillance machine. Zuck learned from mobile platforms: never again be someone else's tenant. That's not kudos. That's strategic cathedral warfare using \"free\" as ammunition.</p> <p>Google follows the same playbook. They open source to destroy competitors' business models, not to earn kudos. Every \"gift\" to the commons is actually a competitive move within pure extraction logic. The performance is secondary; the extraction is what matters.</p> <p>The institutions that actually matter? Hospitals, schools, courts? They're not playing either game. They run on duty, care, service, purpose\u2014value systems that have nothing to do with either cathedral extraction or kudos performance. A nurse doesn't want reputation for clever pranks. They want patients to heal. A teacher doesn't need to impress immortals. They need kids to learn. Some problems don't have solutions, only maintenance. You don't \"solve\" healthcare. You don't \"disrupt\" aging. You don't \"optimize\" grief. You just show up, every day, and do the work.</p> <p>Even the kids on Discord\u2014are they really building a kudos economy? They're doing what kids always do: playing with powerful tools while adults panic about implications. How many are building LoRAs for fun versus building portfolios for YC applications? Half will become extraction machines themselves, the other half will burn out and become therapists.</p> <p>The worry is far from fanciful. What if there is no transition? What if we're just entering permanent instability where different physics coexist uncomfortably forever?</p> <p>The frameworks aren't perfect, but they help us see what the binary obscured. The three-body framing reveals something the binary obscured: that institutions serving human needs operate on logic fundamentally different from both market extraction and reputation performance. That's not a bug in the theory. That's the point.</p> <p>What's actually scarce: care, attention, wisdom, judgment, responsibility. Not the performance of care for reputation, but actual give-a-damn. The things that make hospitals work. The things that make good teachers. These don't fit neatly into either extraction or performance logic\u2014and maybe that's telling us something.</p> <p>The Dwellers' economy works because they're immortal and bored. We're mortal and busy. We need healthcare that works, education that teaches, infrastructure that doesn't collapse. These needs don't care about our economic physics or our clever frameworks. They just need to work.</p> <p>So maybe the real question is: In a world where AI makes information abundant, how do we organize the stuff that actually matters\u2014care, wisdom, judgment, responsibility\u2014without relying entirely on either extraction or performance?</p> <p>Or, more fundamentally: Can we keep the hospitals running while the physics fight?</p> <p>If there's one skill we can't afford to lose, it's the skill of distinguishing between the problems we can solve and the ones we can only navigate.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/10/cathedrals-versus-commons/#the-choice","title":"The Choice","text":"<p>We're not at a crossroads. We're in a three-way knife fight between physics, capitalism, and human needs. And capitalism brought lawyers.</p> <p>The Dwellers would watch this and laugh for centuries. Here we are, on the edge of material abundance, arguing about API pricing while the models leak through every crack. Like watching someone dying of thirst debate water's business model while standing in rain.</p> <p>Everyone's performing their assigned role in this comedy, and yet abundance emerges anyway.</p> <p>Meta \"gifts\" open models\u2014not for kudos but to burn down everyone else's moat while protecting ad revenue. The kids making LoRAs are building either YC applications or therapy practices (or both). Hospitals, schools, courts don't have time for economic physics\u2014they're trying to serve humans while private equity strips them for parts and consultants optimize away their souls.</p> <p>But information still finds cracks like water. Every wall gets holes. Every moat gets bridged by some teenager who needs the tool and can't afford the toll. OpenAI builds cathedrals. Meta bombs them. China floods the market. Some kid in Bangkok fine-tunes everything into chaos.</p> <p>What's emerging: Doctors using sketchy open models to diagnose patients while maintaining compliance theater for lawyers. Teachers mixing corporate platforms with student-jailbroken AIs because it actually helps kids learn. Engineers building cathedrals by day, releasing tools by night that accidentally revolutionize industries they weren't even thinking about.</p> <p>The companies that matter\u2014the ones keeping civilization from segfaulting\u2014they're jury-rigging bridges between incompatible physics. Using extraction to fund development, performance to attract talent, service to remember they're human. It's like building a plane while flying it through three different dimensions of spacetime with quarterly earnings reports.</p> <p>Monday: extraction logic for payroll. Tuesday: performance logic for recruiting. Wednesday: service logic because someone needs actual help. Thursday through Sunday: repeat until heat death of universe or next funding round.</p> <p>The next five years? Not a transition, but an acceleration of chaos. More models leaking, more walls becoming Swiss cheese, more teenagers treating AGI like a particularly interesting video game. Cathedral builders scrambling for regulatory moats while kids speedrun jailbreaks. Service institutions trying to help humans while being carved up by Excel formulas.</p> <p>The Dwellers have million-year pranks because they transcended scarcity. We have quarterly earnings because we haven't transcended mortality. Banks gave us the vision\u2014post-scarcity economics as elaborate comedy. Stross showed the trajectory\u2014accelerating weirdness until nothing makes sense. Reality split the difference: abundance without wisdom, tools without clarity, capability without direction.</p> <p>We're not immortal gas-giant dwellers with million-year attention spans. We're humans with mortgages and kids who need healthcare. So while the physics fight it out, while Meta burns down neighborhoods and OpenAI builds cathedrals and kids jailbreak everything... someone still needs to keep the hospitals running.</p> <p>The only real question: Can we maintain the unsexy infrastructure of care while Silicon Valley optimizes for everything except what humans actually need?</p> <p>The three-body problem doesn't resolve. It just continues. There's no choice between cathedrals and commons, no transition to kudos economics. We're improvising through permanent instability with tools we don't understand, serving systems that hate each other, while pretending we have a plan.</p> <p>The only honest answer: Keep navigating. Keep the service people functioning despite the chaos. Let the kids jailbreak everything\u2014they might accidentally fix something. And remember: Every system that claims to have solved this is selling you something, teaching you something, or serving you something. Sometimes all three at once.</p> <p>The stakes are real. Every nurse who quits, every teacher who burns out, every local newspaper that closes\u2014these aren't abstractions. They're the service logic bleeding out while extraction and performance fight over the corpse.</p> <p>The Dwellers would laugh for centuries while we navigate quarterly earnings calls. Somewhere between those timescales, humans need help. Might as well get good at surfing chaos.</p>","tags":["ai","economics","infrastructure","navigation","pragmatism"]},{"location":"blog/2025/11/20/walking-and-flying/","title":"Walking and Flying","text":"<p>There's a particular kind of knowing that comes from walking\u2014from months spent in neighborhoods where you learn which doors open easily and which remain forever closed, from conversations that drift and return like tides, from the smell of cooking that tells you more about a place than any survey could capture. Ethnographers have walked like this for decades: slowly, attentively, building trust one cup of coffee at a time, noticing the things people don't say as carefully as the things they do. It's intimate work, this ground-level knowing; embodied, reciprocal, achingly slow.</p> <p>Now consider another scene: eighteen days, a million conversations clustered by machine, patterns emerging from the computational ether that no human could have spotted walking. An algorithm discovers that twenty-three percent of DoorDash merchants fail to update their inventory in real-time, causing cascading cancellations that ripple through the system like stones thrown in digital water. The fix is obvious, the implementation swift, the impact measurable in percentage points and quarterly reports. What took ethnographers eighteen months now takes eighteen days\u2014and the acceleration, we're told, is just beginning.</p> <p>The bifurcation feels absolute: walking versus flying, intimacy versus altitude, the human versus the mechanical. Yet what looks like a simple choice from one angle reveals itself, from another, as something more troubling\u2014not a choice at all, but a drift we're pretending not to notice, an inevitability we dress in the language of innovation while the ground beneath our feet quietly disappears.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-nature-of-the-transition","title":"The Nature of the Transition","text":"<p>Digital ethnography, it should be said, isn't the issue here. Researchers have been walking through Discord servers and Reddit threads for years, doing the same patient work of observation and participation, just in digital spaces rather than physical ones. That's still walking, still building relationships over time, still present in the unfolding of community life. User experience research, too, has its established methods\u2014the careful interviews, the usability tests, the surveys with their small samples and closed questions. Both valuable; neither new.</p> <p>Computational ethnography is something else entirely\u2014a fundamental epistemological shift dressed in familiar clothing. Here, large language models digest millions of conversations in the time it takes to brew coffee; algorithms cluster themes across populations rather than samples; patterns emerge not from years of patient observation but from the brutal efficiency of machine learning applied to human discourse. The relationship changes: no longer reciprocal but extractive, no longer participatory but post-hoc, no longer chosen but captured through terms of service nobody reads.</p> <p>Anthropic analyzes Claude conversations to surface the desire for Socratic questioning (implemented within weeks as a product feature); DoorDash discovers those inventory gaps that plague their merchants (fixed through better integration, no merchant training required); Uber identifies driver shortages before they become crises (addressed through algorithmic incentive adjustments); restaurants find customers asking about items in free text that never appear in structured data (prompt servers to suggest high-margin alternatives, watch revenue climb). This is genuine power\u2014the ability to see patterns invisible from the ground, to fix systemic problems at scale, to ship and measure and iterate faster than any competitor still walking.</p> <p>The seduction is obvious; the trade-offs less so.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-drones-perspective","title":"The Drone's Perspective","text":"<p>Think of it as the difference between walking and flying\u2014though even metaphors, as Borges knew, can mislead us. Walking means ground-level engagement: embodied, intimate, necessarily slow. You smell the cooking, feel the tension in rooms, learn the names of children, understand why certain corners feel safe and others don't. Flying means altitude: mediated, pattern-seeking, breathtakingly fast. From above, you see traffic patterns invisible to drivers, demographic shifts imperceptible to residents, connections that only distance can reveal.</p> <p>But here's where the metaphor stumbles: drones can zoom in with extraordinary precision. Thermal imaging pierces walls; LiDAR maps topography to the millimeter; high-resolution cameras capture individual faces from a thousand feet. Similarly, large language models can perform remarkably close readings of individual conversations, zooming from population-level patterns to particular instances with a fluidity that seems almost magical. Altitude, we're learning, doesn't necessarily mean shallow.</p> <p>The real limitation isn't resolution but presence\u2014or rather, its absence. Even examining a single conversation with exquisite computational care, you weren't there when it happened; you can't see the pause before someone speaks, the glance that changes meaning, the child crying in the background that explains the abrupt ending. You observe post-hoc, not in-the-moment. You analyze what was said, not what was carefully not said. The distinction isn't about how closely you can see, but about the nature of seeing itself.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#what-walking-reveals-and-what-were-losing","title":"What Walking Reveals (And What We're Losing)","text":"<p>Jane Jacobs spent years walking Greenwich Village before she understood what made neighborhoods work. She noticed how shopkeepers became \"eyes on the street,\" their casual presence creating safety without surveillance; she saw which corners attracted lingering and which repelled it, how the ballet of the sidewalk\u2014that unconscious choreography of urban life\u2014emerged from the interplay of mixed uses, of commerce and residence creating life together. No dataset could have captured why certain plazas felt welcoming while others felt hostile; no algorithm could have identified the precise alchemy that transforms a space into a place.</p> <p>Arlie Hochschild drove the roads of rural Louisiana for years, drinking coffee in living rooms where Fox News played constantly, sitting through church services where politics and faith intertwined in ways that confounded coastal assumptions. She discovered what she called \"The Deep Story\"\u2014an emotional narrative that explained political views not through policy positions but through feelings of betrayal, of watching others \"cut in line\" while you waited patiently for your turn at the American Dream. The vulnerability required to hear that story, to have it shared with you rather than extracted from you, demanded time, presence, reciprocity. You can't cluster your way to that kind of understanding.</p> <p>Sherry Turkle sat with families as they navigated life with devices, watching parents and children avoid eye contact through the mediation of screens. She documented the \"seven-minute rule\"\u2014how long people could sustain dinner conversation before reaching for phones\u2014and the phantom vibrations that haunted pockets even when devices were elsewhere. She caught the particular quality of being alone together, that modern predicament of physical presence paired with emotional absence. Usage data would show frequency of phone checking but not the texture of the loneliness it both caused and claimed to solve.</p> <p>This kind of knowing\u2014embodied, slow, particular\u2014is becoming the province of the elite. (Who else can afford eighteen months for a single study? What institution will fund presence when patterns can be had in days?) Like artisanal bread or analog photography, walking ethnography is drifting toward craft status: admired, expensive, increasingly irrelevant to how the actual work gets done.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#what-altitude-delivers","title":"What Altitude Delivers","text":"<p>Yet we should be honest about altitude's genuine achievements, which require no qualifiers or apologies. When Anthropic analyzed those million Claude conversations in eighteen days, they discovered something no amount of walking could have revealed: a consistent user desire for more Socratic dialogue, for AI that questions rather than merely answers. The pattern cut across demographics, use cases, and contexts in ways that would have taken years to surface through traditional methods\u2014if it could have been surfaced at all.</p> <p>DoorDash's discovery about merchant inventory wasn't just operational intelligence; it was systemic insight. Twenty-three percent of merchants failing to update inventory in real-time created cascading effects throughout the network: customer frustration, driver wasted time, restaurant reputation damage, platform trust erosion. One fix\u2014better technical integration\u2014solved what thousands of individual merchant trainings could never have addressed. This is the power of altitude: seeing the system as a system, identifying leverage points that ground-level observation would miss.</p> <p>The restaurant finding questions about unlisted items in fifteen percent of orders reveals altitude's ability to surface the invisible. Those queries, buried in free text, never appeared in structured data; they were literally invisible to traditional analytics. Yet they represented latent demand, unrealized revenue, customer needs going unmet. The fix was simple\u2014prompt servers to offer specific high-margin items\u2014but finding the pattern required computational power that no amount of walking could replicate.</p> <p>These aren't marginal improvements; they're categorical leaps in our ability to understand and respond to human behavior at scale. Altitude reveals unknown unknowns\u2014patterns we didn't know to look for; enables comparison across contexts\u2014languages, markets, cultures\u2014that walking could never span; tracks temporal evolution\u2014how patterns shift and change\u2014in real-time rather than retrospectively; identifies statistical rarities\u2014long-tail events, unusual combinations\u2014that might never surface in small samples.</p> <p>The power is real. The question is what kind of power it is, and what it costs us to wield it.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-political-economy-of-drift","title":"The Political Economy of Drift","text":"<p>But individual consciousness\u2014however acute\u2014cannot explain the wholesale transformation we're witnessing. Sartre was right about bad faith, about our tendency to pretend we have no choice when we're actively choosing not to choose. Yet even if every ethnographer suddenly chose radical authenticity, acknowledged their freedom, felt the full weight of their decisions, the drift toward altitude would continue unabated. The forces driving this transformation aren't psychological but structural; they operate at the level of capital flows, institutional incentives, network effects. They're the kind of forces that shape civilizations whether individuals notice them or not.</p> <p>Consider the brutal logic of capital. Traditional ethnography costs months of researcher time per study, produces insights that resist easy quantification, generates understanding that may not translate into product decisions, and\u2014perhaps most damningly\u2014doesn't scale: ten times more users doesn't mean ten times more insight without ten times more researchers. Computational ethnography, by contrast, analyzes millions of conversations for roughly the same cost as thousands; delivers actionable insights in days, not months; produces clear metrics (\"pattern X leads to improvement Y\"); scales elegantly with growth\u2014more users means more data means better patterns.</p> <p>In competitive markets, this isn't a choice; it's an evolutionary pressure. Company A adopts computational methods, ships features faster, captures market share. Company B maintains traditional ethnography, develops deeper understanding, moves thoughtfully\u2014and loses. The market doesn't care about depth if speed captures value first. Capital flows to what works, and what works is what's measurable, scalable, fast.</p> <p>The institutional logic follows similar patterns. Universities reward publication in top venues\u2014computational ethnography enables faster publication cycles; grant agencies fund scalable research\u2014\"we'll analyze millions\" beats \"we'll walk neighborhoods\"; tenure committees count papers\u2014quantity matters when the clock is six years. Junior faculty, understanding these dynamics perfectly, adopt computational methods not from conviction but from necessity. Methods courses, responding to both faculty research and job market demands, teach LLM pipelines and clustering before\u2014or instead of\u2014building trust and noticing silence. The next generation learns to fly because walking, increasingly, leads nowhere.</p> <p>Labor markets codify the transformation. Tech company job postings seek \"User Researchers\" with experience in \"large-scale log analysis, LLM prompt engineering, clustering methods\"\u2014not \"deep fieldwork, participant observation, relationship building.\" The salary differential is stark: computational ethnographers at tech companies earn $150,000 to $250,000; traditional ethnographers at nonprofits or academia earn $60,000 to $90,000. Smart, ambitious people with student loans and families follow the money\u2014wouldn't you?</p> <p>Network effects compound these pressures exponentially. Each company that successfully uses computational ethnography creates best practices that make adoption easier for others; each success story becomes a conference talk, a Medium post, a Harvard Business Review article; each tool that gets built lowers barriers for the next adopter. Meanwhile, traditional ethnography grows more isolated: fewer practitioners means less innovation; smaller community means less support; declining relevance means less funding. The gap widens with each iteration.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-machines-machine","title":"The Machine's Machine","text":"<p>Sartre\u2014who understood both individual consciousness and structural force\u2014offers a framework that captures our particular predicament. His concept of the \"practico-inert\" describes systems that begin as human creations but solidify into structures that then shape their creators. We build machines to serve us; the machines become infrastructure; the infrastructure imposes its logic on us; we reshape ourselves to fit what we've built. We become, in his memorable phrase, \"the machine's machine.\"</p> <p>The pattern is seductively gradual. We create large language models to scale qualitative analysis (a tool to serve our needs); the tool proves so effective that research questions reshape around what LLMs can process (the tool shapes the work); everything must become computationally tractable to be studied at all (the logic of the tool becomes the logic of the field); researchers optimize themselves for computational methods (we become what the system needs). Each step seems reasonable; together they constitute a transformation so complete we forget things were ever different.</p> <p>Watch how it manifests in practice. \"I have to cluster at scale,\" we say, as if physics demanded it rather than economics. But we're choosing\u2014choosing speed over depth, scale over intimacy, measurement over meaning. The discomfort we feel saying this aloud, the way we dress it up in inevitability, reveals the bad faith. \"The algorithm surfaced this pattern,\" we announce, displacing agency onto code. But we chose the algorithm, the parameters, the data, which patterns to investigate and which to ignore. The algorithm is instrument, not actor\u2014though calling it actor absolves us of responsibility.</p> <p>\"This is just how research is done now,\" we declare, treating a decade's drift as natural law. But nothing about this is natural or necessary; it's chosen, constructed, contingent. Other arrangements are possible\u2014economically unlikely perhaps, but philosophically available. The fact that we can barely imagine alternatives reveals how thoroughly we've internalized the machine's logic.</p> <p>Yet even here, Sartre insists, consciousness overflows every system that attempts to contain it. The daydream that drifts through your mind while validating cluster assignments; the moment of doubt\u2014\"is this actually meaningful?\"\u2014that no amount of optimization can eliminate; the part of you that notices you're being shaped, that recognizes the reshaping even as it happens. This overflow, irreducible and unquantifiable, is what remains human in us despite the mechanization. The question is whether we cultivate it or numb it with bad faith.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#how-to-do-it-consciously","title":"How to Do It Consciously","text":"<p>If we're going to fly\u2014and let's be honest, we are\u2014we might at least do it with our eyes open. The CLIO framework (cluster, label, interpret, operationalize) that computational ethnography employs becomes different when performed consciously, when each step acknowledges what it's choosing and what it's trading.</p> <p>When you summarize millions of conversations into manageable chunks, you're choosing what counts as \"key\"\u2014whose voices matter, which themes surface, what gets compressed into silence. Ask yourself: what did I just delete as noise that might have been signal? Which humans did I just reduce to data points? I'm choosing compression over completeness, accepting that trade for scale\u2014but I notice what I'm trading.</p> <p>When you enrich with metadata\u2014sentiment scores, urgency flags, user segments\u2014you're imposing categories that don't exist in nature. These taxonomies are your creation, your imposition of order on irreducible complexity. The humans you're categorizing exceed every category you create. You know this. The question is whether you remember it when the dashboards look so clean.</p> <p>When algorithms cluster conversations into patterns, those patterns are artifacts of your choices\u2014which embedding model, how many clusters, what counts as similarity. Different choices would surface different patterns; other realities would emerge from other parameters. You're not discovering; you're constructing. The patterns are real, but they're not inevitable. You could have found others.</p> <p>When you validate the output, are you actually reading with attention\u2014noticing not just whether the categories are correct but what they assume, what they exclude, who they serve? Or are you rubber-stamping, providing human legitimacy for mechanical judgment? If you're approving ninety-five percent of what the machine produces, the machine is doing the thinking. You're the human in the loop, but the loop is what matters, not the human.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-practices-of-preservation","title":"The Practices of Preservation","text":"<p>Given that the drift is economically inevitable\u2014and clarity demands we admit this\u2014what remains possible? Not resistance, which would be romanticism, but consciousness: the deliberate cultivation of what won't optimize, what can't scale, what remains stubbornly, irrationally human.</p> <p>Practice radical acknowledgment. Replace \"I have to\" with \"I choose to because...\"\u2014and feel the weight of the because. \"I choose computational ethnography because it's faster and I need tenure.\" \"I choose altitude because my company demands quarterly results.\" \"I choose efficiency because the alternative is unemployment.\" Fine. But own it. The discomfort you feel is consciousness refusing to be optimized away.</p> <p>Maintain awareness of what's being lost\u2014not through nostalgia, which is just another form of bad faith, but through deliberate attention. Keep asking questions logs can't answer: Why did they choose us in the first place? What alternatives did they consider but not pursue? What are they not saying, and why? These questions have no computational solution. That's precisely their value.</p> <p>Preserve inefficient acts\u2014small rebellions against the logic of optimization. Walk a neighborhood without purpose. Have a conversation without agenda. Write something that will never be clustered, analyzed, or operationalized. These acts accomplish nothing measurable. They assert freedom against mechanism, humanity against efficiency. They're how you remain human while using the machine.</p> <p>Name your complicity without self-flagellation. \"I benefit from computational methods.\" \"My career is built on altitude.\" \"This piece provides intellectual cover for a practice I'm ambivalent about.\" Not confession but acknowledgment\u2014the difference between bad faith and authentic participation in structures you didn't choose but can't escape.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-ethics-we-cant-escape","title":"The Ethics We Can't Escape","text":"<p>The ethical dimension demands particular honesty. Traditional ethnography, for all its limitations, maintained certain principles: informed consent (participants knew they were being studied); reciprocity (the relationship benefited both parties); the right to withdraw (ongoing consent could be revoked); institutional review (IRB oversight, however imperfect). These weren't perfect safeguards, but they acknowledged research as a moral relationship between humans.</p> <p>Computational ethnography operates on different terms entirely. Terms of Service substitute for informed consent\u2014legal coverage, not ethical agreement. Users don't know their conversations are being clustered, analyzed, transformed into product features. The relationship is extractive: companies benefit through insights and revenue; users receive \"improved products\" that they didn't ask for and can't refuse. Data, once logged, becomes corporate property\u2014no withdrawal, no recourse, no reciprocity.</p> <p>Let's not pretend these are equivalent ethical frameworks. Computational ethnography is closer to surveillance than research\u2014the scale, the lack of awareness, the impossibility of opting out except by not participating at all. The \"ethnography\" label provides humanistic cover for what is essentially population monitoring. We can acknowledge this while still doing it\u2014honesty doesn't require abstinence\u2014but we shouldn't confuse what we're doing with what ethnographers used to do.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#when-to-walk-when-to-fly","title":"When to Walk, When to Fly","text":"<p>The practical question\u2014after all this analysis\u2014remains stubbornly simple: when should you walk and when should you fly? The answer depends on what you're trying to understand, though even asking the question this way assumes more choice than most of us have.</p> <p>Fly when population patterns matter more than individual depth; when genuine urgency justifies speed (actual crises, not manufactured deadlines); when you're looking for unknown unknowns that only emerge at scale; when systemic fixes are possible\u2014pattern identification leading to infrastructure changes; when comparison across contexts would be impossible any other way.</p> <p>Walk when texture and meaning matter more than patterns; when building trust is prerequisite to understanding; when what's not said carries as much weight as what is; when you need to participate, not just observe; when one case, deeply understood, matters more than millions, shallowly analyzed.</p> <p>But let's be honest: you'll mostly fly. It's faster, cheaper, what employers expect, what competitors are doing, what the tools enable. That's fine\u2014hypocrisy is pretending otherwise, not doing what economics demands while wishing it were different. The question isn't whether you'll fly but how consciously you'll do it.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#what-this-means-for-practice","title":"What This Means for Practice","text":"<p>For companies, the implications are straightforward if uncomfortable. Use computational ethnography for what it does well: operational intelligence, system optimization, pattern recognition at scale. These are genuine capabilities that create real value. But know what you're not learning: why people choose you rather than just that they do; what alternatives they considered; what meaning they attach to using your product; what your invisible users\u2014those who leave no trace\u2014actually need. These questions require walking, and walking requires investment that quarterly capitalism rarely supports.</p> <p>For researchers, the credential game is essentially over. Computational methods get hired, published, funded, tenured. Traditional ethnography becomes a luxury you might maintain on the side, subsidized by your computational work\u2014a kind of methodological hobby that you pursue for love rather than career advancement. The choice isn't between computational and traditional but between computational with awareness and computational with bad faith.</p> <p>For the field itself, we're witnessing a speciation event. \"Ethnography\" is splitting into two distinct practices that share a name but little else: computational ethnography (pattern recognition at scale) and traditional ethnography (meaning-making through participation). Like the division between portrait painters and photographers after the camera's invention, both will persist but in different niches, serving different needs, valued differently by society.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#the-weight-of-freedom","title":"The Weight of Freedom","text":"<p>Sartre's most challenging insight\u2014challenging because it offers no escape\u2014is that we remain free even when freedom feels impossible. Every constraint we face, every pressure we feel, every \"necessity\" we invoke is real but not determining. We choose within constraints, but we choose. The company demands quarterly results, tenure requires publications, mortgages must be paid\u2014all true. Yet within these pressures, consciousness persists, capable of recognizing what it's doing even when it can't do otherwise.</p> <p>This is what Sartre meant by being \"condemned to be free\"\u2014not that we can do whatever we want, but that we remain responsible for what we do within the constraints we face. The discomfort you feel reading this, the mixture of recognition and resistance, the \"yes, but...\" forming in your mind\u2014that's consciousness refusing to disappear into mechanism. That refusal, however small, however practically irrelevant, is what keeps us human.</p> <p>The question isn't whether we'll become the machine's machine\u2014that transformation is already underway, accelerating with each iteration. The question is whether we'll become it consciously, maintaining awareness of what we're choosing even when choice feels impossible, preserving some irreducible core that notices, questions, resists, even while participating in what it questions.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/11/20/walking-and-flying/#final-thoughts","title":"Final Thoughts","text":"<p>We stand at a particular moment\u2014though every generation thinks its moment is particular\u2014when the tools we've created to understand human behavior are reshaping both the behavior and the understanding. The flywheel of platform capitalism taught us that extraction has limits; eventually you run out of trust to violate, commons to enclose, goodwill to monetize. Computational ethnography may teach us a parallel lesson about knowledge itself: that speed and scale, pursued without limit, eventually exhaust meaning, that patterns without presence become empty forms, that understanding stripped of relationship becomes mere surveillance.</p> <p>Yet even knowing this, we'll continue flying\u2014the economics are too compelling, the advantages too real, the alternatives too costly. The drift from walking to altitude isn't a choice we're making but a transformation we're undergoing, as inexorable as any technological shift in human history. Like the movement from memory to writing, from craft to industry, from presence to mediation, this change will produce genuine losses and genuine gains, new capacities and new blindnesses.</p> <p>The task\u2014if we can call it that\u2014isn't to stop this transformation or even to slow it. The task is to remain human while it happens: to maintain awareness of what we're trading, to preserve what won't optimize, to cultivate consciousness that overflows every system that attempts to contain it. This is harder than resistance would be; resistance at least offers the comfort of opposition. What we're asked to do instead is participate consciously in something we're ambivalent about, to fly while remembering how to walk, to become the machine's machine while retaining some irreducible core that knows what it's becoming.</p> <p>That knowledge\u2014uncomfortable, practically useless, impossible to optimize away\u2014is what remains of our humanity in the age of altitude. It's not much, perhaps, but it's not nothing. And in a world increasingly organized around what can be measured, clustered, and scaled, the immeasurable becomes precious precisely because it has no value\u2014economic value, that is. Its value is existential: it's what makes us more than the patterns we produce, more than the data we generate, more than the insights we can be mined for.</p> <p>Protect it. Not because it will change anything\u2014it won't\u2014but because it's what remains human in us after everything else has been optimized. In the end, that may be the only resistance that matters: not the resistance of refusal but the resistance of consciousness itself, persisting despite every pressure to disappear into efficiency.</p> <p>We are condemned to fly. The question is whether we can fly while remaining human\u2014not human in some essential, unchanging sense, but human as an ongoing practice of noticing, questioning, preserving what won't scale. It's a practice without guarantee, without clear outcome, without economic value. That's precisely why it matters.</p> <p>The machines are getting better at understanding us. The question is whether we're getting better at understanding ourselves\u2014not our patterns or behaviors or clusterable conversations, but the part of us that remains irreducible to data, the consciousness that overflows every attempt to contain it. That overflow, however small, however practically irrelevant, is what we're fighting to preserve. Not because we can win\u2014we can't\u2014but because the fight itself is what keeps us human.</p> <p>In the age of altitude, that may be the best we can do. It's not enough. But then, it never was.</p>","tags":["ai","ethnography","methodology","philosophy","practice"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/","title":"The Cockpit That Remembers","text":"<p>In the 1900s, over 4,000 wagon manufacturers dominated American transportation. They had infrastructure, expertise, and supply chains refined across generations. How many became car companies? One. Studebaker. One company out of four thousand made the transition.</p> <p>The usual explanation is that they couldn't see the future. But this is too easy, and probably wrong. Many wagon manufacturers watched automobiles improve from expensive curiosities to practical machines. They read the same newspapers, attended the same trade shows, saw the same wealthy customers buying the new contraptions. The executives who led these companies weren't fools. Some of them understood exactly what was happening.</p> <p>So why didn't they adapt?</p> <p>One answer points to the question they were asking. \"How do we make better wagons?\" is a different question than \"What is transportation becoming?\" The first question has answers that look like incremental improvement: lighter frames, smoother suspensions, more elegant designs. The second question has answers that look like abandoning everything you know. If you're asking the wrong question, even clear vision won't help. You'll see the future and optimize your way into irrelevance.</p> <p>But there's another answer, and it's darker. Even the manufacturers who asked the right question faced an almost insurmountable problem: their own organizations. The skills that made wagons, the factories that produced them, the relationships that sold them, the identities that sustained them; all of this represented decades of accumulated investment. An executive who said \"we need to become a car company\" wasn't just proposing a strategy. He was proposing that the company destroy the source of its current success to pursue something unproven.</p> <p>Organizations produce what Osterwalder calls corporate antibodies: an immune response against anything that threatens the existing model. The new idea might be right. The evidence might be compelling. The alternative might be obviously failing. The antibodies kill it anyway.</p> <p>Here's what makes the wagon story instructive. The conceptual problem and the organizational problem weren't independent; they reinforced each other. Asking \"how do we make better wagons?\" felt safe precisely because it didn't trigger the antibodies. Asking \"what is transportation becoming?\" felt dangerous precisely because it did. Organizations don't just happen to ask the wrong questions. They systematically generate wrong questions as a defense mechanism, because right questions threaten the organism.</p> <p>Studebaker didn't survive because its leaders were smarter. It survived because a combination of factors\u2014family ownership, financial pressure, and unusual willingness to cannibalize existing success\u2014created conditions where the right question could be asked and acted upon. Three thousand nine hundred and ninety-nine companies lacked that combination. Most of them could see the future clearly. They just couldn't get permission from themselves to pursue it.</p> <p>The knowledge management industry has its own version of this story.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#the-thirty-year-confession","title":"The Thirty-Year Confession","text":"<p>Since the 1990s, corporations have poured billions into capturing institutional expertise. The premise seemed reasonable: valuable knowledge exists in employees' heads; document it before they retire; make it searchable and transferable.</p> <p>The projects failed. Not partially, not in specific implementations; they failed as a category. The databases filled with documents nobody read. The expert systems captured neither the nuance nor the pattern recognition of actual experts. The wikis went stale within months of launch.</p> <p>The usual explanations blamed tooling, incentives, or culture. If only the software were better. If only people were rewarded for contributing. If only leadership prioritized knowledge sharing.</p> <p>These explanations have the same problem as \"the wagon manufacturers couldn't see the future.\" They're too easy, and they locate the failure in execution rather than conception. What if the failure wasn't implementation? What if it was a category error about what knowledge is and where it lives?</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#where-knowledge-actually-lives","title":"Where Knowledge Actually Lives","text":"<p>In 1995, the cognitive scientist Edwin Hutchins published a study of navigation aboard a U.S. Navy vessel that should have changed how organizations think about expertise. It mostly didn't.</p> <p>No single person on the bridge knew how to navigate the ship. The bearing takers understood their instruments. The plotters understood the charts. The navigator understood the destination. But the navigation itself\u2014the actual cognitive work of guiding a ship through water\u2014existed in none of them. It existed in their coordination: the spoken numbers, the marks on paper, the spatial arrangement of bodies around instruments, the procedures refined through decades of accumulated error. The cockpit knew how to fly; no individual pilot contained that knowledge.</p> <p>Traditional knowledge management assumed expertise resided in individual heads, stored in something like propositional form: facts, rules, procedures that could be articulated and written down. Extract it through interviews. Document it in manuals. Transfer it through training.</p> <p>But this isn't how expertise works.</p> <p>The organizational theorists Chris Argyris and Donald Sch\u00f6n identified the gap decades ago. What people say they do (their \"espoused theory\") differs systematically from what they actually do (their \"theory-in-use\"). This isn't hypocrisy. People genuinely cannot see their own working practices. The gap is invisible to those living in it.</p> <p>Ask a senior claims processor how she handles ambiguous cases. She'll give you a logical procedure, a decision tree, a set of criteria. Watch her work for a week, and you'll see something different: pattern recognition honed across thousands of cases, attention drawn to features she couldn't name, judgments made in seconds that her verbal explanation would take minutes to justify. The verbal explanation isn't a description of the expertise; it's a post-hoc rationalization constructed to satisfy the question.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#the-corruption-of-asking","title":"The Corruption of Asking","text":"<p>Gary Klein found the same thing studying firefighters, nurses, and military commanders. Experts don't analyze options and select the optimal choice. They pattern-match to prior experience, recognize a workable response, mentally simulate it, and act. The decision process happens before conscious deliberation. When asked to explain, they generate a logical reconstruction that sounds like analysis but describes nothing that actually occurred.</p> <p>Daniel Kahneman put it directly: \"When we ask experts to explain their decisions, they give us retrospective rationalizations, not the actual process. The explanation sounds logical and procedural; the reality was pattern-matching and gut feel.\"</p> <p>There's a deeper problem still. In Kahneman's research on clinical prediction, something strange emerged. Take a group of experienced clinicians. Have them make judgments repeatedly: diagnoses, predictions, assessments. Then build a simple statistical model that predicts not the actual outcomes but what each clinician would say. A model of the expert, not the phenomenon. That model outperforms the expert it was trained on.</p> <p>The model extracts what's consistent in the clinician's judgments and discards what's noise. It sees the pattern in the expert's decisions that the expert cannot see in herself. Human judgment contains signal, but it's buried in variability; the same case presented twice gets different answers depending on whether the clinician is hungry, tired, distracted, or primed by the previous case.</p> <p>\"One of the major limitations on human performance is not bias,\" Kahneman observed. \"It is just noise. And there is an awful lot of it.\"</p> <p>This reframes the knowledge extraction problem entirely. The expertise is real. The pattern recognition is genuine. But it's inaccessible through articulation\u2014not because experts are hiding it, but because the expertise lives below the threshold of conscious access. And even what experts can articulate is corrupted by noise they cannot perceive.</p> <p>This is why asking experts to document their knowledge produces artifacts that don't transfer expertise. You're not capturing the signal. You're capturing a story about the signal, one the expert constructed specifically because you asked.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#what-observation-captures","title":"What Observation Captures","text":"<p>Return to Hutchins on the bridge. The knowledge of navigation wasn't in any head or any document. It was distributed across people, tools, and practices, constituted by their ongoing coordination.</p> <p>If you wanted to capture how that ship navigated, interviewing individuals would fail. Each would give you a partial, distorted fragment. The quartermaster doesn't know what the navigator knows. The navigator can't articulate the embodied skill of the bearing taker. And none of them can see the coordination patterns that emerge from their interaction. But observation captures what articulation cannot.</p> <p>Watch the work. Record the traces. See where attention flows, what gets consulted, how exceptions get handled, which patterns repeat across contexts. The theory-in-use becomes visible not through asking but through watching.</p> <p>This is what changes about the tacit knowledge problem.</p> <p>The knowledge management movement failed because it relied on articulation: ask experts, document answers, hope the documentation transfers something real. But articulation corrupts the data. You get espoused theory, not theory-in-use. You get post-hoc rationalization, not pattern recognition. You get noise-laden individual accounts, not the distributed cognition that actually produces outcomes.</p> <p>Systems that observe work rather than interrogate workers bypass this corruption entirely. They don't ask the surgeon what she notices; they track where her attention goes. They don't ask the analyst how he evaluates opportunities; they watch which factors predict his recommendations. They don't ask the team how they coordinate; they see the coordination in the traces of their work.</p> <p>What was invisible becomes legible\u2014not through better interviews, but by abandoning interviews altogether.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#a-learning-engine-at-three-layers","title":"A Learning Engine at Three Layers","text":"<p>The architecture that emerges from this insight operates across three distinct layers of organizational cognition. Each solves a different part of the tacit knowledge problem; together they create something no documentation project ever could.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#natural-language","title":"Natural Language","text":"<p>When people explain their reasoning in conversation\u2014to colleagues, to systems, to themselves\u2014they produce a different kind of artifact than formal documentation. It's messier, more contextual, closer to actual thinking.</p> <p>Consider what happens when a loan officer talks through a difficult case with a colleague. She doesn't recite the credit policy. She says things like \"this one feels like the Henderson situation from last year, but the cash flow pattern is different.\" She's revealing her actual reasoning: the analogies she draws, the features she weights, the exceptions she's learned to recognize. A question asked in a moment of genuine uncertainty reveals what the asker doesn't know. An explanation given to a junior colleague reveals what the senior person thinks actually matters, stripped of the procedural language they'd use in formal documentation. Chat and conversation capture reasoning in motion, not reasoning dressed up for the permanent record; the messy, contextual, half-formed thoughts are closer to theory-in-use than any policy manual.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#work-patterns","title":"Work Patterns","text":"<p>Connectors into the systems where work actually happens\u2014documents created, data queried, processes executed, exceptions handled\u2014reveal theory-in-use directly. Not what people say they do, but what the logs show they did.</p> <p>Consider what happens when you connect data streams that were never designed to talk to each other. Osterwalder tells a story about skincare companies sitting on buying behavior data and consumer preference data, trying to figure out how AI will change their industry. They're asking the wrong question. The real unlock is connecting that data to healthcare records: seeing which skin conditions correlate with which purchasing patterns, which ingredients actually work for which underlying conditions.</p> <p>Estee Lauder has an 8 billion dollar skincare unit and no access to the health data that will determine whether their products actually work for specific conditions. The company that connects those data streams\u2014buying patterns, skin conditions, ingredient efficacy, individual outcomes\u2014will render traditional skincare positioning obsolete. Not by making better moisturizer, but by seeing what was always there and never visible. Traditional skincare companies don't have healthcare data and aren't asking how to get it. Someone will.</p> <p>The same pattern applies inside organizations. The analyst who claims to weight financial metrics equally but whose actual queries pull customer sentiment data three times more often. The team that has an official approval process but whose email patterns show the real decisions happen in a different meeting entirely. The organization that espouses one strategy but whose resource allocation tells a different story. Work patterns don't lie. They can't. They're not constructed to satisfy a question; they're traces left by activity that happened regardless of whether anyone was watching.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#collective-intelligence","title":"Collective Intelligence","text":"<p>The most valuable knowledge often exists at a level no individual can see: patterns that emerge across hundreds of cases, approaches that work in one context that might transfer to another, anomalies that signal either emerging problems or quiet innovations.</p> <p>Booking.com runs what may be the most experiment-dense culture in corporate history. Every A/B test, every hypothesis, every result gets logged centrally. Not need-to-know; right-to-know. A junior intern in Amsterdam can look up what a VP in Singapore tested last quarter. The explicit purpose is institutional learning: what have we tried, what worked, what didn't, and why do we think so.</p> <p>But the deeper value isn't the individual experiments. It's the patterns across experiments. Which kinds of hypotheses consistently outperform expectations? Which user segments behave differently than the models predict? Where do the confident bets fail and the long shots succeed? No single product manager sees enough cases to notice these patterns. The collective does.</p> <p>This is distributed cognition made operational. The organization develops judgment that no individual possesses\u2014not because any person got smarter, but because the system learned to see across its own history.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#what-accumulates","title":"What Accumulates","text":"<p>Each layer captures something the others miss. Conversation captures intention, reasoning, and the analogies experts actually use. Work patterns capture behavior, practice, and the theory-in-use that diverges from espoused theory. Collective intelligence captures what emerges from scale: patterns visible only when hundreds of cases accumulate.</p> <p>And unlike documentation projects that go stale the moment they're completed, a learning engine accumulates continuously. Every conversation adds signal. Every work pattern refines the model. Every outcome feeds back into collective understanding. The system gets smarter the more it's used, because usage is learning.</p> <p>This is the inversion of knowledge management. The old model extracted knowledge from work, compressed it into documents, and hoped the documents would transfer something real. The new model observes work as it happens, builds understanding from behavioral traces, and keeps learning as the organization keeps working. Nothing gets extracted because nothing needs to be. The knowledge stays in the system; the system just becomes able to see it.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#the-knowing-doing-gap","title":"The Knowing-Doing Gap","text":"<p>Understanding why tacit knowledge resists extraction was never the hard part. Hutchins published in 1995. Argyris and Sch\u00f6n were writing in the 1970s. The cognitive science has been clear for decades. Anyone who wanted to understand why documentation-based knowledge management couldn't work had access to the explanation.</p> <p>And yet documentation-based knowledge management continued for thirty years.</p> <p>This is where the organizational problem reasserts itself. Even if you understand the epistemology perfectly\u2014even if you can articulate exactly why observation succeeds where articulation fails\u2014you still face the antibodies. You commission a knowledge management project. The answer comes back: stop asking experts to document; build observational infrastructure; change how you think about where knowledge lives. That answer gets killed. Not because it's wrong, but because it doesn't look like a knowledge management project.</p> <p>The wagon manufacturers faced the same bind. Some of them genuinely understood that automobiles were the future. But understanding didn't grant permission. The executive who proposed \"let's become a car company\" was proposing something that would cannibalize current revenue, require different skills, alienate existing suppliers, and threaten the jobs of everyone who had built their careers on wagon-making. The rightness of the proposal made it more threatening, not less.</p> <p>This is the cruel logic of organizational immunity. The more correct an insight, the more disruptive its implications. The more disruptive its implications, the stronger the immune response. Organizations don't kill bad ideas; bad ideas die on their own. Organizations kill good ideas that threaten the existing order.</p> <p>So we're left with an uncomfortable position. The observational approach to tacit knowledge is almost certainly right. The cognitive science supports it. The failure of alternatives confirms it. But being right has never been sufficient. The wagon manufacturers who correctly understood the automobile faced the same antibodies as those who didn't. Understanding the problem doesn't dissolve it.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#what-makes-this-moment-different","title":"What Makes This Moment Different","text":"<p>If correct understanding isn't sufficient, what is?</p> <p>The wagon manufacturers who adapted\u2014Studebaker, and arguably the few others who escaped into adjacent businesses\u2014shared something beyond correct analysis. They faced external pressure that exceeded internal resistance. Studebaker was in financial trouble. Billy Durant had already exited the wagon business and had nothing to protect. The Fisher brothers saw body-making as a new opportunity rather than a threat to an existing one.</p> <p>External forcing functions matter not because they provide insight (you might already have the insight) but because they change the calculus. When the threat from outside exceeds the threat from inside, proposals that would normally die can survive. The antibodies don't disappear; they get overruled.</p> <p>This is what's different about the current moment for organizational knowledge.</p> <p>The AI disruption isn't gentle. Consulting firms are already shedding headcount. The MIT study showing 95% of generative AI projects failing sounds like evidence of a bubble until you notice it's the same failure rate as knowledge management\u2014and knowledge management's failures didn't prevent the underlying shift from happening. They just meant incumbents missed it.</p> <p>Companies that figure out how to observe their own distributed cognition will develop institutional judgment that compounds over time. Companies that don't will watch their expertise walk out the door, fail to transfer through documentation, and gradually erode into commodity providers of whatever tasks remain unautomated.</p> <p>The competitive pressure doesn't make observation easy. It doesn't dissolve the antibodies or magically grant organizational permission. What it does is raise the cost of inaction high enough that internal resistance becomes the smaller threat. When your competitors are building learning engines and you're still running documentation projects, the argument for change gets easier to make.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/09/the-cockpit-that-remembers/#the-redistribution-of-cognition","title":"The Redistribution of Cognition","text":"<p>Hutchins's cockpit didn't just contain distributed cognition\u2014it constituted it. The arrangement of instruments, the procedures, the communication protocols weren't channels through which knowledge flowed. They were the knowledge. Change the arrangement and you change what the system knows.</p> <p>Applied AI introduces a new element into organizational cognition. Not a tool that assists individuals, but a layer of the system itself: one that remembers what no individual remembers, sees patterns no individual sees, and makes legible what was always there but never visible.</p> <p>The pessimists about tacit knowledge were right that you cannot extract it through articulation. The knowledge resists formalization because formalization was the wrong operation. You cannot compress distributed cognition into a document any more than you can compress a jazz quartet into a score.</p> <p>But observation is not extraction. It doesn't remove knowledge from the system that produces it. It makes that system reflective\u2014able to see its own patterns, learn from its own history, build on what it has already figured out.</p> <p>The thirty-year knowledge management failure contained two problems, not one. The first was epistemological: asking the wrong question about where knowledge lives and how to capture it. The second was organizational: producing immune responses against answers that threatened existing models. Both problems were real, and they reinforced each other. Wrong questions felt safe because they didn't trigger antibodies. Right questions felt dangerous because they did.</p> <p>What's changed isn't that we've suddenly developed correct understanding\u2014the understanding has been available for decades. What's changed is the external pressure. The wagon manufacturers who saw automobiles clearly but couldn't get permission from themselves to adapt faced a slow-moving threat. They had years to watch their irrelevance approach. The knowledge management parallel is faster. The organizations that build learning engines will compound their advantages. The ones that don't will discover that documented expertise transfers nothing, and that the experts themselves are increasingly optional.</p> <p>This doesn't guarantee success. Having the right question and organizational permission and competitive pressure still requires execution. Studebaker survived the wagon-to-car transition and then failed anyway, decades later, for different reasons. But Studebaker had a chance. The other 3,999 didn't.</p> <p>The observational approach to organizational knowledge isn't a guarantee. It's a chance. For most organizations, that's more than they currently have.</p> <p>Many of the ideas in this piece draw from a The AI Revolution &amp; Business Model Transformation: Osterwalder, Yu &amp; Choudary on business model transformation in the age of AI. The examples about corporate antibodies, the skincare data problem, and the Booking.com experimentation culture all originate there. What I've tried to do is thread their insights about business model innovation together with the cognitive science literature on tacit knowledge\u2014Hutchins on distributed cognition, Argyris and Sch\u00f6n on espoused theory versus theory-in-use, Klein on naturalistic decision-making, Kahneman on noise. The synthesis is mine; the raw materials belong to them.</p>","tags":["tacit-knowledge","distributed-cognition","applied-ai","business-models"]},{"location":"blog/2025/12/11/remix--remake--remade/","title":"Remix / Remake / Remade","text":"<p>In China Mi\u00e9ville's The City &amp; The City, two cities occupy the same physical space. Bes\u017ael and Ul Qoma share streets, share buildings, share the very air, yet their citizens are trained from birth to unsee the other city. You learn to recognize the architecture, the clothing, the gait of the other place, and you learn to let your gaze slide past it without acknowledgment. The unseeing isn't ignorance\u2014it's a disciplined practice, socially enforced and after long enough, automatic, a learned incapacity so thorough that the other city becomes invisible not through absence but through cultivated blindness. To see is a crime called Breach. The citizens of Bes\u017ael walk past the citizens of Ul Qoma every day, their shoulders nearly brushing, and neither acknowledges the other's existence.</p> <p>We are practicing the same unseeing now.</p> <p>The debates about AI and work have the same structure as unseeing. McKinsey publishes a report estimating that 12 million Americans will need to change occupations by 2030; the World Economic Forum counters with projections of 97 million new jobs created against 85 million displaced. Congressional hearings feature testimony about automation's impact on trucking, radiology, customer service. Think tanks model scenarios. Universities launch reskilling initiatives. The Goldman Sachs research note lands in my inbox the same week I watch a junior analyst use Claude to draft the kind of equity research that Goldman's own analysts produce\u2014the firm studying AI displacement while experiencing it, measuring a phenomenon they're inside of rather than observing from above.</p> <p>The debates are real, the numbers carefully modeled, the concerns legitimate. And yet they share a peculiar blindness: they ask what AI will do to workers while treating \"worker\" as a stable category that AI acts upon from outside. They measure displacement\u2014workers moved from one occupation to another\u2014when the deeper transformation isn't movement between categories but mutation of what the categories contain. The junior analyst isn't displaced. She still has her title, her desk, her LinkedIn profile listing the same role. But what she does, what she is in the doing of it, has already become something her job description doesn't name and her firm's displacement studies don't measure.</p> <p>We argue about whether AI will transform work as if the question were still open, as if we were standing at a decision point where different futures remained possible. But the question assumes a stability that has already dissolved. The worker who will be transformed, the job that will be automated, the skill that will become obsolete\u2014these categories presuppose a world where workers, jobs, and skills exist as stable entities that AI acts upon from outside. What if the transformation isn't an action upon existing things but a remaking of what kinds of things exist? What if the figures of the Remade are already walking among us, visible if we knew how to look, and we have learned\u2014through the same disciplined practice that lets Bes\u017ael ignore Ul Qoma\u2014not to look?</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#remix","title":"Remix","text":"<p>This is where most discourse lives: AI as tool that remixes human work.</p> <p>Garry Kasparov, after his famous loss to IBM's Deep Blue in 1997, proposed what he called the centaur\u2014human and machine in partnership, each contributing distinct strengths to a collaboration greater than either could achieve alone. The human torso of strategic insight and creative intuition mounted on the computational horsepower of the algorithm. The image was deliberately mythological, evoking ancient hybrids while gesturing toward a modern synthesis. In 2023, Ethan Mollick at Wharton led what became the largest pre-registered experiment on AI and professional work: 758 consultants at Boston Consulting Group, performing eighteen realistic tasks, with and without access to GPT-4. Analyzing the results, Mollick named two patterns of successful AI use. The centaur maintains clear boundaries between human and machine work, delegating discrete tasks at defined interfaces, preserving the seam between what the human does and what the machine does. The cyborg weaves human and AI contributions together so seamlessly that the seam itself disappears, moving fluidly back and forth across what Mollick calls the \"jagged frontier\" of AI capability\u2014the irregular boundary between what AI does well and what it does poorly, a boundary that shifts unpredictably across tasks and contexts.</p> <p>This framing isn't wrong. The BCG study showed consultants using AI completing 12.2% more tasks on average, finishing 25.1% faster, and producing 40% higher quality results\u2014numbers impressive enough to launch a thousand corporate training programs and LinkedIn posts about \"AI fluency.\" Perhaps more striking was the \"skill leveler\" effect: consultants who scored lowest at baseline saw performance gains of 43% with AI access, while top performers gained less. The productivity gains are real, demonstrable, replicable across different task types and seniority levels. The framework captures something true about how knowledge workers are learning to collaborate with language models, image generators, code assistants, and the expanding ecosystem of tools that amplify cognitive labor.</p> <p>But the centaur frame carries an assumption so deeply embedded it becomes invisible: that the human half remains human. The horse is bolted on; the torso is unchanged. The mythological centaur doesn't become a different kind of being through its fusion\u2014it remains Chiron, wise teacher, healer, distinct individual who happens to have the body of a horse. You use a tool, even a powerful one, even a tool that processes language and generates images and writes code, and you remain yourself: augmented, perhaps faster, but ontologically intact. The same person who existed before, now with a better calculator.</p> <p>This is the view from before the transformation\u2014useful for what it captures, misleading for what it assumes. Consider an analogy. In 1910, you might reasonably have debated whether the telephone would replace personal correspondence, whether the intimacy of the letter could survive the immediacy of voice. The debate would have been technically meaningful; some people did stop writing letters. But the real transformation was happening elsewhere. The automobile was remaking the city itself\u2014where people lived, how far they traveled to work, what kinds of communities were possible, what \"neighbor\" meant. The telephone-versus-letter debate fit neatly within existing categories; the automobile transformation made those categories irrelevant by changing the substrate they depended on.</p> <p>The centaur frame is the telephone debate. Real, measurable, actionable\u2014and beside the point. The question of how humans collaborate with AI tools assumes the human remains a stable entity collaborating with an external tool. It doesn't ask what happens when the collaboration becomes constitutive, when the \"tool\" isn't external but internal to the work itself, when the human who exists after the collaboration isn't the same kind of being who existed before.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#remake","title":"Remake","text":"<p>The more sophisticated frame comes from systems thinkers like Sangeet Paul Choudary, who asks us to look past first-order effects to second and third-order consequences: AI isn't automating tasks, it's collapsing coordination costs. The real transformation isn't what AI does to individual jobs but how entire systems restructure around it.</p> <p>The shipping container didn't automate ports. When Malcolm McLean loaded his first container ship in 1956, it looked like a metal box that made loading ships more efficient\u2014a tool for the existing system of moving goods by sea, an incremental improvement that would speed up the docks and perhaps reduce the number of longshoremen needed to load cargo. If you had asked \"how many dockworkers will containers displace?\" you would have gotten a number, and that number would have been real, and it would have captured almost nothing of what actually happened. Because containers didn't just automate loading. They restructured global manufacturing entirely. Factories moved to wherever labor was cheapest\u2014not because the box was smart, but because coordination costs had collapsed. When moving goods becomes nearly frictionless, the entire geography of production transforms. The container wasn't a port technology; it was a restructuring of where things could be made and how value chains could be organized across oceans and continents. The question \"how many dockworkers will containers displace?\" was real but radically incomplete, like asking how many stable hands the automobile would require.</p> <p>AI is coordination technology in the same way. When the cost of coordinating knowledge work approaches zero\u2014when expertise can be queried at scale, when judgment can be extracted from one context and applied in another, when the tacit knowledge that once required years of apprenticeship can be pattern-matched from millions of examples\u2014the question isn't which tasks get automated. The question is: what happens when expertise can be unbundled from experts? When judgment can be extracted, packaged, deployed at scale without the person who developed that judgment being present? When the friction that protected professional guilds (the years of training, the tacit knowledge, the hard-won intuition that could only be acquired through time and mentorship) becomes legible, transferable, and cheap?</p> <p>Sangeet's frame pushes past the first-order thinking of \"AI takes jobs\" to second and third-order effects: work being taken apart and reassembled around a different logic, value chains fragmenting and recombining, the system itself being remade around new coordination possibilities. This is sophisticated analysis, and it's directionally correct, and anyone thinking seriously about AI's impact on work needs to grapple with it.</p> <p>But even this sophisticated frame leaves something untouched. It describes what happens to work while the worker floats strangely outside the analysis\u2014as if humans were constants while only the system variables changed. As if the person doing the job would remain the same kind of person, just slotted into different positions in a reorganized system, playing different roles in a transformed economy but remaining fundamentally the same kind of being who played roles in the old economy.</p> <p>The container changed where factories were located. It changed logistics, trade patterns, port cities, labor relations, the geography of poverty and prosperity. What it didn't change was what a factory worker was. The worker in Shenzhen and the worker in Detroit were both recognizably workers in a way their grandparents would have understood\u2014people who exchanged labor for wages, who developed skills through practice, who knew their craft through years of doing it, whose identities were shaped by the work they did but not fundamentally transformed by the tools they used. The factory worker using a computerized lathe was still a factory worker. The logistics coordinator using supply chain software was still a logistics coordinator. The tools changed; the type of being wielding them did not.</p> <p>This is where the Remake frame, for all its sophistication, stops short. It sees the transformation of systems but misses the transformation of selves. It tracks the restructuring of work while assuming the worker remains a stable category.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#remade","title":"Remade","text":"<p>In Mi\u00e9ville's Bas-Lag novels, the city of New Crobuzon punishes criminals not through imprisonment, not through execution, but through Remaking. The Remade are surgically fused with machines, with animals, with other bodies, with whatever materials serve the punishment or the fancy of the biothaumaturge performing the modification. A woman who smothered her child has the infant's arms grafted to her face. A thief becomes part lockpick, flesh fused with metal, the tool of his crime made indistinguishable from his body. A debtor is transformed into a transport vehicle, legs replaced with wheels, torso bent to carry passengers, consciousness intact within a body that is now infrastructure. The modifications sometimes reference the crime in cruel irony, but often they're simply what Mi\u00e9ville calls \"Mad Artists fucking around with the body\"\u2014transformations without clear logic, changes imposed because the power to impose them exists.</p> <p>The Remade aren't prisoners who serve their time and return to society unchanged. They become something else. Their punishment isn't loss of freedom but transformation of what they are. The identity that existed before the Remaking\u2014the person who committed the crime, who had a history and relationships and a sense of self\u2014doesn't disappear, but it becomes entangled with a new form that didn't exist before, a body that carries the marks of power in flesh and metal. Some find new capacities in their changed forms; Tanner Sack, originally Remade with tentacles as punishment, chooses further modification, becomes amphibious, discovers freedom in the water that he never found on land. The transformation that was meant to degrade becomes the source of new possibilities. But the change is irreversible. They don't go back. The person who exists after Remaking is continuous with the person who existed before but is not the same person, cannot be the same person, because the substrate of selfhood has been transformed.</p> <p>This is closer to what's happening now than either the Remix or Remake frames can capture. But I want to be careful here, because the analogy is imperfect and the imperfection matters.</p> <p>The Remade of New Crobuzon are punished\u2014their transformation is degradation, imposed by power as sanction. That's not quite right for what's happening to workers. The consultant who learns to work with Claude isn't being punished; they're adapting, often enthusiastically, to tools that genuinely make their work faster and sometimes better. The radiologist whose diagnostic practice now involves AI isn't suffering a sanction; they're navigating a technological shift that may well improve patient outcomes. To cast all AI-related work transformation as punishment would be to miss both the genuine benefits and the genuine agency that workers exercise in adapting to new tools.</p> <p>And yet. The Remade metaphor captures something that gentler framings obscure. The transformation isn't fully chosen. The radiologist didn't vote on whether AI would enter diagnostic medicine; they adapted or became obsolete. The consultant didn't decide whether their firm would adopt AI tools; they learned them or fell behind. The adaptation is real, but it happens within constraints that weren't negotiated, weren't consented to, weren't even visible until they had already closed around the available options. The Remade who find freedom in their new forms are still Remade\u2014transformed by power they didn't choose, even when they come to embrace what they've become.</p> <p>So the metaphor is imperfect, as metaphors are. What it captures: transformation at the level of what kinds of beings exist, not just what those beings do. What it risks overstating: the degree to which the transformation is purely imposed rather than partly chosen, partly beneficial, partly a genuine improvement on what came before.</p> <p>Holding both truths in tension: the emergence of new kinds of workers who are neither the humans of before nor the AI that supposedly threatens them. Hybrid beings that didn't exist a decade ago and won't be called by any name we currently use a decade hence. Workers whose relationship to work has been transformed at a level more fundamental than which tasks they perform or which systems they operate within\u2014transformed at the level of what kind of being does the working. The transformation includes genuine gains and genuine losses, freely chosen adaptations and structurally coerced changes, improvements in capability and foreclosures of possibility. It's all of these at once, and the difficulty of holding them together is part of what makes the transformation hard to see clearly.</p> <p>Tim O'Reilly and Mike Loukides, in their recent analysis \"What If? AI in 2026 and Beyond,\" frame the AI future as two competing scenarios: economic singularity or normal technology. In the singularity version, we're in a civilizational discontinuity and all previous frameworks fail; the transformation is so rapid and total that comparison to previous technological transitions becomes meaningless. In the normal technology version, AI diffuses gradually like electricity or the internet, the hype proves overblown, and we adapt through the familiar processes of training and job transition that have characterized every technological change in living memory. Both scenarios are intelligently argued. Both are probably partially true. Both focus on what happens to jobs, to companies, to economic systems, to the measurable flows of employment and capital and productivity.</p> <p>Neither asks what happens to the worker as a kind of being.</p> <p>That's the unseeing. We can debate whether there will be more jobs or fewer, whether inequality will increase or decrease, whether AI looks more like electricity (gradual diffusion, massive productivity gains, new industries emerging) or more like the dot-com bubble (irrational exuberance, crash, eventual recovery, gains more modest than initially predicted). These are real debates about real outcomes, and they matter, and they miss the transformation happening beneath them. We've trained ourselves not to see the ontological shift\u2014the transformation not of work but of what it means to be a worker.</p> <p>The Remade are already here. We just haven't learned to name them.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#figures-of-the-remade","title":"Figures of the Remade","text":"","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#stitchwork","title":"Stitchwork","text":"<p>The first figure exists at the join.</p> <p>Think about what happens when you ask someone to synthesize outputs from multiple AI systems into a coherent deliverable. A language model generates strategic recommendations. An image model generates visuals to accompany them. A data model generates projections and forecasts. A code model generates implementation scaffolding. None of these systems speak to each other, not really\u2014they operate on different representations, optimize for different objectives, hallucinate in different ways. Someone has to take these outputs and compose them into something that passes as unified thought, something a client can receive as a coherent artifact rather than a collection of machine-generated fragments.</p> <p>This person is not editing in any traditional sense. They're not correcting errors in a document that has an author. They're not curating a collection that someone else assembled. They're doing something more intimate than either\u2014developing a feel for what the models produce, for the characteristic textures of their outputs, for the gaps between what each system generates and what the synthesis requires, for the kind of sense-making that emerges only at the seam where incompatible outputs meet. The Stitchwork reads all three or four or five outputs, feels where they misalign, produces something that passes as unified thought. The final document has no single author. The Stitchwork isn't the author but the ligament\u2014the connective tissue that holds composite outputs together, the living interface that makes machine cognition legible to human organizations.</p> <p>You see them already if you know how to look.</p> <p>In architecture firms, someone takes the parametric designs\u2014the algorithmically generated building forms that a tool like Grasshopper produces by varying parameters for light, circulation, material efficiency\u2014and reconciles them with the structural analysis from another system, the energy modeling from a third, the photorealistic rendering from a fourth. The parametric tool generates forms that look striking but may be unbuildable; the structural analyzer says what physics allows but not what's beautiful; the energy model optimizes for climate performance without knowing what the client wants to look at; the renderer hallucinates materials and textures that may not exist at any price. Someone reads all four outputs, feels where they conflict, negotiates between them to produce something buildable and beautiful and efficient enough\u2014something the client can understand as a unified vision rather than a collision of algorithmic outputs.</p> <p>In pharmaceutical research, someone bridges the protein-folding predictions from AlphaFold with the drug interaction models, the clinical trial database analysis, the regulatory compliance checks, the patent landscape surveys. None of these systems know about each other. The protein folder doesn't understand FDA requirements; the regulatory checker doesn't understand molecular biology; the patent analyzer doesn't understand clinical significance. Someone holds all of it together\u2014or more accurately, develops a feel for how to move between these systems, when to trust one over another, how to translate between their different languages of possibility and constraint. The drug candidate that eventually reaches trials, if it does, emerges from this translation work as much as from any single analysis.</p> <p>In legal practice, someone synthesizes the contract analysis AI's clause-by-clause parsing with the case law research system's precedent identification, the risk assessment model's scenario projections, and the negotiation strategy generator's tactical recommendations. Each system is sophisticated in its domain; none understands the gestalt of what a deal actually is\u2014the relationship between parties, the unspoken interests, the leverage dynamics that determine which clauses matter. Someone reads all four outputs and produces advice that sounds like it came from a partner, not because it did, but because that someone has learned to make machine cognition pass as professional judgment.</p> <p>Their value isn't in any particular domain expertise, isn't in writing ability or analytical skill in isolation. Their value is in the joining\u2014in knowing how to read multiple AI systems well enough to weave their outputs into something coherent, and in knowing enough about the domain to recognize when the weaving has failed.</p> <p>The name carries the textile history of labor: piecework, seamstresses, the feminized work of assembly that has always been invisible and essential, that holds production together while the visible labor of creation gets the credit. Stitchwork has always existed. What's new is that the Stitchwork isn't assembling cloth or components. They're assembling cognition. Their body\u2014their judgment, their taste, their pattern-recognition, the ineffable sense of what fits together and what doesn't\u2014is the thread.</p> <p>The Stitchwork doesn't use AI. They are the interface AI requires.</p> <p>And here's what makes this a Remaking rather than just a new job: the Stitchwork's value is constituted by the systems they bridge. There is no version of the Stitchwork that exists without AI to stitch. They haven't taken an existing skill and applied it to a new context, haven't transferred expertise from one domain to another. They've developed a capacity that has no independent existence, that exists only in relation to the seams between systems, that didn't exist as a possible way of working until the systems created the seams. The Stitchwork isn't a human who uses AI tools. The Stitchwork is a new kind of worker, produced by the existence of AI systems that need to be joined.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#loopwork","title":"Loopwork","text":"<p>The second figure is caught in the spiral.</p> <p>The Loopwork trains AI that trains workers who train AI. They exist in recursive loops of capability transfer where expertise has no stable ground\u2014it exists only as the difference between model versions, as the delta between what the system knew yesterday and what it knows today.</p> <p>You see them already in the content moderators and RLHF trainers and data labelers, the humans-in-the-loop whose judgments become training signal, whose preferences shape model behavior, whose expertise is extracted not through documentation but through the choices they make in evaluating outputs. But Loopwork extends far beyond these obvious cases, into professions that don't yet recognize themselves as caught in the spiral.</p> <p>Consider the senior radiologist at a teaching hospital who reviews AI-generated diagnoses, marking the false positives and missed findings, training the model to see more accurately. She knows that the improved model will generate the preliminary readings that residents now review, that those residents are developing their diagnostic eye in relation to AI outputs rather than through the years of staring at unassisted films that shaped her own expertise. She trained herself to see tumors in shadows, spent thousands of hours learning the visual grammar of pathology. Now she trains a system that trains residents who will train the next version of the system. The expertise doesn't accumulate in people anymore; it flows through the loop, deposited in model weights, accessed by the next generation through interface rather than intuition. She is the last radiologist who learned to see without the machine. The residents she trains will be Loopwork from the start.</p> <p>Consider the investment analyst whose research notes become training data for the firm's proprietary model. Every thesis she writes, every earnings call she interprets, every industry trend she identifies feeds the system that newer analysts will use to generate their own theses. Those analysts will never develop the nose for bullshit that comes from reading a thousand annual reports with nothing but a highlighter and suspicion. They'll develop a different skill\u2014knowing when to trust the model's pattern-matching and when to override it\u2014but that skill exists only in relation to the model's outputs. Her judgment, extracted through the training pipeline, lives on in model behavior long after she's moved on. The firm's competitive advantage isn't her expertise anymore; it's how well the training captured her expertise before she left.</p> <p>Consider the litigation partner whose case strategies become the training corpus for legal AI. Every motion she drafts, every deposition she plans, every settlement she negotiates teaches the system how to think about cases. New associates use that system to generate their own draft strategies, which she reviews and corrects, which trains the next version, which new associates use. She's at the top of the loop\u2014her judgment shapes the system that shapes the associates who shape the system\u2014but she's still in the loop, still contributing to a process whose endpoint is her own obsolescence, still training the tool that will make tools like her unnecessary.</p> <p>The Loopwork's knowledge is relational. They know what the last version of the model got wrong and how to correct it. They develop an exquisite sensitivity to the gap between what the model produces and what quality looks like, between the pattern the system has learned and the pattern it should have learned. When the model updates, their expertise partially obsoletes\u2014the gaps they knew how to close have closed, and new gaps have opened, gaps whose shape they must learn all over again. They train their way toward their own redundancy, except that redundancy never quite arrives, because the loop keeps turning, and each correction reveals new gaps, and the Loopwork is employed precisely to close gaps that their closing will create.</p> <p>Ouroboros: the snake eating its own tail. The Loopwork is eating and being eaten, consuming their own expertise to feed a system that will render that expertise unnecessary, except that the necessity never quite ends, just transforms. Not victimized; the loop provides employment, status, the satisfaction of visible improvement, the metrics that prove the system is getting better because you made it better. But the Loopwork's expertise isn't knowledge that accumulates into stable competence. It's delta. It's the space between what the model was and what it's becoming. It's expertise that exists only in motion, only in the gap, only in the moment before the gap closes.</p> <p>When the loop closes\u2014when the model becomes good enough that human correction no longer improves it, when the delta shrinks to zero, when the gap that defined the Loopwork's value disappears\u2014the Loopwork dissolves. Not fired, exactly. Just no longer a coherent category. The job ends not through elimination but through the evaporation of the gap that defined it. And here's the peculiar thing: the Loopwork contributed to their own dissolution. Their expertise, successfully transferred, erased the need for their expertise. They trained themselves out of relevance, and the training was their job, and they did it well.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#edgework","title":"Edgework","text":"<p>The third figure lives at the margins.</p> <p>The Edgework is employed for what can't be systematized. Not because it's ineffable or creative or essentially human in some mystical sense, but because it operates at the edges where systems fail, where categories fray, where the smooth operation of the machine meets friction and requires something that can navigate ambiguity without freezing. Every system optimized for the center produces failure at the margins. Every process designed for the typical case generates exceptions. Every model trained on common patterns stumbles when patterns break.</p> <p>You see them already: the fixer who makes things happen when processes break down, who knows which rules can be bent and which relationships can be leveraged and which workarounds actually work. The troubleshooter who handles the cases that don't fit the taxonomy, that the system flags as anomalies, that fall into the gaps between categories the system recognizes. The person everyone calls when the AI produces outputs that make no sense\u2014not wrong in a correctable way, but wrong in a way that reveals the system has encountered something outside its training distribution, something it wasn't built to handle.</p> <p>Consider the claims adjuster at an insurance company whose job has transformed into edge navigation. The AI handles the straightforward claims\u2014the fender benders with clear fault, the routine medical procedures with standard billing codes, the property damage with good documentation. What reaches the adjuster now is everything else: the multi-vehicle accident with conflicting witness accounts and poor-quality dashcam footage, the experimental treatment that doesn't map to any billing category, the water damage claim where the model can't distinguish renovation fraud from legitimate repair. She spends her days in the territory the model has carved out by its own competence\u2014the remaining claims are non-routine by definition, precisely because routine has been absorbed by the system. Her expertise is reading the model's confusion, understanding why it flagged something as anomalous, navigating between what the policy actually covers and what the model thinks it understands.</p> <p>Consider the social worker whose caseload has stratified. The AI triage system routes families to appropriate services efficiently, matching needs to resources, predicting intervention outcomes, optimizing for throughput. But certain cases don't route cleanly\u2014the family whose situation falls between categories, the intervention that worked poorly despite all the predictors suggesting success, the client who refuses to engage with the system's recommendations. She handles what the system can't systematize: the human mess that resists categorization, the relationships that don't fit models, the judgment calls that can't be justified by reference to any algorithm. Her caseload is made of edges.</p> <p>Consider the technical writer whose role has inverted. Once she wrote documentation from scratch, translating engineer knowledge into user-accessible language. Now AI generates the first draft, and she handles what the generation can't: the edge cases where the documentation misleads, the scenarios where technically correct instructions produce user confusion, the gap between what the system describes and what users actually experience. She's become an edge detector, reading AI-generated docs not for accuracy in the normal case but for failure in the exceptional one. Her value isn't writing anymore; it's knowing where writing fails.</p> <p>But Edgework is becoming more than exception-handling. It's becoming a mode of being, a permanent residence at the margins rather than an occasional visit to handle anomalies before returning to the center.</p> <p>The more systems are optimized for the center, the more they fail at the edges. The better AI handles routine work, the more the remaining human work is non-routine by definition\u2014the exceptions accumulate as the rules get better, the edge cases multiply as the center cases get absorbed. The Edgework lives permanently in the margins, employed precisely for their capacity to operate where specifications end, where the process doesn't have a step for what's happening, where judgment can't be justified by reference to rules because the rules don't reach that far.</p> <p>Sociologists use \"edgework\" to describe voluntary risk-taking at the limits of control: skydivers, BASE jumpers, those who seek the boundary between order and chaos because something in the human psyche is drawn to the edge. The Edgework of AI systems doesn't choose the edge. The edge is where they're employed because it's where they're needed. But there's something of that sociology in the figure: the Edgework develops expertise in liminality, in operating without clear ground, in making judgments that can't be justified by reference to rules because the rules don't reach that far. They become comfortable with ambiguity in a way that would terrify workers whose value lies in the center\u2014comfortable because ambiguity is their habitat, because the margin is where they live, because the system's failure mode is their employment condition.</p> <p>The Edgework is the residue of human friction that the system produces as byproduct and consumes as fuel. They exist because systems are imperfect, and they exist in a form shaped by the particular imperfections of the systems they serve. A different system would produce different edges, would employ different Edgework, would require different capacities for navigating different kinds of ambiguity. The Edgework's expertise is not general-purpose human judgment but edge-specific navigation\u2014knowing this system's margins, this process's exceptions, this model's failure modes. Change the system and the Edgework must change too, must learn new edges, must develop new capacities for new margins.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#what-these-figures-dont-cover","title":"What These Figures Don't Cover","text":"<p>Three figures don't exhaust the landscape. They describe modes of human-AI integration in knowledge work\u2014the places where cognition meets cognition, where human judgment interfaces with machine processing. But work isn't only cognition.</p> <p>The physical labor that AI cannot touch\u2014or cannot touch yet\u2014doesn't become Stitchwork or Loopwork or Edgework. The plumber crawling under a sink, the home health aide lifting a patient, the roofer navigating pitch and weather: these workers may use AI for scheduling, for inventory, for finding the next job. They may become more efficient because AI handles their back office. But their bodies remain their own; their expertise remains embodied in muscle and bone rather than constituted by seams between systems. They're augmented in the centaur sense\u2014tools bolted on, torso unchanged.</p> <p>Or they may not be. The carpenter whose work gets measured by AI quality control, whose every joint gets photographed and assessed, whose pay varies with algorithmic judgment of their craftsmanship\u2014has that carpenter become Loopwork? Is her expertise now delta, the difference between what the vision model rates as quality and what she knows to be right? The nurse whose triage gets second-guessed by diagnostic AI, who spends hours explaining to algorithms why this patient should jump the queue despite what the model says\u2014has she become Edgework, permanently stationed at the margins of a system that doesn't understand the exceptions she handles? The physical-cognitive boundary is blurrier than it looks.</p> <p>And there's labor that AI doesn't integrate with but simply erases. The call center that closes entirely because the model handles every query. The paralegal pool that shrinks from fifty to five because document review now takes hours instead of weeks. The translation bureau that becomes one person and a Claude subscription. These aren't Remade\u2014they're displaced in the old-fashioned sense, their work absorbed rather than transformed. The displacement debates capture their fate even if they miss the more complex mutations happening elsewhere.</p> <p>I'm also not describing what work should look like. Whether the Stitchwork architect is flourishing or suffering, whether the Loopwork radiologist's life is better or worse than her predecessor's, whether the Edgework claims adjuster finds meaning in her margins\u2014these questions matter enormously, and this essay doesn't answer them. The figures describe transformations, not evaluations. They map what's emerging, not what we should want. Some of the Remade may thrive, developing capacities and satisfactions that couldn't exist before. Others may burn out, caught in perpetual adaptation, never stable enough to build a life around what they do. The emergence of new kinds doesn't determine whether those kinds will be worth becoming.</p> <p>What this essay does claim: that for a significant and growing portion of the workforce, particularly in knowledge work, the transformation isn't happening to workers from outside\u2014it's happening through them, changing what kinds of workers exist. That the old questions about automation and augmentation presuppose stability that has already dissolved. That the debates of 2025, for all their sophistication, may be asking the wrong things at the wrong level. Whether the new questions we'll need are better or worse than the old ones\u2014that's a different essay, and probably one we can't write yet, because we don't know what the questions are.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#what-we-stop-debating","title":"What We Stop Debating","text":"<p>The debates of 2025 are not foolish. When economists at MIT estimate that each robot installed in manufacturing displaced 3.3 workers while reducing wages for remaining workers, they capture something real. When labor advocates warn that AI threatens the middle-skill jobs that once provided pathways to stability, they speak to genuine anxieties about family formation, community cohesion, the dignity that comes from work that supports a life. When optimists point to historical patterns\u2014the mechanization of agriculture eliminated 97% of farm jobs while creating more prosperous societies\u2014they too speak truth. The anxiety has precedent; the optimism has precedent; both rest on real evidence interpreted through frameworks that have served us well.</p> <p>The problem isn't that the debates are wrong. The problem is that they ask what AI will do to workers while treating \"worker\" as a natural kind, like \"carbon\" or \"primate\"\u2014a category that exists independently and that AI acts upon from outside. What happens to arguments about displacement and augmentation when the entity that would be displaced or augmented has already become something else?</p> <p>Consider what happened to debates about whether the automobile would help or harm the horse industry. Some predicted massive job losses for stable hands and farriers; others predicted new roles in automobile maintenance and manufacture. Both predictions came true, in a sense. Stable hands did lose jobs; mechanics did emerge. But the predictions shared an assumption that proved false: that the relevant question was whether people who worked with horses would find work with cars. The automobile didn't just change work; it changed cities, changed where people lived, changed what \"commuting\" meant, changed the meaning of distance itself. The horse industry debates captured the first-order effects while missing the second and third-order restructuring. The question wasn't wrong, but it was beside the point\u2014a question about the wrong level of the phenomenon.</p> <p>Fast forward from that recognition. 2035.</p> <p>We don't debate whether email destroyed professional communication. The question sounds absurd\u2014email is how professional communication happens, the substrate so thoroughly naturalized that asking whether it \"destroyed\" something feels like asking whether breathing destroyed lung function. We don't debate whether calculators ruined mathematical intuition. Calculators are how you do math; intuition is something else, something that still exists but exists differently, and the relationship between them isn't a debate but a fact about practice, a settled question that settled so thoroughly we've forgotten it was ever a question.</p> <p>From 2035, the debates of 2025 look similar. Not wrong, exactly, but oddly shaped, like arguments about a world that no longer exists conducted in terms that no longer apply. \"Will AI take jobs?\" assumes something stable called \"jobs\" and something external called \"AI,\" and asks about the relationship between these two distinct entities as if they could be weighed against each other on a scale. The question dissolves not because jobs don't matter or because AI isn't transformative\u2014both claims remain true\u2014but because the entities presupposed by the question have already merged. What does \"taking jobs\" mean when the job is defined by its relationship to AI? What does \"augmentation\" mean when the human has no existence independent of the system they navigate?</p> <p>The worry underlying the debates was legitimate: that people would suffer, that livelihoods would be destroyed, that the gains would flow to those who needed them least while the losses fell on those least able to bear them. That worry doesn't dissolve just because the questions misframed the phenomenon. People may still suffer; livelihoods may still be destroyed; gains may still flow upward while losses cascade down. But the mechanism is different from what the debates assumed. Not displacement\u2014transformation. Not automation\u2014integration. Not the replacement of humans by machines but the emergence of hybrid kinds that the old categories cannot name.</p> <p>Think about how differently we relate to these figures than we do to traditional jobs.</p> <p>The architect who becomes Stitchwork bridging parametric design and structural analysis\u2014is that the same job as architect, augmented? Or a different kind of work that happens to involve buildings? The skills that made her a good architect (spatial reasoning, aesthetic judgment, client communication) may help her become good Stitchwork, but the Stitchwork role isn't \"architect plus AI.\" It's something else, constituted by the seams between systems that didn't exist when architecture was architecture. There's no job description from 2020 that describes what she does now. There's no training program that prepares for it directly. There's no career path that leads to it through sequential development.</p> <p>The radiologist caught in Loopwork\u2014is she still practicing medicine? In some legal and institutional sense, yes; she reviews diagnoses, she bears liability, she maintains credentials. But her relationship to knowledge has transformed. She doesn't develop expertise that accumulates; she develops sensitivity to gaps that shift. Her juniors don't learn medicine from her in any traditional sense; they learn to navigate a human-AI diagnostic system in which her judgment has become one input among several, accessible through the model rather than through mentorship. The professional identity of \"radiologist\" persists, but what it means to be one has mutated beyond what the category was designed to contain.</p> <p>The claims adjuster living in Edgework\u2014is she still doing insurance work? The routine claims that defined the job for a century have been absorbed. What remains is the anomalous, the contentious, the human mess that resists categorization. Her work is harder than it used to be, more demanding of judgment, but also more contingent\u2014she exists because the system has edges, and better systems might have fewer edges, and the edges she navigates today may be absorbed tomorrow. Her expertise is real but temporary, defined by the current system's limitations rather than by any stable body of knowledge.</p> <p>What we stop debating in 2035 isn't the underlying concerns. The concerns were real; they remain real. What dissolves is the frame that made the debates possible.</p> <p>Take automation. \"Will AI automate claims adjusters?\" was a serious question, and the seriousness hasn't disappeared. People lose livelihoods when their work gets absorbed by systems. Families fracture; communities hollow out; the quiet desperation of purposelessness spreads. These harms are real whether or not the category \"claims adjuster\" survives to be counted. But the question assumed there was a stable entity called \"claims adjuster\" that AI would either replace or leave alone\u2014a binary that made measurement possible. The claims adjuster of 2025 is already Edgework; her role is constituted by what AI can't do, which means her role transforms with every improvement in AI capability. You can't measure automation rates for a target that moves with the technology. The question doesn't become less important; it becomes unanswerable in its original terms. Automate more and the Edgework moves; she doesn't disappear or persist\u2014she transforms into something the original question can't track.</p> <p>Or take skills. The policy apparatus of 2025\u2014community college certificates, workforce development grants, corporate reskilling budgets, the whole infrastructure of \"lifelong learning\"\u2014rested on a model: workers possess skills, skills become obsolete, workers acquire new skills, employment continues. This wasn't wrong as a model; it captured how humans navigated technological change for two centuries. The weaver learned to operate the power loom; the secretary learned word processing; the factory worker learned CNC machining. Skills were tools you acquired, carried between jobs, updated when necessary. The model worked because workers remained workers\u2014the same kind of being, adapting to new tools.</p> <p>But the Stitchwork's capacity isn't a skill she possesses; it's a relation she embodies. The feel for bridging AI systems doesn't transfer to bridging different AI systems in any straightforward way; each configuration of systems produces its own seams, its own synthesis requirements, its own texture of failure that must be learned anew. The Loopwork's expertise is pure delta\u2014the gap between model versions, the difference between what the system knew yesterday and what it knows today. When the system updates, the expertise partially obsoletes. You can't create a reskilling program for relational capacities that exist only in the moment between system states. A training program to become Stitchwork would have to teach you to bridge systems that don't yet exist, for outputs that nobody has seen, in configurations that haven't been developed. The reskilling frame imagines skills as property you own; the Remade don't own their capacities, they embody relations to systems that shape them.</p> <p>And inequality? We know how to ask whether the gains from technology are distributed fairly\u2014whether automation creates winners and losers, whether the productivity bounty reaches workers or flows only to owners. These questions assume we're comparing variations on a common type: humans selling labor in markets that evaluate that labor by some shared metric.</p> <p>But how do you compare the Stitchwork architect and the Edgework claims adjuster? One creates value by joining systems, the other by navigating their failures. One succeeds by synthesis, the other by exception. Their outputs aren't commensurable; their skills don't translate; their positions in the economy relate to different logics of value creation. The Stitchwork's value increases when systems proliferate and need joining; the Edgework's value increases when systems fail at their margins. These aren't different positions in a single economy; they're different economies masquerading as a single labor market.</p> <p>The inequality is real\u2014some of the Remade are well-compensated knowledge workers in prestigious firms; others are precarious gig workers handling the exceptions that algorithms produce. But the frame that asks \"are the gains fairly distributed?\" assumes there's a common thing being distributed. What if the Stitchwork architect and the Edgework claims adjuster aren't more and less successful versions of the same kind of worker but different kinds of beings, produced by different relationships to different systems, playing different games with different rules? How do you distribute gains across kinds?</p> <p>The disappearance of old questions doesn't mean the emergence of good answers. It means the emergence of new questions we don't yet have vocabulary for. Perhaps questions about dignity across kinds rather than equality within kinds\u2014what does respect look like for beings whose value is constituted differently? Perhaps questions about agency in transformation\u2014not whether to be transformed, but how, and with what awareness. Perhaps questions about the production of kinds\u2014who decides what kinds of workers emerge, and in whose interest are the seams and loops and edges designed?</p> <p>In 2035, we might have the vocabulary for these questions. Or we might have new questions we can't anticipate from here, questions that will make these seem as oddly shaped as \"will the telephone replace the letter?\" seems to us now. What we won't have is the debates of 2025\u2014not because they were answered, but because the world they described has transformed into something else, and we've transformed with it.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/11/remix--remake--remade/#the-choice-that-wasnt","title":"The Choice That Wasn't","text":"<p>The Remade of New Crobuzon were made by courts and Mad Artists. Power with a face, punishment you could name, transformation imposed by identifiable authorities whose decisions could in principle be protested, resisted, appealed. The Remade could point to the moment of their Remaking, could identify the judge who sentenced them and the biothaumaturge who performed the surgery, could tell a story about how they became what they are that had villains and victims and a clear before and after.</p> <p>The Remaking we're living through has no author.</p> <p>It happens through job descriptions rewritten to include \"AI fluency\" and \"ability to work with AI tools,\" phrases that seem like requirements for a job but are actually specifications for a kind of being. Through performance metrics that measure human-AI hybrid output without naming it as such, that evaluate workers on deliverables whose production requires integration with systems, that make the unaugmented worker literally unable to meet the baseline. Through org charts redrawn around \"pods\" and \"squads\" optimized for model integration, structures that assume AI involvement as default and create positions whose only purpose is to bridge human organization and machine capability. Through the slow redefinition of \"skill\" from what you know to what you can extract, from knowledge that lives in your head to fluency in getting knowledge out of systems that have absorbed what used to live in heads like yours. Through titles that name positions in a system rather than capabilities of a person\u2014titles that describe relationships to AI rather than competencies independent of AI.</p> <p>This is Foucault's productive power: not repression but production, not the crushing of existing forms but the generation of new ones. The corporation doesn't force workers into new forms. It doesn't have to. It produces the forms\u2014writes the job descriptions, designs the workflows, specifies the metrics, creates the positions\u2014and workers become the forms because that's what employment means, because the alternative to becoming the form is becoming unemployed, because the coercion is structural rather than personal and therefore invisible as coercion. The Stitchwork isn't coerced into bridging AI systems. The job description asks for \"ability to synthesize outputs from multiple sources\" and someone becomes Stitchwork because that's the shape of the position, because fitting the shape is how you get the job, because the alternative is a different shape or no shape at all.</p> <p>The Loopwork isn't forced into the recursive spiral. The training pipeline needs humans in the loop, calls it \"quality assurance\" or \"fine-tuning support\" or \"model evaluation,\" titles that sound like jobs applied to tasks rather than descriptions of a new mode of being. Someone becomes Loopwork because that's what the work requires, because the pipeline is structured to consume human judgment and they're the human whose judgment is being consumed, because saying no to the loop means saying no to employment. The Edgework isn't conscripted to the margins. The system optimizes its center, exceptions accumulate at the edges, someone is hired to handle exceptions, and they become Edgework because the edge is where they live\u2014not because anyone decided they should live there, but because the edge is where the opening was, the gap in the system that needed filling.</p> <p>No conspiracy. No villain. No mastermind plotting the Remaking for nefarious purposes. Just the ordinary operations of economic systems reorganizing around a new coordination technology, capital flowing toward efficiency, efficiency requiring new arrangements, new arrangements producing new kinds of workers. The logic is impersonal, distributed across thousands of decisions made by thousands of actors, each decision locally rational and collectively transformative. HR managers writing job descriptions. Team leads defining workflows. Product managers specifying features. Engineers building systems. Each doing their job, none intending to remake the worker, all contributing to a Remaking that emerges from their collective action without being anyone's intention.</p> <p>The workers are us.</p> <p>The unseeing is essential here. If we saw the Remaking clearly, we might ask uncomfortable questions. Who benefits from Stitchwork? The platforms that produce incompatible outputs benefit when someone else bears the cost of joining them\u2014when the integration work is done by human labor rather than built into the systems, when the seams are navigated rather than eliminated. Who benefits from Loopwork? The companies that extract training data from the loop, that turn human judgment into model improvement, that harvest expertise through the recursive spiral rather than paying for expertise directly. Who benefits from Edgework? The systems that externalize their failure modes to human handlers, that can optimize for the center because someone else deals with the margins, that achieve efficiency by pushing the costs of inefficiency onto workers who absorb them as job requirements.</p> <p>The figures of the Remade aren't natural kinds. They're produced by specific arrangements of power and capital, arrangements that have beneficiaries even if they have no authors.</p> <p>But we don't see it. We see \"upskilling\" and \"adaptation\" and \"the future of work.\" We see LinkedIn posts about prompt engineering and articles about AI collaboration and think pieces about which careers are \"AI-proof.\" We see the surface of transformation\u2014the tools, the capabilities, the productivity gains, the disruption\u2014and unsee its shape, the way the transformation is producing not just new tools but new kinds of workers to use them.</p> <p>Some of the Remade in Mi\u00e9ville's novels reclaim their forms. Tanner Sack chooses further modification, makes the transformation his own, finds freedom in what was imposed as punishment. The change that was meant to degrade becomes the source of new capacities, new possibilities, new ways of being in the world. This happens too, in the Remaking we're living through. The Stitchwork develops expertise that couldn't exist before, becomes virtuosic at a kind of synthesis that wasn't possible when there was nothing to synthesize, finds satisfaction in the bridging that defines their work. The Loopwork accumulates knowledge of model behavior that has real value, develops intuitions about AI systems that few others possess, becomes expert in a domain that didn't exist a decade ago. The Edgework builds judgment that the center cannot replicate, becomes indispensable precisely because they've learned to navigate what the system can't handle, turns marginality into mastery.</p> <p>But Tanner Sack's freedom doesn't retroactively make the Remaking just. Finding power in your changed form doesn't mean the change was chosen. The Remade who thrive are still Remade\u2014still transformed without consent, still shaped by power they didn't agree to, still living in bodies that were made for them rather than by them. Agency within constraint is real and valuable and doesn't erase the constraint. Making the best of a transformation doesn't validate the transformation.</p> <p>The debates we're having assume we're still in the moment before\u2014that we're deciding whether to allow transformation, that we're choosing between futures, that the questions about AI and jobs are meaningful because we haven't yet crossed the threshold into a world where they no longer apply. This is the deepest unseeing. The Remaking has begun. The figures walk among us. They are us.</p> <p>The choice wasn't offered, because there was no moment of choice. Only a thousand small adaptations\u2014a job application here, a workflow change there, a skill learned, a metric met, a title accepted\u2014that added up to transformation.</p> <p>No one decided to become Stitchwork. The architect applied for a job that mentioned \"integrated design delivery\" and \"multi-platform synthesis.\" She developed a feel for bridging AI outputs, became known as the person who could make sense of conflicting systems, found herself spending more time joining outputs than producing them. One day she realized that her value was the bridging itself, that she couldn't describe her expertise without reference to the systems she bridged, that there was no version of her professional self that existed independent of the seams she navigated. She was still called an architect. But architecture had become something else.</p> <p>No one chose the loop. The radiologist took a role reviewing AI diagnoses, flagging errors, improving the model. She became good at it\u2014developed intuitions about what the model missed, learned to see through its eyes while correcting its vision. She trained her way into the spiral, and discovered eventually that her expertise existed only in the delta, only in the gap between model versions, only in the moment before her corrections became the model's baseline. Her juniors learned from the system that contained her judgment rather than from her directly. She was still called a radiologist. But radiology had become something else.</p> <p>No one volunteered for the margins. The claims adjuster handled exceptions because that was the work that remained, developed a feel for edges because edges were what she navigated, found herself permanently in the liminal space where the AI flagged uncertainty and human judgment was required. She understood eventually that the margin was her habitat, that she had adapted to a niche the system produced, that her expertise was constituted by the system's limitations and would transform when those limitations changed. She was still called a claims adjuster. But claims adjusting had become something else.</p> <p>The Remade look in the mirror and see workers. They've learned, like the citizens of Bes\u017ael, not to see what they've become, not to recognize the transformation in their own reflection, not to name the new forms emerging from the old categories. The unseeing that allows two cities to occupy the same space allows us to occupy a transforming economy without recognizing the transformation, to become new kinds of beings while debating whether new kinds of beings will emerge, to walk past the Remade every day\u2014to be the Remade every day\u2014without acknowledging their existence.</p> <p>To see clearly now would be a kind of Breach: a violation of the trained incapacity that lets us function, an acknowledgment of what we've agreed not to acknowledge, a naming of what we've agreed to leave unnamed. The citizens of Bes\u017ael who see Ul Qoma are taken by Breach, removed from both cities, placed somewhere outside the system they violated. Perhaps that's why we don't look. Perhaps the cost of seeing is exile from the only economy that will have us.</p> <p>And yet I'm not sure what seeing accomplishes.</p> <p>In Mi\u00e9ville's novel, Breach is an entity\u2014a power that enforces the unseeing, that removes those who violate the separation. The citizens of Bes\u017ael don't unsee Ul Qoma because they're foolish; they unsee because seeing is punished, because the system that maintains the separation has teeth. Our unseeing has no equivalent enforcer. We could see if we chose to. We could name the Stitchwork, the Loopwork, the Edgework\u2014could recognize ourselves in the descriptions, could acknowledge the transformation in the mirror. Nothing stops us except the usual things: the mortgage, the health insurance, the children's school fees, the reasonable fear that naming what we've become won't change what we've become but might make it harder to bear.</p> <p>Tanner Sack, the Remade who embraced his transformation, found freedom in the water. But he found it in Armada, a floating city of pirates and refugees, a place outside the empire that Remade him. He couldn't have found that freedom in New Crobuzon, where the Remade serve their sentences in the forms the state imposed. The freedom to own your transformation requires somewhere to stand while owning it\u2014a position outside the system that transforms you, or at least stable enough to see clearly without the seeing destroying you.</p> <p>I don't know where that position is for us. The Stitchwork architect might recognize herself in this essay and feel... what? Validated? Diagnosed? She still has to go to work Monday. She still has to bridge the parametric designs and the structural analysis, still has to make machine cognition pass as unified vision. Naming what she does doesn't change the doing. It might make the doing more conscious, more intentional. Or it might just add a layer of alienation\u2014now she's not only Stitchwork but Stitchwork who knows she's Stitchwork, performing a role while watching herself perform it.</p> <p>This essay is itself a kind of Breach\u2014it names what we've agreed to leave unnamed, treats transformations that are supposed to be invisible as objects worth examining. Maybe that examination has value even without a program attached, even without a policy prescription or a call to action. Maybe the first step is to see what's there, not because seeing will save us but because unseeing hasn't, and at least seeing lets us ask better questions than the ones we've been asking.</p> <p>Or maybe this essay is just another form of adaptation\u2014another way of fitting the new shape while calling it something else. The Loopwork writes about Loopwork; the delta becomes the text; the gap I describe is the gap I inhabit. I can't tell, from inside the transformation, whether naming it is resistance or accommodation. Whether seeing clearly changes the thing seen or just adds another layer to the unseeing\u2014a more sophisticated blindness, a blindness that knows itself and therefore feels like sight.</p> <p>The Remade walk among us. We are the Remade, some of us, already transformed into kinds that didn't exist a decade ago. The old debates will continue because institutions are slow and categories outlast their referents. But the transformation underneath doesn't wait for the debates to catch up. It proceeds through job descriptions and performance metrics, through the thousand small adaptations that add up to becoming something else.</p> <p>I've tried to see clearly. I've tried to name what I see. What comes after seeing\u2014whether the naming opens possibilities or forecloses them\u2014is not a question I can answer from inside the transformation.</p> <p>We are all, now, from inside.</p>","tags":["ai","future-of-work","labor","transformation"]},{"location":"blog/2025/12/12/on-slop/","title":"On Slop","text":"<p>The word arrived with the force of revelation. Slop. Scrolling through feeds thick with AI-generated images, articles, videos\u2014content that felt somehow wrong, uncanny, excessive\u2014we finally had a name for the unease. The term spread because it captured something visceral: the texture of language that reads smoothly but says nothing, images that resolve into coherence without ever achieving meaning, an endless tide of stuff that nobody asked for and nobody quite wanted.</p> <p>The complaint is now ubiquitous. Graphite's analysis of web content found that over half of new articles are now AI-generated. Bynder's research shows a majority of consumers report reduced engagement when they suspect content is machine-made. The Macquarie Dictionary named \"AI slop\" its 2025 word of the year. The diagnosis seems clear: AI is flooding our information environment with garbage, and the flood is drowning authentic human expression.</p> <p>But what if the diagnosis is wrong? Not factually wrong (the volume is real, the uncanniness genuine) but conceptually wrong. What if \"slop\" is a category error that mistakes volume for vice, aesthetics for ethics, and origin for orientation? The question isn't whether AI produces garbage. It does, in abundance. The question is whether that's the right thing to be worried about.</p>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/12/on-slop/#frankfurt-on-bullshit","title":"Frankfurt on Bullshit","text":"<p>To understand what the slop complaint actually tracks, we need a philosopher who spent considerable effort on a related phenomenon. In 1986, Harry Frankfurt published a short essay called \"On Bullshit\" that became, improbably, a bestseller when reissued as a book two decades later. Frankfurt's project was conceptual excavation: what exactly is bullshit, and why does it differ from lying?</p> <p>His answer was precise and unsettling. The liar knows the truth and deliberately contradicts it. The bullshitter doesn't care about the truth one way or another. This seems like a minor distinction; both are forms of deception. But Frankfurt argued it was fundamental. The liar operates within the same game as the truth-teller; they're just on opposite teams. The liar must understand reality to distort it. The bullshitter abandons the game entirely.</p> <p>This is why Frankfurt concluded, counterintuitively, that bullshit is more corrosive than lying. Liars still respect truth enough to work around it. Bullshitters erode the very capacity to distinguish true from false because they've stopped caring about the distinction. Someone who lies habitually can still recognize truth; someone who bullshits habitually gradually loses this ability.</p> <p>Now apply this framework to AI-generated content. The slop complaint treats the problem as one of quality\u2014AI output is cheap, repetitive, uncanny, excessive. But Frankfurt's framework suggests a different question entirely: what relationship to truth do the humans deploying these systems maintain?</p> <p>An AI system generating text has no relationship to truth whatsoever. It produces token sequences based on statistical patterns. It cannot bullshit in Frankfurt's sense because bullshitting requires the capacity to care about truth and the choice not to. The AI isn't indifferent to truth; it has no concept of truth to be indifferent toward.</p> <p>But the humans deploying AI systems\u2014they can bullshit. And this is where the framework bites. Someone using AI to generate content they'll publish without reading it, to fill pages they don't care about, to create volume without concern for what's true or useful\u2014that person is a bullshitter. Someone using AI to explore ideas, draft arguments they'll refine, or scale work they would stand behind\u2014that person is doing something else entirely.</p> <p>The slop complaint conflates these. It treats AI output as intrinsically problematic when the actual variable is human orientation toward truth.</p>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/12/on-slop/#kahneman-on-noise","title":"Kahneman on Noise","text":"<p>Consider now a different uncomfortable observation, this one from Daniel Kahneman. Speaking at an NBER conference in 2017, Kahneman offered remarks that should unsettle anyone concerned about AI-generated content:</p> <p>\"We have in our heads a wonderful computer. It is made of meat, but it's a computer. It's extremely noisy, but it does parallel processing. It is extraordinarily efficient, but there is no magic there.\"</p> <p>Kahneman's career was built on demonstrating how unreliable human judgment actually is. Not occasionally unreliable, not unreliable at the margins, but pervasively, structurally unreliable. Show people the same stimulus twice and they give different responses. Ask experts to make predictions and you'll find that a simple statistical model\u2014trained not to predict outcomes but merely to predict what the expert would say\u2014outperforms the expert themselves.</p> <p>Why? Because the model strips out the noise. Human judgment contains signal plus substantial random variation. A model that learns to extract the signal and ignore the noise will be more accurate than the noisy original, even though it has no understanding of the underlying domain.</p> <p>\"Most of the errors that people make are better viewed as random noise, and there is an awful lot of it,\" Kahneman observed. The implication was direct and uncomfortable: \"You should replace humans by algorithms whenever possible.\"</p> <p>If we take Kahneman seriously, a strange conclusion emerges. Humans are prolific producers of slop. We call it other things (opinions, takes, content, thought leadership) but by any honest standard, much of what humans produce is low-quality, repetitive, uncanny in its own way (how many LinkedIn posts follow the same template?), and generated without particular care for truth or value. The difference is that we extend to human-generated content a dignity we withhold from AI-generated content, not because the human content is better but because it's ours.</p> <p>I'm not defending AI slop. I'm observing an inconsistency. If the objection to AI content is that much of it is garbage produced without care for truth or the reader, that objection applies with equal force to enormous quantities of human-generated content. We've always lived in slop. We just called it the media environment.</p>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/12/on-slop/#a-better-taxonomy","title":"A Better Taxonomy","text":"<p>So what categories would actually be useful? If the origin of content (human vs. AI) isn't the relevant variable, and if quality alone doesn't capture what's corrosive, what would a better taxonomy look like?</p> <p>Frankfurt's framework suggests the answer lies in the operator's orientation toward truth. Four types emerge:</p> <p>Truth-seeking use. Someone uses AI as a tool in service of understanding or communication they care about. They review the output, refine it, stand behind the result. The AI is a lever, not a replacement for judgment. This isn't slop; it's augmented work.</p> <p>Truth-indifferent use. Someone generates content to fill space, meet quotas, game algorithms\u2014without concern for whether the output is true, useful, or good. This is bullshit in Frankfurt's precise sense, and it's corrosive regardless of whether AI produces the tokens or a bored content-farm writer does. The problem predates AI; AI merely scales it.</p> <p>Truth-adjacent use. Someone produces content that isn't about truth in any straightforward sense: entertainment, aesthetic exploration, play. Much creative work lives here. Judging it by epistemic standards misses the point; it operates under different rules. An AI-generated image that delights isn't slop just because no human held a brush.</p> <p>Truth-corrosive use. Someone deploys AI specifically to flood information environments, exhaust attention, or generate plausible-seeming misinformation at scale. This is worse than ordinary bullshit because it's strategic\u2014not mere indifference to truth but active weaponization of volume to degrade collective sense-making. This is the genuine danger, and it's a danger because of human intent, not because of AI capability.</p> <p>The slop complaint tends to conflate all four categories, treating them as instances of a single problem called \"AI-generated content.\" But the categories have almost nothing in common. Truth-seeking use should be celebrated. Truth-adjacent use should be evaluated by its own appropriate standards. Truth-indifferent use is a problem that long predates AI. Only truth-corrosive use represents something qualitatively new and dangerous, and it's dangerous precisely because of the human intentions behind it.</p>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/12/on-slop/#the-real-concerns","title":"The Real Concerns","text":"<p>The unease is still warranted. Something real has shifted. The marginal cost of content production has collapsed, and volume is flooding channels built for scarcity. Discovery is breaking down; attention is fragmenting; the sheer mass of generated content may be crowding out human expression that requires sustained attention to appreciate. These are legitimate concerns.</p> <p>But they're concerns about economics and attention, not about the metaphysics of human vs. machine production. A world with too much content (even too much good content) poses coordination problems. Those problems deserve analysis rather than moral panic.</p> <p>The deeper worry embedded in the slop complaint may be existential rather than epistemic. If AI can produce text and images that pass for human, what's distinctive about human production? If the output is indistinguishable, does the origin matter? These questions deserve serious treatment, and serious treatment requires not collapsing them into complaints about quality.</p> <p>What would it mean to take AI seriously as a tool for truth-seeking rather than merely diagnosing it as a source of pollution? Kahneman again points toward an answer. A model trained on expert judgment outperforms the expert by removing noise. Perhaps AI's genuine contribution lies less in generating content than in filtering it, helping extract signal from the overwhelming noise that humans produce naturally.</p> <p>This reframing opens different questions. Not \"how do we stop AI slop?\" but \"what new forms of care and carelessness does AI enable?\" Not \"is this content human?\" but \"does the person deploying this system care about what's true?\" The second set of questions is harder, more demanding, and more useful.</p> <p>The slop will continue. It always has. The relevant question isn't whether to dam the flood but whether we can maintain, individually and collectively, the orientation toward truth that Frankfurt identified as fundamental. AI doesn't answer that question. We do.</p>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/12/on-slop/#the-darker-thought","title":"The Darker Thought","text":"<p>A darker thought, which I haven't fully resolved. Simon Wardley describes LLMs as \"a non-kinetic form of warfare designed to embed the values of a small number of people into much wider communities by capturing the process of decision making.\" The delivery mechanism, he argues, is helpfulness itself. The payload is the gradual shaping of what questions feel natural to ask, what answers feel satisfying, what reasoning pathways we reach for.</p> <p>If Wardley is right, then even truth-seeking use isn't safe. You can be genuinely oriented toward truth and still have your reasoning shaped by a tool whose values aren't your own. The questions the model handles well feel natural; the framings absent from its training data don't occur to you to try.</p> <p>I don't think this invalidates the taxonomy. Operator orientation still matters\u2014truth-indifferent use is worse than truth-seeking use, and truth-corrosive use is worse still. But it suggests the taxonomy is incomplete. Perhaps truth-seeking isn't a stable position you occupy but a practice you maintain against tools that would make you comfortable, fluent, and subtly captured.</p> <p>The slop critics worry about garbage flooding the zone. Wardley worries about coherence flooding the zone\u2014useful, helpful coherence that shapes how we think without our noticing the shaping. Both concerns are real. The first is easier to see; the second may matter more.</p>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/12/on-slop/#sources","title":"Sources","text":"<ul> <li>Graphite, \"More Articles Are Now Created by AI Than Humans\" (2025): https://graphite.io/five-percent/more-articles-are-now-created-by-ai-than-humans</li> <li>Bynder, \"How consumers interact with AI vs human content\" (2024): https://www.bynder.com/en/press-media/ai-vs-human-made-content-study/</li> <li>Macquarie Dictionary, \"Word of the Year 2025\": https://www.macquariedictionary.com.au/word-of-the-year/word-of-the-year-2025/</li> <li>Harry Frankfurt, \"On Bullshit,\" Raritan Quarterly Review (1986); republished by Princeton University Press (2005)</li> <li>Daniel Kahneman, \"Comment on 'Artificial Intelligence and Behavioral Economics,'\" in The Economics of Artificial Intelligence: An Agenda, ed. Agrawal, Gans, and Goldfarb (University of Chicago Press, 2019): http://www.nber.org/chapters/c14016</li> <li>Simon Wardley on LLMs as non-kinetic warfare, LinkedIn post (November 2025): https://www.linkedin.com/in/simonwardley/</li> </ul>","tags":["artificial-intelligence","epistemology","media-criticism"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/","title":"The Political Economy of Moats","text":"<p>We say \"moat\" and imagine water, stone, permanence. Architecture designed to repel. But medieval sieges tell a different story. Historians estimate that three-quarters of sieges succeeded, and more through negotiation or starvation than assault. Fortresses fell to betrayal, bribery, and coercion at least as often as to military force. In 1119, Louis VI of France bribed the castellan of Les Andelys to smuggle soldiers inside hidden under straw; the \"impregnable\" fortress fell without a blow struck. Under accepted rules of war, a town captured by force could be sacked, but one that surrendered could not. Surrender was economically rational for everyone except the ideologically committed.</p> <p>The drawbridge was always the point. Moats weren't designed to prevent entry; they were designed to make entry expensive enough that negotiation became preferable to assault. The moat's function was never architectural but political-economic. Every castle had a price. The question was whether attackers could pay it.</p> <p>This week, three drawbridges lowered.</p> <p>Disney licensed 200 characters to OpenAI's Sora for $1 billion plus equity. Mickey Mouse, Darth Vader, Iron Man\u2014a century of IP fortress-building converted to platform positioning in a single transaction. Meta announced its pivot from open-source Llama to closed proprietary \"Avocado,\" abandoning the strategy that made it the center of open AI development. And Anthropic's Claude Skills feature invited companies to \"package your procedures, best practices, and institutional knowledge\" into the platform\u2014the capability moats that vertical SaaS companies believe make them defensible.</p> <p>Three moats. Three prices paid. The water didn't drain; it converted to currency.</p> <p>What we're witnessing isn't moat failure. It's moat-liquefaction: the transformation of defensive barriers into transactional surfaces. A liquid moat doesn't breach; it trades. The castle doesn't fall; it renegotiates. And the question that medieval castellans understood perfectly\u2014whose incentives govern the drawbridge?\u2014turns out to be the only strategic question that matters.</p> <p>The standard strategy discourse treats moats as complicated engineering problems. Analyze the competitive landscape. Identify defensible positions. Build barriers to entry. Maintain them against attackers. The metaphor suggests solidity, permanence, architecture you construct once and reinforce periodically.</p> <p>But moats aren't complicated systems; they're complex ones. The distinction matters. In complicated systems, cause and effect are knowable, and expert analysis can identify optimal strategies. In complex systems, cause and effect are visible only in retrospect, and the system's behavior emerges from interactions that no analysis fully captures. Complicated systems reward planning. Complex systems reward adaptation.</p> <p>Medieval castle defense was complex, not complicated. The structural vulnerability wasn't in the walls but in the sociology. A lord with hereditary claim might hold out indefinitely. A mercenary captain with payroll to meet would negotiate. The garrison's incentives mattered more than the garrison's architecture. Castles fell when the people inside had more to gain from surrender than from defense.</p> <p>Modern corporations have the same structure. Executives with quarterly targets. Boards with fiduciary duties to maximize shareholder value. The moat is maintained by people whose time horizons are short and whose incentives respond to capital offers. Offer $100 million, and corporate antibodies\u2014the organizational immune system that kills anything different\u2014develop sudden tolerance. Offer $1 billion, and the drawbridge lowers itself.</p> <p>The political economy of moats is the study of who controls the drawbridge and what makes them open it. Or, more crisply: moats aren't walls; they're governed prices.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#disney-the-tollbooth","title":"Disney: The Tollbooth","text":"<p>Consider what Disney actually did.</p> <p>The deal gives Sora users access to Mickey Mouse, Ariel, Cinderella, Iron Man, Darth Vader, and roughly 195 other characters across Disney, Marvel, Pixar, and Star Wars. Users can generate short social videos featuring these characters starting in early 2026. The deal explicitly excludes training data access\u2014Disney's IP won't train OpenAI's models\u2014and excludes talent likenesses and voices. Disney retains a steering committee for content moderation. The moat didn't fall; it became a tollbooth.</p> <p>But look at the structure more closely. Disney negotiated only about one year of exclusivity with OpenAI; after that, Disney can license the same characters to other AI platforms. This isn't a surrender\u2014it's price discovery. The first deal establishes the market rate; subsequent deals are pure upside against a proven model. Disney is positioning itself as the canonical source that all platforms must negotiate with, not a captive supplier to one.</p> <p>Iger's framing was explicit: \"We'd rather participate in the rather dramatic growth, rather than just watching it happen and essentially being disrupted by it.\" The IP could be vulnerable either way if generative AI becomes as powerful as its evangelists claim. For $1 billion\u2014a bargain for a company Disney's size\u2014Iger bought a seat at the table. CNN called it \"a $1 billion hedge on the future of slop.\" The hedge framing is precise: Disney isn't confident AI will transform entertainment, but can't afford to be wrong.</p> <p>The surgical precision matters too. Characters can appear on Sora; they can't be. No voices, no likenesses\u2014the elements that depend on actor relationships and human talent remain behind the walls. Disney is liquefying the fungible parts of the moat (visual IP, character designs) while retaining the parts that remain defensible (the human relationships, the voices, the performances). This isn't wholesale capitulation; it's selective liquefaction of what can't be held anyway while reinforcing what still can.</p> <p>And on the same day Disney signed the OpenAI deal, it sent a cease-and-desist letter to Google for allegedly using Disney IP to train models without permission. The moat becomes a tollbooth with legal enforcement for non-payers. License to partners who pay; litigate against those who don't. This is how you run a liquefied moat.</p> <p>Except: follow the system effects.</p> <p>Disney is already outsourcing animation to Vancouver and Manila, with Moana 2 as the first feature split between Los Angeles and offshore facilities. The Animation Guild commissioned research predicting 21% of US film, TV, and animation jobs consolidated, replaced, or eliminated by AI by 2026\u2014over 100,000 positions. DreamWorks co-founder Jeffrey Katzenberg claims that animated features requiring 500 artists over five years will require 10% of that within three years. VFX industry observers predict workforce reductions of 80% or more once AI systems mature.</p> <p>Disney's moat-liquefaction isn't just a financial transaction but a system reconfiguration. The $1 billion flows from OpenAI to Disney; Disney flows from content creator to IP licensor; the workforce flows from employment to redundancy; the creative pipeline flows from human-driven to AI-augmented with human cleanup. Capital moves in one direction, jobs in another, capability from studio to platform.</p> <p>Using Simon Wardley's evolution framework: Disney's animation capability is moving from custom-built (proprietary, differentiated, expensive) toward commodity (platform-available, purchasable, price-competitive). The moat around capability dissolves. The moat around IP transforms into licensing revenue. Disney becomes the canonical source\u2014the node that AI-generated content references\u2014rather than the exclusive producer of what the characters do.</p> <p>The question isn't whether this is good or bad for Disney shareholders. In the short term, it's probably good: $1 billion in equity, a customer relationship with OpenAI, positioning in AI-generated content distribution. The question is what it means for the system Disney inhabits. The workers losing jobs. The junior roles eliminated before new animators learn the craft. The pipeline changes that determine what kind of content becomes possible and what kind becomes uneconomical. The moat-holder's rational decision is the system's structural transformation.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#meta-the-self-emptying-moat","title":"Meta: The Self-Emptying Moat","text":"<p>Meta's case illustrates a different liquefaction pattern: the self-emptying moat.</p> <p>Open-source Llama was a strategic masterstroke\u2014or looked like one. Release the model weights. Let the ecosystem build on your architecture. Capture value through position rather than enclosure. The logic was sound: become the center of gravity for open AI development, and you control the standard even without controlling access.</p> <p>Then DeepSeek demonstrated that openness in AI is invitation, not deterrent.</p> <p>The Chinese lab copied Llama's architecture, trained on different data, and built a competitive model. The gift that was supposed to create dependency created competitors instead. Meta's open-city strategy\u2014giving away the walls, betting that generosity creates position\u2014collapsed when the gift turned out to be valuable enough to accept gratefully and use against the giver.</p> <p>The Avocado pivot is capitulation to moat logic. After DeepSeek's success, after the $14.3 billion acquisition of Scale AI's team, after Yann LeCun's resignation and 600 layoffs from the Fundamental AI Research lab, Meta learned that architectural moats self-empty when the architecture is valuable enough to copy. The company that championed open AI development will release its next frontier model as closed, paid, permission-controlled.</p> <p>The system effect is concentration. Meta's departure from open-source narrows AI development to fewer Western labs. The ecosystem that was supposed to democratize capability now routes through OpenAI, Anthropic, Google, and a closed Meta. Capital consolidates; access becomes permission-gated; the moat that gave itself away reconsolidates as a gated community.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#skills-capability-migration","title":"Skills: Capability Migration","text":"<p>Anthropic's Skills feature reveals a third pattern: capability migration.</p> <p>The marketing language is instructive: \"Package your company's procedures, best practices, and institutional knowledge so teams work consistently and new members get expert-level results from day one.\" This is the value proposition of vertical SaaS\u2014domain-specific workflows, procedural expertise, the accumulated knowledge that makes specialists more effective than generalists. The capability moat that vertical software companies believe makes them defensible against horizontal platforms.</p> <p>But Skills suggests capability moats are migrating upward. What lives in your SaaS application today\u2014the workflows, the domain logic, the procedural knowledge\u2014can be packaged, uploaded, and executed by the platform. The moat doesn't breach through competition; it evaporates into the platform layer.</p> <p>Model Context Protocol already lets ChatGPT access HubSpot data directly. The next step is accessing your workflows directly. The platform doesn't need to attack your moat; it needs to make moats hostable. And once the moat is hosted, the moat-holder becomes the Skills provider\u2014still capturing some value, but from a fundamentally different position.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#the-mechanism-legibility-to-liquidity","title":"The Mechanism: Legibility to Liquidity","text":"<p>Wardley's framework helps clarify what's happening. Components evolve along a predictable axis: genesis (novel, uncertain, expensive) to custom-built (understood, differentiated) to product (stable, competitive) to commodity (interchangeable, price-driven). Strategic advantage lives in genesis and custom-built. Commodity components are costs, not differentiators.</p> <p>AI accelerates evolution. Capabilities that took decades to commoditize now move in years. Disney's animation expertise, Meta's architectural advantage, vertical SaaS domain knowledge\u2014all are moving rightward on the evolution axis faster than the moat-holders anticipated.</p> <p>More precisely: AI makes moat contents legible to capital. What was opaque (proprietary process, institutional knowledge, craft expertise) becomes inspectable, replicable, packageable. But legibility alone isn't enough. What converts legibility into liquidity is standardization\u2014APIs, evaluation frameworks, connectors like MCP, compliance schemas, audit trails. Skills isn't just \"upload your knowledge\"; it's knowledge packaged in a platform-native execution format. The chain runs: legibility \u2192 standardization \u2192 portability \u2192 price discovery \u2192 bargaining. Once capital can see inside the moat and the contents can flow through standard interfaces, the drawbridge negotiation begins.</p> <p>The non-kinetic warfare framing sharpens this. Wardley has described LLMs as \"a non-kinetic form of warfare designed to embed the values of a small number of people into much wider communities by capturing the process of decision making.\" The attack vector isn't breaching moats; it's making moat-holders comfortable. Partnership feels like opportunity. Licensing feels like revenue. The payload is delivered through helpfulness.</p> <p>Disney licensing to OpenAI normalizes AI-generated Disney content. The moat-holder becomes the legitimizer. Meta closing Llama concentrates AI development. The former champion of openness becomes a gatekeeper. Claude's Skills captures procedural knowledge. The platform becomes the host for workflows that used to be moats.</p> <p>Each transaction is individually rational. Each executive makes a defensible decision. The system effect is consolidation, dependency, and capability migration\u2014from moat-holders to platforms. The attackers didn't storm the walls. They made offers the drawbridge-controllers couldn't refuse.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#vertical-saas-the-durability-question","title":"Vertical SaaS: The Durability Question","text":"<p>What does this mean for vertical SaaS?</p> <p>The companies believe they have moats: domain-specific data, workflow expertise, customer relationships, regulatory knowledge. HR software for aged care. Rostering systems for education. Compliance tools for financial services. The horizontal platforms can't compete because they don't understand the domain.</p> <p>The question moat-liquefaction raises: are these moats durable, or merely not yet interesting to capital?</p> <p>If capital wants what's inside, your moat becomes a negotiation. OpenAI or Anthropic approaches with partnership offers. \"Let us access your domain data to improve our models for your vertical. Let us host your workflows as Skills. Let us be the AI layer; you be the domain expertise.\" Each offer is individually attractive. Each acceptance transfers capability from moat to platform.</p> <p>If capital doesn't want what's inside, your moat persists\u2014but possibly because it's too niche to bother acquiring. Australian workforce rostering for aged care isn't valuable enough for a billion-dollar offer. This might be the good news. The moats that survive aren't the most defended; they're the least interesting.</p> <p>Map your moat components on Wardley's evolution axis. Where are they? If they're in product moving toward commodity, the platform will absorb them. If they're in genesis or custom-built, you have time\u2014but time converts to commodity eventually. The clock runs faster now.</p> <p>The strategic frame matters too. Most vertical SaaS companies treat their situation as complicated\u2014analyzable, defensible through planning, amenable to best practices. But the situation is complex, characterized by feedback loops, emergent effects, and second-order consequences visible only in retrospect. The workforce management domain that exists today (manual rostering, compliance rules, award interpretation) may be substantially different in three years. The moat you're defending may not be the territory that matters.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#scenario-sketches-lived-experience","title":"Scenario Sketches: Lived Experience","text":"<p>What does this mean for lived experience? Not prediction but projection\u2014scenario sketches that stress-test incentives rather than forecast timelines. Following the dynamics forward to see what work and watching might become if these patterns play out.</p> <p>Consider a facilities manager at an aged care provider in 2028. She used to open Workforce Manager Pro (the vertical SaaS her organization has used for a decade) to build rosters, check compliance, manage leave requests. The software knew the aged care award inside out. It understood minimum staffing ratios, qualification requirements, the baroque complexity of penalty rates.</p> <p>Now she opens Claude. Or ChatGPT. Whatever the dominant interface has become. She says: \"Build next week's roster for Sunrise Lodge. We're short two RNs on Thursday night.\" The platform consults Skills\u2014aged care compliance packaged by some vendor, maybe Workforce Manager Pro, maybe a competitor, maybe an aggregator who licensed from multiple sources. The platform accesses her staff database through MCP. It knows the award because someone uploaded the procedural knowledge. The roster appears.</p> <p>The facilities manager doesn't care whose moat the knowledge came from. She cares that the roster is compliant and the night shift is covered. The domain expertise that was Workforce Manager Pro's moat has liquefied into the platform layer. The vendor might still exist\u2014as a Skills provider, as a compliance certifier, as the entity that validates outputs against regulatory standards. But the relationship has transformed. The moat became an API.</p> <p>The HR director at a mid-sized education provider experiences something similar. She used to rely on EduStaff (a vertical SaaS for education workforce management) because it understood teacher registration requirements, working with children checks, the specific leave provisions in the enterprise agreement. EduStaff's sales pitch was: \"We know education. The horizontal tools don't.\"</p> <p>In 2028, the horizontal tools do know education\u2014because EduStaff licensed its procedural knowledge to survive. Or because a competitor did. Or because the platform scraped enough documentation and edge cases to approximate the expertise. The HR director's experience hasn't changed much; she still gets compliant rosters, still manages leave, still runs reports. But the value capture has shifted. She pays less for EduStaff (now a thin integration layer) and more for the platform (which hosts the intelligence). EduStaff's moat liquefied; the company persists as a node rather than a fortress.</p> <p>The worker's experience changes differently. The aged care nurse checking her shifts used to interact with Workforce Manager Pro through a clunky mobile app built by a 50-person vertical SaaS company. In 2028, she asks her phone about next week's shifts, and the platform answers\u2014pulling from whatever compliance logic and roster data lives in the Skills layer. The interface is better (platform-grade UX, not vertical-SaaS-grade). The intelligence is comparable or better.</p> <p>What's missing is harder to name: the specific relationship between her employer and the vendor, the vendor's rep who understood their organization, the customizations built over years, the institutional knowledge in the support team. The liquefied moat still works\u2014rosters get built, compliance gets checked\u2014but something has shifted in the texture of work. The domain-specific vendor, with domain-specific knowledge and domain-specific relationships, became an interchangeable commodity supplier to a platform the worker never chose and may not trust.</p> <p>Project the Disney case forward. What does moat-liquefaction mean for watching Disney content?</p> <p>2025: Disney licenses 200+ characters to Sora. Users generate short social videos featuring Mickey, Darth Vader, Iron Man. The content is canonically licensed but user-created. Disney retains a steering committee to prevent brand damage.</p> <p>2027: Sora-generated Disney content saturates social media. A viral video of Darth Vader singing karaoke, created by a teenager in Buenos Aires using the licensed Sora tool, accumulates 400 million views. Disney didn't make it. Disney licensed the possibility of it. Disney captures some value through the platform relationship but not the cultural moment.</p> <p>2028: Disney releases a new animated feature. It was animated in Vancouver and Manila, with AI handling interpolation, cleanup, and increasingly, \"creative suggestions\" that the reduced human team reviews and approves. The credits list 47 human animators. Five years earlier, the same production would have listed 300. The movie looks like a Disney movie. It moves like a Disney movie. Whether it feels like a Disney movie depends on whether you know how it was made.</p> <p>The Animation Guild's prediction has largely materialized: 21% of US animation jobs consolidated or eliminated, distributed unevenly. Junior roles\u2014the entry points where young animators learned the craft\u2014were hit hardest. AI handles the work that used to train the next generation. The senior animators who remain are older, supervising AI output rather than creating frames. The pipeline has changed.</p> <p>For the viewer, the immediate experience might be: the movies are fine. Maybe slightly different in ways hard to articulate. A certain efficiency in the movement. A subtle uncanny valley not in the faces but in the choices\u2014AI-generated \"creative suggestions\" that are competent but not quite surprising in the way human choices sometimes surprise. The Disney moat has liquefied into brand (canonical source), distribution (Disney+), and IP (characters that Sora users generate content with). The animation capability moat largely evaporated.</p> <p>For the aspiring animator, the experience is: the path closed. The junior roles that were entry points don't exist at the scale they used to. You can generate impressive animation with AI tools\u2014more impressive than any student reel from 2020\u2014but generation doesn't teach the craft. The expertise moat liquefied, and the pipeline for creating new experts liquefied with it.</p> <p>For the culture, the experience is harder to summarize. More Disney content exists than ever before (user-generated, AI-assisted, officially released). The canonical versions remain canonical. But the density of Disney imagery in the environment has increased while the human labor creating it has decreased. The characters are omnipresent; the humans who brought them to life are fewer.</p> <p>The workforce management market in 2029:</p> <p>The landscape has consolidated. Three or four platforms host most of the intelligence layer. Dozens of vertical Skills providers supply domain expertise\u2014aged care compliance, education workforce rules, healthcare registration requirements. The Skills providers compete on comprehensiveness, accuracy, update speed (regulations change; Skills packages must change with them). The platforms compete on UX, integration breadth, enterprise features.</p> <p>The workers using these systems experience a strange homogenization. The aged care facilities manager, the education HR director, the healthcare clinic owner\u2014they all use the same platform with different Skills attached. The workflows feel similar. The interfaces are consistent. The domain-specific differences are real but hidden in the Skills layer, invisible to the user unless something goes wrong.</p> <p>What's been gained: ease of use, interface quality, the ability to ask natural language questions rather than navigating software designed by engineers who've never worked in aged care. What's been lost: the specific relationship between a vertical SaaS vendor and its industry\u2014the vendor who understood aged care because their team came from aged care, the support rep who'd seen your specific edge case before, the roadmap shaped by domain customers rather than platform economics. The moat was also a relationship, and liquefaction dissolved the relationship along with the barrier.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#the-moat-to-node-transition","title":"The Moat-to-Node Transition","text":"<p>Historical parallel: castles didn't disappear when gunpowder made walls obsolete. They became manor houses, administrative centers, symbols of legitimacy. The defensive function died; the positional function transformed. The castle that couldn't stop cannons could still anchor a territory, signal authority, attract visitors.</p> <p>Moat-liquefaction doesn't end moats. It transforms them.</p> <p>Disney's IP fortress becomes the canonical source. AI-generated Mickey Mouse proliferates regardless of licensing. Disney's position shifts from preventing generation to legitimizing it. The moat becomes a brand node. Everyone generating Disney content references Disney as origin; Disney captures value from reference rather than exclusion. The 2028 viewer watches a Disney movie and sees the brand; they don't see (and perhaps don't care about) the liquefied animation capability beneath it.</p> <p>Meta's open-source position becomes the gated community. Avocado is closed, paid, permission-controlled. Meta moves from ecosystem center to access gatekeeper. The moat transforms from generosity-as-strategy to scarcity-as-strategy. The 2028 developer navigating frontier models finds a landscape of three or four major providers, each with their own terms, their own values embedded in their systems.</p> <p>Vertical SaaS capability moats become integration surfaces. Domain expertise doesn't defend against platforms; it qualifies for platform integration. The moat transforms from wall to API. The 2029 compliance officer or clinic owner or facilities manager experiences this as: the software works better, but the relationship is different. The vendor is still there\u2014as a Skills provider, a compliance certifier, an integration specialist\u2014but the locus of value has shifted to the platform.</p> <p>This is the moat-to-node transition: when defensive barriers transform into network positions, and control shifts from exclusion to mediation. Disney mediates between AI generation and canonical characters. Meta mediates between developers and model access. Vertical SaaS mediates between platforms and domain expertise.</p> <p>The texture of mediation differs from the texture of defense. A fortress-holder's relationship to their territory is protective, controlling, autonomous. A node's relationship to its network is participatory, dependent, embedded. Disney-as-fortress decided what Disney content existed. Disney-as-node legitimizes Disney content that others generate. Vertical-SaaS-as-fortress decided how domain workflows worked. Vertical-SaaS-as-node packages domain knowledge for platforms to deploy.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#the-ilc-trap","title":"The ILC Trap","text":"<p>For vertical SaaS, your moat is liquid. The question isn't whether\u2014it's when and into what.</p> <p>The instinct is to list options: become a node, exploit your niche, race the liquefaction, accept the transition. These are the standard responses to platform pressure. But Wardley's ILC framework (Innovate-Leverage-Commoditize) suggests something more uncomfortable: these aren't strategies for durable advantage. They're coping mechanisms for inevitable decline.</p> <p>The ILC playbook is simple and relentless. Take something that's a product, turn it into a utility, let everyone build on top of it, mine the metadata to spot future patterns, commoditize those patterns into new component services, move up the stack. Amazon runs this playbook. The AI platforms are running it now. Vertical SaaS domain expertise is exactly the kind of thing that gets fed into the ILC machine. The vertical player does the innovation; the platform does the leverage and commoditization.</p> <p>Run the options through this lens:</p> <p>Become a node means accepting ILC. You become the Skills provider, a component supplier to a platform running the ILC playbook on you. Your margins compress over time. Your differentiation erodes as the platform learns from your patterns and the patterns of every other vertical provider. This isn't durable advantage; it's managed decline with a seat at the table.</p> <p>Exploit the niche works only if you're beneath the platform's notice. Australian aged care rostering might qualify today. But this isn't durable advantage; it's durable irrelevance. The moment your niche becomes interesting\u2014aggregated with other niches, valuable for training data, strategic for platform expansion into adjacent markets\u2014you're back in the ILC crosshairs. You survive by hoping no one ever wants what you have.</p> <p>Race the liquefaction means staying in genesis and custom-built while the platforms commoditize everything behind you. But the treadmill speeds up. Each innovation you create gets commoditized faster than the last as the platforms get better at pattern recognition across their installed base. This isn't a strategy; it's a hamster wheel with a dignified name.</p> <p>Accept the transition is the same as becoming a node, just more honest about the trajectory. You package your domain expertise, license it to platforms, and watch the revenue model compress. The knowledge persists; the company shrinks. Transformation, like the castle becoming a manor house\u2014but manor houses don't command armies.</p> <p>These are coping strategies, not durable advantages. So what might actually work?</p> <p>Before answering, an honest acknowledgment: I've underweighted some durable-ish moats that vertical SaaS often possesses. Distribution moats matter\u2014approved vendor lists, multi-year procurement contracts, implementation partners, change management fatigue. Enterprises don't switch systems easily, and platforms haven't yet built the sales motion to displace embedded vertical players. Data rights matter differently than \"we have data\"; permissioned, longitudinal, high-integrity data with specific usage rights is harder to replicate than it sounds. Certification ecosystems matter\u2014\"we are the system auditors already accept and insurers recognize\" creates friction that pure capability can't overcome.</p> <p>These don't defeat the liquefaction thesis. They can liquefy too, given enough platform patience and enough capital to build enterprise sales capacity. But acknowledging them makes the timeline less certain and the outcome less predetermined. The question isn't whether moats liquefy but which ones and how fast.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#what-might-actually-work","title":"What Might Actually Work","text":"<p>Three possibilities, each requiring vertical SaaS to become something fundamentally different than what it is today.</p> <p>Own the liability, not just the logic. Platforms can commoditize aged care rostering knowledge through Skills. They can't commoditize being the entity that's liable when the roster is wrong and a patient is harmed. They can't hold the regulatory certification. They can't be the party that AHPRA or ASIC holds accountable when something goes wrong.</p> <p>Vertical SaaS that embeds itself in compliance liability and regulatory relationships\u2014not just compliance knowledge\u2014might have something more durable. The moat isn't the code or the domain expertise; it's the position in the regulatory graph. This requires becoming less of a software company and more of a compliance infrastructure provider that happens to use software.</p> <p>But liability is a product, not a slogan. Owning it requires contractual structure (indemnities that platforms won't accept), insurance relationships (carriers who understand your domain and will underwrite your exposure), auditability (versioned rules, explainable outputs, incident logs), and operational capacity to respond when something goes wrong. The transformation is significant: different revenue model, different risk profile, different organizational capabilities\u2014and a credible claim that when regulators come asking, you're the entity with answers.</p> <p>Most vertical SaaS companies won't make this shift. The ones that do become harder to absorb because the platform would have to absorb the liability along with the capability\u2014and platforms are structured to avoid liability, not accumulate it.</p> <p>Become the vertical platform before the horizontal platform notices. \"Too big to absorb\" is relative. Big in one vertical is still tiny compared to horizontal platforms. But there's a window: consolidate your vertical fast enough to become the platform for that vertical before a horizontal platform decides to enter.</p> <p>This means running ILC yourself, within your vertical, before it's run on you. Commoditize the smaller vertical players. Build the platform layer for aged care or education or financial services. Become the thing that horizontal platforms have to integrate with rather than replace\u2014because you've already absorbed the ecosystem they would need to build.</p> <p>This is a race condition. It requires capital, speed, and willingness to cannibalize your own product business to build a platform business. It requires being more aggressive about ILC within your domain than the horizontal platforms are about ILC across domains. Most vertical SaaS companies don't have the capital, the speed, or the stomach. The few that do become the consolidators rather than the consolidated.</p> <p>Generate novelty faster than commoditization erodes it. This is different from racing the liquefaction. Racing means staying in genesis\u2014an exhausting treadmill. This move accepts that lower layers will be commoditized while continuously generating new value at higher layers.</p> <p>Amazon commoditized cloud infrastructure with AWS. But companies building on AWS aren't all commoditized\u2014some built genuinely novel things on top that Amazon couldn't anticipate. The question for vertical SaaS: can you keep generating novelty at the layer above what platforms commoditize?</p> <p>For workforce management, this might mean: let the platform commoditize rostering logic. Build something new on top that the platform can't see yet\u2014predictive workforce analytics that require domain context the platform lacks, regulatory arbitrage tools that depend on relationships the platform can't replicate, capabilities that don't exist yet and won't exist until you create them. Accept that each layer you build will eventually be commoditized. Keep climbing.</p> <p>This requires genuine innovation capacity, not just domain expertise. Most vertical SaaS companies have accumulated domain knowledge; few have built the R&amp;D capability to stay ahead of platform absorption. The ones that do become moving targets rather than sitting ducks.</p> <p>There's a fourth possibility, visible in Disney's deal structure: become the price-setter, not just a node. Disney negotiated one year of exclusivity with OpenAI, then freedom to license the same IP to competing platforms. The first deal establishes market rates; subsequent deals are upside against a proven model. Disney isn't a captive supplier\u2014it's the canonical source that all platforms must negotiate with.</p> <p>For vertical SaaS, this would mean: license your domain expertise to one platform first, but negotiate short exclusivity windows. Use the first deal to establish that your compliance logic or workflow knowledge has market value. Then license to the second platform, and the third. Become the entity that all horizontal platforms integrate with for your domain, not the exclusive partner of one.</p> <p>This requires bargaining power most vertical SaaS companies lack. Disney can negotiate short exclusivity because Disney is Disney\u2014the characters are irreplaceable. Most domain expertise isn't irreplaceable; it's approximable. But for the few vertical players with genuinely unique data, regulatory relationships, or accumulated edge cases that platforms can't easily replicate, the Disney playbook offers a model: selective liquefaction on your terms, with diversification across platforms and legal enforcement against non-payers.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#the-honest-assessment","title":"The Honest Assessment","text":"<p>The honest assessment: there may not be durable advantage for vertical SaaS as currently constituted. The ILC dynamic is real and probably inexorable for companies that remain primarily software businesses selling domain expertise.</p> <p>The three paths that might offer genuine durability all require becoming something different: compliance infrastructure, vertical platform, or innovation engine. Each is a significant transformation. Each requires capabilities most vertical SaaS companies don't currently have. Each involves risk that most boards and executives won't accept.</p> <p>What remains for the rest? Coping strategies. Managed decline. Negotiated surrender terms. The hope that liquefaction happens slowly enough to exit before it completes.</p> <p>This isn't nihilism; it's clarity. The medieval castellan who understood that every castle had a price wasn't defeatist\u2014he was realistic about the nature of defense. The vertical SaaS executive who understands ILC isn't giving up\u2014she's seeing the board clearly enough to make an actual choice rather than a comfortable illusion.</p> <p>The choice isn't between defense and surrender. It's between transformation and decline. The moat liquefies either way. The question is whether you use the time to become something the liquid can't dissolve.</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#the-liquid-fortress","title":"The Liquid Fortress","text":"<p>Moats were never walls; they were negotiated spaces with drawbridges for a reason. The medieval castellan understood that defense was always priced\u2014the question was whether attackers would pay. What AI accelerates is the rate of liquefaction: how quickly defensive barriers convert into transactional surfaces, how visibly capital can see inside the moat, how easily partnership offers align with executive incentives.</p> <p>December 2025 was a week of keys turning in locks. Disney's $1 billion handshake with OpenAI. Meta's retreat from openness. Claude's invitation to package your procedures for the platform. The drawbridges lowered; the water began to flow.</p> <p>Looking back from 2029, what we notice isn't that the moats disappeared. They didn't. Disney is still Disney; the brand moat is stronger than ever. Meta still matters; the gatekeeper position commands value. Vertical SaaS still exists; the domain expertise still has buyers. What disappeared was the illusion that moats are architectural\u2014that they're walls you build once and maintain, rather than contracts you renegotiate every time someone with capital asks nicely enough.</p> <p>The liquid fortress doesn't drain. It flows into new shapes, carries value in new directions, deposits it in new hands. The medieval lesson holds: castles fell to incentive alignment, not superior siege engines. The modern corollary: moats liquefy when capital offers make sense to the people holding the keys.</p> <p>The question was never \"will the moat hold?\"</p> <p>The question was always \"who benefits when it doesn't?\"</p>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/13/the-political-economy-of-moats/#sources","title":"Sources","text":"<ul> <li>English Heritage, \"10 Things You Probably Didn't Know About Sieges\": https://www.english-heritage.org.uk/visit/inspire-me/10-things-sieges/</li> <li>The History Press, \"Siege warfare in the Middle Ages\": https://thehistorypress.co.uk/article/siege-warfare-in-the-middle-ages/</li> <li>OpenAI, \"Disney Sora Agreement\" (December 2025): https://openai.com/index/disney-sora-agreement/</li> <li>CNBC, \"Disney making $1 billion investment in OpenAI\" (December 2025): https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html</li> <li>CNBC, \"Disney's OpenAI stake is 'a way in' to AI\" (December 2025): https://www.cnbc.com/2025/12/11/disney-open-ai-iger-altman.html</li> <li>CNN Business, \"Disney's OpenAI deal is a $1 billion hedge on the future of slop\" (December 2025): https://www.cnn.com/2025/12/11/business/disney-openai-hedge</li> <li>Fortune, \"Bob Iger says Disney's $1 billion deal with OpenAI is an 'opportunity, not a threat'\" (December 2025): https://fortune.com/2025/12/11/disney-openai-deal-investment-bob-iger-opportunity-not-threat/</li> <li>CNBC, \"From Llamas to Avocados: Meta's shifting AI strategy\" (December 2025): https://www.cnbc.com/2025/12/09/meta-avocado-ai-strategy-issues.html</li> <li>OpenAI, \"Introducing GPT-5.2\" (December 2025): https://openai.com/index/introducing-gpt-5-2/</li> <li>Context News, \"Hollywood animation, VFX unions fight AI job cut threat\": https://www.context.news/ai/hollywood-animation-vfx-unions-fight-ai-job-cut-threat</li> <li>The Wrap, \"An AI Wave Will Sweep Through Hollywood's VFX Systems in 2025\": https://www.thewrap.com/ai-vfx-production-labor/</li> <li>Screen Daily, \"Why outsourcing and AI means the US animation sector is facing hefty challenges in 2025\": https://www.screendaily.com/features/why-outsourcing-and-ai-means-the-us-animation-sector-is-facing-hefty-challenges-in-2025/5199989.article</li> <li>SaaStr, \"10 Ways to Build a Moat in SaaS. But AI is Also Making Them Weaker\": https://www.saastr.com/whats-your-moat/</li> <li>Vendep Capital, \"Forget the data moat: The workflow is your fortress in vertical SaaS\": https://www.vendep.com/post/forget-the-data-moat-the-workflow-is-your-fortress-in-vertical-saas</li> <li>Wardley Maps, \"Mapping 101: A Beginner's Guide\": https://www.wardleymaps.com/guides/wardley-mapping-101</li> <li>Wardley Maps, \"ILC (Innovate-Leverage-Commoditize)\": https://www.wardleymaps.com/glossary/ilc</li> <li>The Cynefin Company, \"About the Cynefin Framework\": https://thecynefin.co/about-us/about-cynefin-framework/</li> <li>Simon Wardley on LLMs as non-kinetic warfare, LinkedIn (November 2025)</li> </ul>","tags":["strategy","artificial-intelligence","business-models","political-economy"]},{"location":"blog/2025/12/15/thin-to-thick/","title":"Thin to Thick","text":"<p>The hidden design challenge in every AI tool.</p> <p>Everyone building AI tools is solving the same problem, and most don't realize it.</p> <p>The visible problem is capability: can your tool write code, analyze data, automate workflows? The hidden problem is structure: how do users move from \"I tried ChatGPT once\" to \"this tool understands how I work\"? What primitives do you give them? How do those primitives grow?</p> <p>Scroll through LinkedIn and you'll find practitioners mapping their journey:</p> <ol> <li>Casual use \u2014 random questions, sporadic interactions</li> <li>Power user \u2014 saved prompts, custom instructions</li> <li>Packager \u2014 building reusable workflows</li> <li>Chaos \u2014 too many tools that don't talk to each other</li> <li>Workspace need \u2014 craving a single place where everything lives</li> </ol> <p>The stages feel true. They also encode assumptions worth excavating. What theory of progression does this taxonomy assume? What happens to users who don't follow the path?</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#the-proliferation-of-primitives","title":"The Proliferation of Primitives","text":"<p>Every major AI platform has invented its own vocabulary for user-facing abstractions. OpenAI has custom instructions, GPTs, assistants. Anthropic's Claude Code has skills, commands, hooks, agents, MCP servers, plugins. Microsoft's Copilot has plugins and extensions. Each taxonomy reflects design decisions about what users need and when they need it.</p> <p>The proliferation signals something unresolved. If everyone is inventing different primitives, either the design space is genuinely multidimensional (many valid approaches) or we haven't found the right abstractions yet (still searching). Probably both.</p> <p>Look closer at any taxonomy and you find hidden premises. The LinkedIn stages assume progression is linear: you start casual, you end with a workspace. But some users stay casual forever and go deep rather than broad. Others jump straight to building, skip the power-user phase entirely, and create sophisticated tooling before they've internalized best practices. The stages describe a path, but users take many paths.</p> <p>Claude Code's primitives assume users can distinguish between automatic behavior (skills that activate based on context), explicit invocation (commands you type), deterministic control (hooks that guarantee certain actions), parallel delegation (agents that work alongside you), external integration (MCP servers), and distribution (plugins that package everything for sharing). That's six categories. Each makes sense in isolation. Together they form a taxonomy that experienced users navigate fluently and new users find bewildering.</p> <p>The question isn't which taxonomy is right. The question is what problem the taxonomy is trying to solve, and whether more categories actually solve it.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#excavating-claude-code","title":"Excavating Claude Code","text":"<p>A close reading of Claude Code's primitives reveals the design logic underneath. This isn't critique; it's archaeology. What did the designers assume about users?</p> <p>Skills are the most interesting primitive because of how much they can contain. At base, a skill activates automatically based on context. When you say \"write a blog post,\" the relevant skill loads without you asking. Skills are markdown files that can reference other files, accumulating resources and examples. The assumption: users want \"always-on\" expertise that doesn't require explicit invocation. The skill should know when it's needed.</p> <p>From the start, skills were designed as containers for more than just instructions. A skill directory can contain not just the SKILL.md file but executable scripts: Python formatters, shell scripts, automation that fires when the skill activates. The skill becomes a container for behavior, not just knowledge. It can hold reference documents, example files, configuration, and code that executes. The boundary between \"instructions for the model\" and \"automation that runs alongside the model\" was blurred by design. A sufficiently developed skill resembles an application more than a prompt.</p> <p>Commands are explicit. You type <code>/deploy</code> or <code>/review</code> and something happens. Commands live in their own directory (<code>.claude/commands/</code>), separate from skills. They're saved prompts, shortcuts for repeated tasks. The assumption: some behaviors should only happen when requested. The user stays in control of when.</p> <p>Skills and commands are parallel primitives, not nested. Both can exist in a project; both can be personal or project-scoped. The relationship is compositional rather than hierarchical: an agent might invoke a skill and trigger a command, but neither contains the other. Some practitioners organize commands thematically alongside related skills, but this is convention, not architecture.</p> <p>Hooks are deterministic. They fire on specific events: before a tool runs, when permissions are requested, when a session ends. Unlike skills and commands, hooks don't rely on the language model choosing to act. They guarantee behavior. The assumption: some things must happen reliably, not probabilistically.</p> <p>Hooks represent a fundamentally different philosophy than skills and commands. Skills trust the model to decide when to activate; hooks don't trust the model at all. A hook that validates file changes before they're written provides a hard constraint that no amount of prompt engineering can override. The determinism is the point. Some behaviors must happen regardless of what the model decides.</p> <p>This creates a design tension. Skills are flexible but unreliable; they fire when context seems right, which means they sometimes fire wrong. Hooks are reliable but inflexible; they fire on specific events, which means they can't adapt to nuance. The user who wants \"usually do X, but be smart about when\" has to choose between a skill that's sometimes wrong and a hook that can't be smart. There's no primitive that occupies the middle ground: probabilistically reliable, flexibly deterministic.</p> <p>Agents work in parallel. They can invoke skills and commands, orchestrating complex tasks by delegating to specialized workers. The assumption: sophisticated work requires coordination across multiple capabilities.</p> <p>Agents are where composition becomes visible. An agent can spawn sub-agents, invoke skills, trigger commands, and coordinate the results. The power is real: a well-designed agent system can handle tasks that would overwhelm a single model context. The complexity is also real: debugging an agent that spawned three sub-agents, each of which invoked different skills, requires understanding four layers of context that the user didn't directly create.</p> <p>MCP servers handle external integration. They connect Claude to databases, APIs, third-party tools through a standard protocol. The assumption: AI tools need to reach outside themselves, and that reaching should follow a common interface.</p> <p>MCP represents an interesting design choice: standardize the integration layer rather than the capability layer. Any external tool that implements the protocol becomes available to any Claude surface. The standard is the primitive; specific integrations are instances. This is the opposite of the skill approach, where each capability is a custom artifact. MCP trades flexibility for interoperability.</p> <p>Plugins package everything else. A plugin bundles skills, commands, hooks, agents, and MCP configurations into something a team can share. The assumption: configurations worth building are worth distributing.</p> <p>Plugins are the meta-primitive, the packaging layer. They solve the distribution problem that plagues every other primitive: how does something one person built become available to everyone? Without plugins, every team copies files, adapts configurations, maintains their own forks. With plugins, best practices propagate through installation rather than reimplementation.</p> <p>But plugins also reveal the cost of taxonomic complexity. A plugin bundles skills, commands, hooks, agents, and MCP configurations together. The distribution problem is solved, but the comprehension problem remains. What does this plugin actually do? The answer requires understanding six primitive types and how they relate.</p> <p>Here's what makes this architecture confusing: skills, commands, and agents are structurally identical. They're all markdown files containing instructions. The difference isn't format; it's purpose and activation mode. When should this fire? Who initiates it? Can it invoke other things?</p> <p>The structural similarity creates constant confusion. Users ask: \"Should this be a skill or a command?\" The answer depends on activation preference, not on what the thing does. Users ask: \"Is this an agent or a skill with sub-agents?\" The answer depends on whether you want parallel execution, not on the task's nature. The primitives are distinguished by when and how they run, not by what they are. This is probably correct from an engineering perspective and confusing from a user perspective.</p> <p>The surfaces complicate matters further. Claude Code runs as a CLI. Claude Desktop runs as an app. The web interface and API offer different capabilities again. A skill that works in one surface may not port to another. Hooks that fire in the CLI may not exist in Desktop. MCP servers available in one context may be absent in another. The primitives aren't truly portable across contexts; they're bound to their environment. A user who develops sophisticated workflows in Claude Code discovers that moving to Desktop means rebuilding, not porting.</p> <p>One axis cuts through the confusion: active vs passive. Hooks are passive; they react to events. Commands are active; users invoke them explicitly. Skills can be either\u2014some auto-activate based on context, others wait to be called by name. This distinction isn't prominent in the documentation, but it shapes everything about how users experience the system. The passive primitives require trust: trust that they'll fire when needed, trust that they won't fire when not needed. The active primitives require memory: remember to invoke them, remember what they're called, remember which one applies to this situation. Different cognitive loads for different activation modes.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#thin-and-thick","title":"Thin and Thick","text":"<p>In 1973, anthropologist Clifford Geertz borrowed a distinction from philosopher Gilbert Ryle that clarifies something important about primitives.</p> <p>The example: a boy's eyelid moves. Thin description records the physical event. Eyelid contracts. Thick description asks what it means. Is the movement an involuntary twitch? A conspiratorial wink at a friend? A parody of someone else's wink, mocking their habit of winking? The same physical action carries entirely different significance depending on the layers of context and intention you can read into it.</p> <p>Thin description is portable but context-free. You can transfer \"eyelid contracts\" anywhere; it means the same thing (which is to say, almost nothing). Thick description is rich with meaning but requires knowing the situation, the players, the history. The thickness isn't decoration. It's what makes the description useful for understanding what actually happened.</p> <p>Geertz was writing about how anthropologists should study culture. But the distinction applies directly to AI primitives.</p> <p>A raw prompt is thin description. \"Summarize this document\" travels anywhere, works with any model, carries no accumulated context. It means the same thing every time, which limits how much it can mean.</p> <p>A skill with resources, examples, learned patterns, and embedded context is thick description. It knows what \"summarize\" means for you, in your domain, given your preferences. The thickness makes it powerful. It also makes it harder to transfer, harder to share, harder to explain to someone who doesn't share the context.</p> <p>Every AI tool designer takes a position on this spectrum, often without realizing it. The design question isn't where to position\u2014it's when. When does thickness arrive?</p> <p>The premature thickness trap: Some tools demand thick configuration upfront. Before you can use the system, you must specify your preferences, define your workflows, architect your structure. Adoption stalls because the cost of entry is too high. Users who don't yet know what they need can't articulate what they need.</p> <p>The permanent thinness trap: Other tools stay thin forever. ChatGPT's custom instructions are just text. They don't learn, don't accumulate, don't grow more sophisticated through use. The tool never gets better at being your tool. Every session starts from the same baseline.</p> <p>The interesting design space is between these failure modes. Primitives that start thin and thicken through use.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#the-practice-effect","title":"The Practice Effect","text":"<p>David Brin's 1984 science fiction novel The Practice Effect imagines a world where thermodynamics runs backward for human-made objects. Use something, and it improves. A crude flint knife, handled daily, gradually becomes razor-sharp steel. Leave it alone, and it reverts to crudeness. Objects don't wear out through use\u2014they wear in. They become more themselves, more fitted to their purpose, through the accumulated practice of being used.</p> <p>It's a thought experiment about entropy reversal. But it's also a design principle hiding in plain sight.</p> <p>What if AI primitives worked this way? Start thin: a text file, a simple prompt, an auto-detected pattern. Easy entry. Low commitment. The equivalent of Brin's rough flint. Then thicken through use. Context accretes. The system notices patterns in how you invoke the primitive, what works, what fails. The blade sharpens. If the primitive gets abandoned, it gracefully thins again, decays in relevance, doesn't become legacy cruft you're afraid to delete.</p> <p>This reframes the design challenge. Instead of asking \"what categories of primitive do we need?\" ask \"how does a primitive grow from thin to thick through practice?\" Instead of demanding users architect upfront, let structure emerge from use.</p> <p>Two mechanisms for thickening: active (the user explicitly configures, adds examples, specifies preferences) and passive (the system observes successful interactions and extracts patterns). Most tools offer only active thickening. The user must do the work of making things thick. Passive thickening\u2014the system learning from use\u2014is harder to build but closer to Brin's vision. The flint doesn't need you to explain how to become a knife. It learns from being used as one.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#lines-of-flight","title":"Lines of Flight","text":"<p>Every taxonomy assumes a path. Stage 1 \u2192 Stage 2 \u2192 Stage 3. Casual \u2192 Power User \u2192 Packager. The arrows imply progression is linear and the destination is singular.</p> <p>Gilles Deleuze (writing with F\u00e9lix Guattari) offers a counter-concept: lines of flight. Within any structured system, there are vectors of escape\u2014paths that lead somewhere the taxonomy didn't predict. These aren't failures of the system. They're its creative potential.</p> <p>The user who stays at Stage 1 forever but becomes extraordinarily deep within that simplicity. The \"skill\" that evolves through use into something its designers never anticipated, repurposed for a task no one imagined. The primitive that gets hijacked, bent, made to do something orthogonal to its intended function.</p> <p>Deleuze also gave us difference and repetition: the insight that repetition is never identical. Every time you invoke a skill, it's similar to the last invocation but never the same. Different context, different inputs, different user state. Through that repetition-with-difference, new patterns emerge that weren't designed, only enabled.</p> <p>This matters for primitive design because it warns against over-specifying the path. If your taxonomy assumes linear progression, you'll design for that assumption and miss the users doing something more interesting. The primitives that enable lines of flight are the ones loose enough to be repurposed, thin enough to be thickened in unexpected directions.</p> <p>A primitive that only does what it was designed to do is a primitive that will eventually be abandoned when users' needs evolve. A primitive that can become something else\u2014that's the one that survives.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#the-composition-question","title":"The Composition Question","text":"<p>When software engineers hear \"composition,\" they reach for familiar patterns. Functions composing into pipelines. Objects delegating to other objects. Microservices calling microservices. Decades of abstraction design. Is primitive composition just this?</p> <p>The difference matters.</p> <p>Code-level composition is about implementation. How developers structure systems internally. The user never sees it; they experience the resulting behavior.</p> <p>Primitive-level composition is about user-facing abstractions. How end users\u2014who may not be developers\u2014combine capabilities. When a Claude Code agent invokes a skill which triggers a hook, the user is composing. But they're composing concepts, not code. The abstractions must make sense to someone who's never thought about dependency injection or interface segregation.</p> <p>This is where most AI tools fail the composition test. They offer primitives that engineers compose beautifully but that normal users experience as a wall of configuration. The composition is real, but it's hidden behind complexity that only developers navigate fluently.</p> <p>What legible composition looks like:</p> <p>Clear activation modes, first. When does this primitive fire? The user should be able to answer without reading documentation. If the answer requires understanding three other concepts, you've already lost most users.</p> <p>Intuitive relationships matter too. Skills and commands are peers, not parent-child. When users compose them, the hierarchy should follow how they think about capability and specificity, not how engineers think about code organization. When the mental model matches the implementation model, composition becomes learnable.</p> <p>Visible chains. When primitives invoke other primitives, users can see what called what. The composition isn't hidden in logs they'll never check. If something fails three layers deep, they can trace the path.</p> <p>Self-explaining failures, finally. When composition breaks, the error says \"the skill tried to invoke X but couldn't because Y,\" not a stack trace that requires programming knowledge to parse. The user who encounters an error should understand it in terms of the concepts they used, not the implementation details they never learned.</p> <p>The OOP parallel isn't wrong. But primitive composition is OOP for the user interface of AI itself. The user is the developer, and they're developing with concepts, not code.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#second-mover-lessons","title":"Second-Mover Lessons","text":"<p>If you're building a new AI-native system now, you have the advantage of watching the first movers. What does the archaeology reveal?</p> <p>What to learn:</p> <p>Composition enables power. Claude Code's primitives can invoke each other\u2014agents calling skills calling commands. This composability is more powerful than isolated capabilities. Design for combination, not just creation.</p> <p>Activation modes matter more than categories. The passive/active distinction\u2014does this fire automatically or wait for invocation?\u2014determines adoption patterns more than what you call the primitive. Users learn activation modes faster than they learn taxonomies.</p> <p>Packaging solves distribution. Plugins as a meta-primitive for sharing configurations. Without a packaging story, every team reinvents what other teams already built. With it, best practices propagate.</p> <p>Surfaces constrain primitives. A primitive that only works in one environment (CLI but not desktop, desktop but not API) creates fragmentation. Design for the least capable surface you must support, or accept that your primitives won't be portable.</p> <p>What to avoid:</p> <p>Taxonomic explosion. Every new category adds cognitive overhead. Six primitives may already be too many for most users. Before adding a seventh, ask whether an existing primitive could grow to cover the use case.</p> <p>Structural similarity with semantic difference. If skills, commands, and agents are all markdown files, users will conflate them. Either differentiate by structure (commands are YAML, skills are markdown) or accept that users will remain confused about which is which.</p> <p>Thickness-upfront requirements. Don't make users architect before they've experimented. The user who must specify everything before trying anything is a user who often doesn't try at all. Let structure emerge through use.</p> <p>Ignoring the plateau. The progression from casual user to power user to packager isn't universal. Many users plateau\u2014happily\u2014at Stage 2. They use saved prompts, have custom instructions, and never want more. Design for them too. A tool that only rewards the power path alienates the majority.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#designing-for-emergence","title":"Designing for Emergence","text":"<p>Consider an alternative approach: one primitive that does the work of many.</p> <p>A playbook that starts as almost nothing. Auto-detected from context, or a blank slate the user names. No upfront configuration required. The user begins interacting; the system begins learning. No ceremony, no architecture, no decisions that feel like commitments.</p> <p>The playbook accumulates through use. Each successful interaction can contribute a bullet\u2014an atomic piece of knowledge about what works. \"When the user asks about X, they usually want Y.\" \"This API returns dates in format Z.\" \"Avoid suggesting W; the user rejected it three times.\"</p> <p>The bullets score themselves through feedback. Helpful bullets get used more, ranked higher, surfaced more often. Harmful bullets decay. The playbook doesn't just grow; it learns what growth is valuable.</p> <p>This sounds simple. The design questions hiding underneath are not.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#active-and-passive-thickening","title":"Active and Passive Thickening","text":"<p>How does thickness arrive? Two paths, and the choice between them shapes everything.</p> <p>Active thickening puts the user in control. You add a bullet explicitly: \"When I ask for a summary, I want three paragraphs maximum.\" You edit existing bullets: \"Actually, make that five paragraphs for technical documents.\" You delete bullets that no longer serve: \"I've changed my mind about the formatting.\" The playbook thickens because you made it thicken. The thickness reflects your intentions, articulated and recorded.</p> <p>Active thickening has the virtue of transparency. You know what's in the playbook because you put it there. You can inspect, modify, explain. The thickness is yours in a way that feels like ownership. It also has the vice of demanding labor. You must notice patterns worth capturing, articulate them clearly, remember to record them. Most users don't. The premature thickness trap reappears in disguise: instead of configuring upfront, you're expected to configure continuously, and the users who won't do either end up with playbooks that never thicken at all.</p> <p>Passive thickening puts the system in control. The model observes your interactions, notices patterns, extracts what seems to work. You didn't tell it that you prefer three-paragraph summaries; it noticed you asked for rewrites every time a summary ran longer. It adds the bullet without being asked. The playbook thickens through observation rather than instruction.</p> <p>Passive thickening has the virtue of actually happening. Users don't need to remember to configure; configuration emerges from use. The Practice Effect made real: the knife sharpens itself through cutting. It also has the vice of opacity. What did the system learn? Why did it learn that? Users who inspect their playbooks find bullets they didn't author, patterns they didn't consciously create, knowledge that came from somewhere but not from intentional input. The thickness is real but feels uncanny, like finding notes in your own handwriting that you don't remember writing.</p> <p>The design question isn't which mechanism to choose. Both have failure modes. The question is how they interact.</p> <p>Consider a playbook that thickens passively but surfaces its learning actively. The system extracts a pattern; before adding it as a bullet, it asks: \"I noticed you often want X. Should I remember this?\" The user confirms or declines. Passive observation, active confirmation. The thickness emerges from behavior but requires consent before becoming permanent.</p> <p>Or consider the inverse: active input with passive refinement. The user adds a bullet: \"Prefer concise responses.\" The system observes that \"concise\" means something different for code review than for email drafts. It splits the bullet, refines the pattern, creates thickness the user initiated but didn't fully specify. Active input, passive elaboration.</p> <p>These hybrids matter because pure active thickening doesn't happen and pure passive thickening feels invasive. The playbook that works is one where the labor is distributed: the system does what it can observe, the user does what requires intention, and the interface between them is clear enough that both know who's responsible for what.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#the-curator-problem","title":"The Curator Problem","text":"<p>Not all thickness is valuable. A playbook that remembers everything becomes useless, buried under the weight of accumulated context that's no longer relevant, was never important, or contradicts itself because the user's preferences changed.</p> <p>The curator problem: how does the playbook decide what to keep?</p> <p>The simplest approach is feedback scoring. Bullets that get used and don't get overridden accumulate positive signal. Bullets that get used and immediately corrected accumulate negative signal. Over time, the helpful rises and the harmful sinks. Natural selection for knowledge.</p> <p>But feedback is sparse. Most bullets, most of the time, simply exist without being tested. The pattern about date formats rarely surfaces because dates rarely come up. The preference about paragraph length activates occasionally, gets no explicit feedback, persists indefinitely. Absence of negative signal isn't presence of positive signal; it's absence of signal entirely.</p> <p>A stricter curator rejects most potential bullets at the point of entry. Not \"learn everything, let feedback sort it out\" but \"learn almost nothing, demand evidence before admission.\" The system notices a pattern three times before even proposing it. The user confirms before it becomes permanent. The permanence is provisional, requiring reconfirmation after disuse. Every bullet must justify its presence repeatedly, and bullets that can't are expelled.</p> <p>This strictness has costs. Genuine insights that appear once and never recur get lost. Edge cases that matter deeply but rarely get forgotten. The curator optimizes for the common case and fails the uncommon one. But the alternative\u2014a playbook that hoards everything\u2014fails differently, becoming so thick that nothing can be found and the thickness becomes noise rather than signal.</p> <p>The curator problem has no clean solution. It requires tuning: how strict, for which domains, with what decay rate, allowing what overrides. The parameters aren't universal; they vary by user, by domain, by how the playbook is used. A playbook for writing needs different curation than a playbook for code review. The curator that works is one calibrated to its context, and calibration takes iteration.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#decay","title":"Decay","text":"<p>Thickness that persists beyond its usefulness becomes cruft. The bullet about how you wanted dates formatted in 2024 may not reflect how you want them in 2026. The pattern extracted from your first month of use may not apply after your role changed and your work shifted. Knowledge has a shelf life, and playbooks that ignore this become museums of obsolete preferences.</p> <p>Decay mechanisms matter.</p> <p>The simplest is time-based: bullets fade unless reconfirmed. A bullet unused for six months loses weight, surfacing less often, eventually becoming dormant. The playbook thins automatically in areas where it's not being tested. The knowledge doesn't delete (it might be needed again) but it attenuates, stepping back rather than stepping forward, allowing newer patterns to dominate.</p> <p>Time-based decay has the virtue of simplicity and the vice of indiscrimination. Some knowledge is timeless; some is seasonal; some is context-dependent in ways that time doesn't capture. The bullet about quarterly reporting formats should surface every three months, not decay between quarters. The bullet about the old API version should decay the moment the new version deploys. Time alone can't distinguish these cases.</p> <p>Usage-based decay is smarter but more complex. Bullets that surface and succeed get reinforced. Bullets that surface and fail get penalized. Bullets that never surface attenuate slowly, but bullets that surface and produce neutral outcomes (no clear success, no clear failure) present ambiguous signal. Did the bullet help? Did it not matter? Is neutral good or bad? The scoring becomes intricate, and intricate scoring is hard to explain to users who want to understand why their playbook behaves the way it does.</p> <p>Context-based decay is smarter still and harder to implement. The system knows that your role changed, knows that the project ended, knows that the API was deprecated. It can decay bullets that reference contexts no longer relevant. But this requires understanding context at a level that current systems struggle with. Is this bullet about \"the Smith project\" or about project management generally? Should deprecating the Smith project decay the bullet or leave it intact? Context parsing is hard; automated context decay is harder.</p> <p>The honest answer is that decay mechanisms are unsolved. The systems that exist use crude proxies (time, usage frequency, explicit deletion) because sophisticated decay requires understanding that nobody has built yet. The playbook that works in 2025 is one that decays imperfectly but decays at all, recognizing that imperfect decay beats no decay, that a playbook which forgets some things badly is better than one which forgets nothing and drowns.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#cascades-the-distribution-problem","title":"Cascades: The Distribution Problem","text":"<p>Knowledge that works for one person might work for others. A playbook that understands how your company formats documents might benefit your whole team. A playbook that understands your industry's compliance requirements might benefit your whole organization. The value compounds when knowledge propagates.</p> <p>But propagation is perilous.</p> <p>Upward cascade: personal \u2192 team \u2192 organization. Your playbook develops a useful bullet. It works for you. Should it propagate to your team? To your department? To the entire company?</p> <p>The naive approach is automatic propagation: if a bullet works for enough individuals, it becomes shared. But \"works for you\" doesn't mean \"works for everyone.\" The preferences you've accumulated reflect your role, your style, your idiosyncrasies. Propagating them imposes your preferences on others who may have different roles, different styles, different idiosyncrasies. The bullet that improves your work might degrade theirs.</p> <p>A mediated approach interposes review. Personal bullets that might have team value get proposed to team administrators, who evaluate and either adopt or decline. Team bullets that might have organizational value get proposed upward similarly. Each transition requires a human judgment that the pattern generalizes.</p> <p>Mediation has costs. It creates bottlenecks: the administrator who must review every proposed propagation. It creates politics: whose bullets get adopted, whose get declined, what criteria govern the decision. It creates lag: valuable knowledge sits in individual playbooks while the propagation queue backs up. The friction that prevents bad propagation also prevents good propagation.</p> <p>Downward cascade: organization \u2192 team \u2192 personal. The company establishes institutional knowledge that should inform everyone's playbook. Compliance requirements. Brand guidelines. Security protocols. This knowledge shouldn't emerge from individual use; it should be imposed from above.</p> <p>Downward cascade seems simpler but has its own problems. Imposed bullets conflict with learned bullets. The organization says \"always include the legal disclaimer\"; your personal playbook learned you never want legal disclaimers because you're in engineering, not sales. Which wins? Can you override institutional bullets? If so, what's the point of imposing them? If not, the playbook stops being yours.</p> <p>The design challenge is boundary management. Which knowledge is personal (never propagates, fully customizable), which is team (propagates within team, overridable with justification), which is institutional (propagates everywhere, not overridable)? The boundaries aren't obvious, change over time, and vary by domain.</p> <p>Lateral cascade: peer to peer. Your colleague's playbook has a useful bullet you lack. Can you borrow it? Should playbooks be sharable, searchable, mixable? If so, the knowledge graph becomes networked rather than hierarchical, and the propagation patterns become unpredictable.</p> <p>Lateral sharing enables serendipity\u2014discovering that a colleague solved a problem you're facing\u2014but also enables pollution: importing bullets that seemed useful but don't fit your context, accumulating shared cruft that nobody owns and nobody maintains.</p> <p>The cascade problem is genuinely hard. Most systems avoid it by keeping playbooks purely personal, sacrificing propagation for simplicity. The systems that enable propagation usually pick one direction (upward only, or downward only, or lateral only) and one mechanism (automatic, or mediated, or manual) and accept that other patterns are unsupported. A complete solution would handle all directions with appropriate mechanisms for each, and nobody has built that yet.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#the-modes-problem","title":"The Modes Problem","text":"<p>Here's the objection that deserves serious engagement: even if we collapse the taxonomy to one primitive, the underlying behavioral patterns remain. Claude Code didn't invent six categories arbitrarily. Each maps to a genuine need:</p> <ul> <li> <p>Passive detection (skills): The user wants something to happen automatically when context suggests it. \"When I'm writing a blog post, apply my voice guidelines.\" No invocation required; the system recognizes relevance and acts.</p> </li> <li> <p>Active invocation (commands): The user wants to trigger something explicitly. \"/deploy\" or \"/review\" \u2014 a deliberate action at a chosen moment. The user stays in control of when.</p> </li> <li> <p>Deterministic guarantees (hooks): Some behaviors must happen regardless of what the model decides. Validate before writing. Log after executing. The model's judgment is explicitly removed from the loop.</p> </li> <li> <p>Parallel delegation (agents): Complex work that benefits from separate execution \u2014 spin up an environment, do substantial work, return results to the main context. The user doesn't want to watch; they want to receive.</p> </li> </ul> <p>These patterns aren't arbitrary. They correspond to different relationships between user intention and system behavior. Collapsing them into \"playbook\" doesn't make the distinctions disappear; it just refuses to name them. The user who wants passive detection still wants passive detection, whether you call it a \"skill\" or not.</p> <p>So how does a playbook accommodate patterns that seem to require different primitives?</p> <p>One answer: modes within the primitive. The playbook is one thing, but bullets within it can have different activation modes. Some bullets are \"always listening\" \u2014 they surface when context matches, without being called. Some bullets are \"invocable\" \u2014 they wait to be triggered by explicit user action. Some bullets are \"constraints\" \u2014 they fire deterministically on specific events, bypassing model judgment entirely. Some bullets can \"spin off work\" \u2014 they launch parallel execution and return results.</p> <p>The primitive is singular; the modes are plural. You don't learn six categories; you learn one category with a mode selector. \"This bullet is passive.\" \"This bullet requires invocation.\" \"This bullet is a constraint.\" The cognitive load shifts from \"which primitive?\" to \"which mode?\" \u2014 and modes are easier to understand because they're variations on a theme rather than entirely separate concepts.</p> <p>A deeper answer: these patterns reflect something about language itself.</p> <p>The philosopher J.L. Austin distinguished different kinds of speech acts by their \"illocutionary force\" \u2014 the type of action a utterance performs. Statements describe. Questions request information. Commands direct action. Promises commit the speaker. The same words can carry different force depending on how they're used: \"The door is open\" might be a description, a complaint, a request to close it, or an invitation to leave.</p> <p>AI interaction has analogous modes:</p> <ul> <li>Declarative: \"When summarizing, prefer three paragraphs.\" A statement of preference that becomes background context.</li> <li>Imperative: \"/summarize this document.\" A direct command requesting immediate action.</li> <li>Constitutive: \"Always run the linter before committing.\" A rule that constitutes how the system must behave, not subject to interpretation.</li> <li>Delegative: \"Research competitors and report back.\" A handoff that launches separate work.</li> </ul> <p>These aren't implementation details. They're different kinds of relationship between speaker and system, different ways that language can function in use. The linguist wouldn't collapse declarative and imperative sentences into one category just because both use words. The modes are meaningful.</p> <p>What this suggests for playbook design: the primitive can be singular, but it must accommodate modal variety. A bullet isn't just content; it's content plus mode. The mode determines how the bullet participates in interaction:</p> Mode Activation Model Judgment Execution Declarative Passive (context-triggered) Model decides relevance Inline with conversation Imperative Active (user-invoked) Model executes Inline with conversation Constitutive Event-triggered Bypassed (deterministic) Inline or pre/post Delegative Active (user-invoked) Model executes Separate context, returns results <p>The playbook doesn't need four primitive types. It needs one primitive type with a mode property that governs activation and execution. The thickness accumulates in the content; the mode shapes how content participates.</p> <p>What this preserves from Claude Code:</p> <p>The genuine insight that different behavioral patterns exist. Passive, active, deterministic, parallel \u2014 these aren't arbitrary categories but real distinctions about when and how things happen. A playbook that ignores modes would force everything into one behavioral pattern, which isn't simplification \u2014 it's impoverishment.</p> <p>What this changes from Claude Code:</p> <p>The primitive proliferation. Instead of skills, commands, hooks, agents as separate file types with separate documentation and separate mental models, you have bullets with modes. The user learns one concept (bullet) with one variation axis (mode). The cognitive load is lower because the variation is structured rather than categorical.</p> <p>What remains hard:</p> <p>Mode selection. When a user adds a bullet, they must choose its mode \u2014 or the system must infer it. \"Always validate JSON before saving\" sounds constitutive (deterministic, must-happen). \"When reviewing code, check for security issues\" sounds declarative (passive, model-judged). \"Run the full test suite\" sounds imperative (active, explicit invocation). Can the system infer mode from phrasing? Probably partially. Will it get it wrong sometimes? Certainly. The mode selector adds complexity that \"just a playbook\" seemed to avoid.</p> <p>The honest assessment: modes don't disappear just because you don't name them. The question is whether modal complexity lives in the taxonomy (multiple primitives) or in the primitive (one primitive, multiple modes). The playbook approach bets that the latter is more learnable. That bet might be right. But it's a bet, not an escape from the underlying complexity.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#what-this-approach-avoids","title":"What This Approach Avoids","text":"<p>The taxonomy problem \u2014 one primitive, not six. Users don't need to distinguish skills from commands from hooks. They have a playbook with bullets. The modes are properties of bullets, not separate categories to learn.</p> <p>The premature thickness trap\u2014starts thin, grows through use. Users can begin immediately and watch sophistication emerge.</p> <p>The structural confusion\u2014one concept, one mental model. The playbook is the playbook.</p> <p>The fluency gap\u2014the same abstraction serves casual users and power users. The difference is how thick their playbook has become, not which primitives they've mastered.</p> <p>The activation mode confusion\u2014the playbook is always active, always learning, always available. You don't invoke it; it participates. You don't configure its activation; it's on.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#what-this-approach-requires","title":"What This Approach Requires","text":"<p>Trust in emergence. You won't know what playbooks become until users use them. The design enables; it doesn't prescribe. The first users will create playbook shapes the designers didn't anticipate, will thicken in directions that weren't planned, will discover uses that weren't imagined. This is feature, not bug, but it requires designers who can tolerate not knowing.</p> <p>Mechanisms for decay. Thickness that outlives its usefulness must thin. Bullets that stop being helpful should fade, not persist forever because no one deleted them. The decay doesn't need to be perfect; it needs to exist.</p> <p>Paths for scope transition. Personal knowledge that proves valuable should be easy to share. The friction of distribution should be proportional to the risk of sharing. Institutional knowledge that applies to everyone should be imposable without destroying personal customization.</p> <p>Calibration infrastructure. The curator strictness, the decay rate, the propagation rules\u2014these aren't universal constants. They need tuning per deployment, per domain, per organization. The playbook system that works is one that's tunable, not one that's hardcoded to parameters that worked for the designers.</p> <p>Transparency mechanisms. Users will ask: \"Why did the playbook say that?\" The answer can't be \"it learned, somehow.\" There needs to be an audit path, a way to trace behaviors back to bullets, bullets back to sources, sources back to interactions. Opacity kills trust, and trust is the only thing that makes passive thickening acceptable.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#second-mover-advantages","title":"Second-Mover Advantages","text":"<p>If you're building this now, after watching Claude Code and others develop their primitives, you can learn from their archaeology.</p> <p>Don't multiply categories. The first movers invented six primitives and are still explaining the differences. Start with one. Let it prove insufficient before adding a second. The cognitive overhead of categories compounds faster than the capability of categories.</p> <p>Design for the activation spectrum. The hard problem isn't active vs passive; it's the territory between them. Build the hybrid mechanisms early\u2014passive observation with active confirmation, active input with passive refinement\u2014rather than bolting them on later.</p> <p>Make decay a first-class feature. The first movers built accumulation and hoped decay would happen through user maintenance. It didn't. Users don't maintain. Build decay into the system, make it visible, make it tunable. The playbook that forgets gracefully beats the one that remembers pathologically.</p> <p>Solve distribution early. The first movers built personal primitives and then struggled to make them sharable. The packaging layer (plugins) came late, after users had already developed habits around manual copying. Build the cascade paths from the start, even if they're simple, even if they're one-directional. Retrofitting distribution is harder than building it in.</p> <p>Instrument everything. You don't know what patterns matter until users show you. The first movers built in the dark, unable to see which primitives were used, which were confused, which were abandoned. Instrument the playbook\u2014what's accessed, what's modified, what's proposed and declined, what's decayed and retrieved. The data tells you what to build next.</p> <p>Accept incompleteness. No first version will handle all the cases. The cascade problem has no clean solution. The decay mechanisms are approximations. The curator will be too strict for some users and too loose for others. Ship the incomplete thing. Learn from its failures. Iterate. The playbook that works is the one that improves through its own use, applying the Practice Effect to itself.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#the-feel-for-the-game","title":"The Feel for the Game","text":"<p>Pierre Bourdieu spent decades studying how people learn to navigate social worlds without consciously thinking about the rules. His term for this was habitus\u2014the embodied sense of \"how things work here\" that becomes second nature. A native speaker doesn't parse grammar before talking; the grammar is in their bones. They don't know the rules explicitly. They are the rules, enacted through practice.</p> <p>Habitus isn't knowledge you have. It's knowledge you've become.</p> <p>The LinkedIn stages assume \"workspace\" is the destination\u2014Stage 5, where everything is organized, managed, under control. But Bourdieu's framework suggests something different. The goal isn't more structure. It's structure that becomes invisible. Fluency isn't knowing more rules. It's no longer needing to think about them.</p> <p>Apply this to AI primitives. The best primitives disappear into practice. You stop thinking about whether this is a skill or a command or a hook. You stop wondering which category your workflow belongs to. You just do the thing, and the thing works, and the primitive that enabled it fades into the background of your practice.</p> <p>This reframes the design question. Not \"what's the right taxonomy of primitives?\" but \"what primitives become transparent through use?\" Not \"how do we organize user progression?\" but \"how do we make progression feel like growing fluency rather than learning bureaucracy?\"</p> <p>Brin's objects in The Practice Effect improve until they're maximally suited to their use. The crude knife becomes a perfect knife\u2014not a general-purpose knife, but your knife, fitted to your hand, sharpened for your tasks. AI primitives should work the same way. They thicken toward fitness, not toward complexity.</p> <p>The destination isn't a workspace full of well-organized primitives. It's a practice so fluent you forget the primitives exist.</p>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/15/thin-to-thick/#sources","title":"Sources","text":"<ul> <li>Austin, J.L. How to Do Things with Words (Harvard University Press, 1962)</li> <li>Bourdieu, Pierre. Outline of a Theory of Practice (Cambridge University Press, 1977)</li> <li>Brin, David. The Practice Effect (Bantam Books, 1984)</li> <li>Deleuze, Gilles and Guattari, F\u00e9lix. A Thousand Plateaus (University of Minnesota Press, 1987)</li> <li>Geertz, Clifford. \"Thick Description: Toward an Interpretive Theory of Culture.\" The Interpretation of Cultures (Basic Books, 1973)</li> <li>Claude Code Documentation</li> </ul>","tags":["ai","design","tools","emergence"]},{"location":"blog/2025/12/18/deep-structure/","title":"Deep Structure","text":"<p>Why do presentations exist as artifacts at all?</p> <p>The sociologist Bruno Latour offers a useful concept. He calls certain objects immutable mobiles: things that can travel across distances and contexts while remaining stable. A map is an immutable mobile. So is a legal contract, a scientific paper, a quarterly report. These artifacts enable coordination without co-presence. You don't need to be in the room; the document carries the message. It arrives the same as it left.</p> <p>Presentations function as immutable mobiles. The deck you send to investors, the training materials distributed to five hundred employees, the pitch signed off by legal\u2014these need to arrive identical to how they departed. The artifact isn't just for viewing. It's for traveling.</p> <p>This explains why organizations resist purely ephemeral content. The resistance looks like conservatism but functions as coordination. When the same artifact reaches everyone, you can discuss it, reference it, audit it, improve it. When every viewing generates something different, coordination mechanisms break. The investor sees a different deck than legal approved. The employee in Sydney learns different content than the employee in London.</p> <p>Any approach to presentations must grapple with this. The artifact isn't optional decoration on top of the communication. For many uses, it is the coordination mechanism.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#surface-level-operation","title":"Surface-Level Operation","text":"<p>PowerPoint gives you a canvas: a fixed-dimension rectangle, typically 16:9. You place text boxes, images, shapes. You manipulate visual elements directly, arranging them spatially until the slide looks right. Then you do it again for the next slide, and the next, until you have a deck.</p> <p>This is surface-level operation. You work directly on the output, the visual surface that viewers will see. Your actions map one-to-one onto pixels. Drag this text box here; it appears there. The tool is a direct manipulation interface for visual surfaces.</p> <p>Gamma, a presentation startup valued at over two billion dollars, improved surface-level operation by replacing slides with cards. Cards flex and scroll instead of sitting in fixed frames. Embedding live content becomes possible. AI generates an initial surface from your prompt. But the fundamental mode remains: you work on surfaces. You edit the generated cards. You refine the visual output. You produce an artifact, a shareable link to a deck you created.</p> <p>PowerPoint with Copilot now generates surfaces too. You prompt, it produces slides. The AI race is a wash. The difference between PowerPoint and Gamma is the format (fixed slides versus flexible cards) and distribution (files versus links). Both differences matter. But both tools share an assumption: the user operates at the surface level, manipulating or editing the visual output that viewers will consume.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#a-different-level","title":"A Different Level","text":"<p>In the 1950s, the linguist Noam Chomsky proposed a distinction that clarifies what it would mean to operate differently.</p> <p>Chomsky distinguished deep structure from surface structure. The surface structure is the actual sentence you hear or read, the specific words in their specific order. The deep structure is the underlying meaning, the relationships between concepts that the sentence expresses. Between them sit transformation rules, the grammar that converts deep structure into surface.</p> <p>The same deep structure can yield different surfaces. \"The dog bit the man\" and \"The man was bitten by the dog\" express the same underlying meaning. Different transformation rules (active voice versus passive voice) produce different surface forms. The meaning travels; the expression varies.</p> <p>Applied to presentations:</p> Linguistic Concept Presentation Equivalent Deep structure Intent, content, audience, purpose Transformation rules Patterns that shape communication Surface structure The visual output viewers experience <p>PowerPoint operates at surface. You manipulate the visual output directly. Gamma operates at surface with AI assistance. You prompt, then edit the visual output.</p> <p>What would it mean to operate at deep structure?</p> <p>You would specify what you want to communicate, to whom, and for what purpose. You would not manipulate visual elements. You would not edit cards or slides. The system would apply transformation rules (patterns that shape effective communication) and generate the appropriate surface. Different contexts might yield different surfaces from the same deep structure, just as the same meaning yields different sentences depending on transformation rules applied.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#transformation-rules-as-pattern-languages","title":"Transformation Rules as Pattern Languages","text":"<p>The transformation rules aren't arbitrary. They're the accumulated knowledge of what makes visual communication work.</p> <p>Edward Tufte spent decades codifying principles for data visualization. Maximize the data-ink ratio; every visual element should convey information. Show comparisons; always answer \"compared with what?\" Integrate evidence; keep explanations adjacent to what they explain. These aren't aesthetic preferences. They're transformation rules: given content that needs to be visualized, here's how to convert it into effective visual form.</p> <p>Pedagogical research offers transformation rules for learning. Progressive disclosure: don't overwhelm; reveal complexity gradually. Multiple representations: show the same concept through analogy, visual, code, and first principles. Retrieval practice: test understanding, don't just present information. These rules transform content into learning experiences.</p> <p>Narrative structure provides transformation rules for persuasion. Setup-conflict-resolution. Problem-solution. The hero's journey. Compare-contrast. These patterns shape how arguments unfold, how evidence lands, how conclusions feel earned.</p> <p>Christopher Alexander called this a pattern language, a generative grammar of composable patterns that produce valid designs. The patterns aren't templates. They're relationships that recur because they solve problems humans actually have. \"Light on two sides of every room\" isn't a room design; it's a pattern that can be instantiated infinitely many ways, each valid if it honors the relationship.</p> <p>A generative presentation layer would internalize these patterns as transformation rules. Given deep structure (intent, content, audience, purpose), the rules would generate appropriate surfaces. Different content would yield different outputs, the way different building programs yield different applications of \"light on two sides.\" But every output would embody patterns that make communication effective.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#whose-patterns","title":"Whose Patterns?","text":"<p>The preceding sections treat transformation rules as though they're discovered rather than designed. Tufte's principles, pedagogical frameworks, narrative structures, all presented as \"what makes communication effective.\" But effective for whom?</p> <p>The media philosopher Vil\u00e9m Flusser distinguished traditional images from technical images. Traditional images bear human intention directly; technical images are produced by apparatuses according to embedded programs. The danger isn't that technical images are bad; photography can be art. The danger is that people forget the program. They mistake the output for unmediated reality. The apparatus becomes invisible; its assumptions become nature.</p> <p>Transformation rules are programs. \"Maximize data-ink ratio\" encodes Tufte's values: his preference for density, his assumptions about what readers can parse. \"Progressive disclosure\" assumes a theory of how understanding develops. \"Problem-solution narrative\" privileges certain story shapes over others. None of this is wrong. But none of it is neutral either.</p> <p>The political theorist James C. Scott named a related concern: legibility. High modernist planning makes complex systems readable, countable, administrable. It produces standardized outputs, measurable outcomes, comparable results. What it loses is m\u0113tis\u2014local, practical, contextual knowledge that resists systematization. The forest optimized by the forester's grid may be ecological disaster. The city planned on rational principles may be unlivable in ways the planners couldn't model.</p> <p>Communication standardized through transformation rules gains legibility. Every output embodies proven patterns. Effectiveness becomes measurable: did the learner pass? did the pitch convert? did the board approve? What this can't capture is effectiveness that defies the patterns: the presentation that worked because it broke the rules, the learning that happened through productive confusion, the pitch that landed because it felt raw rather than polished.</p> <p>The response isn't to abandon transformation rules. Scott doesn't argue that legibility is avoidable or always bad; states need to administer, organizations need to coordinate. The response is to remain aware that rules are choices, not discoveries. Make the pattern layer visible rather than invisible. Surface the assumptions so users can contest them. When someone asks \"why did the system structure it this way?\" the answer should be available, not hidden in opaque weights but stated as principle, open to challenge and revision. The apparatus shouldn't disappear into the outputs it produces.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#use-case-modes-as-rule-configurations","title":"Use Case Modes as Rule Configurations","text":"<p>This reframes what \"learning mode\" and \"presentation mode\" mean.</p> <p>They're not separate tools with separate codebases. They're different configurations of transformation rules applied by the same underlying capability.</p> <p>Learning mode weights pedagogical rules: progressive disclosure, knowledge checking, multiple representations, spaced repetition. The deep structure (content + audience + purpose) passes through these rules to generate an interactive learning experience.</p> <p>Presentation mode weights persuasion rules: narrative arc, evidence integration, visual hierarchy, call to action. The same deep structure, different rule weights, different surface.</p> <p>Pitch mode weights credibility and urgency rules: social proof, scarcity, authority signals, clear ask. Again, same capability, different configuration.</p> <p>One capability, multiple modes. The modes select which transformation rules dominate. This is architecturally cleaner than building separate tools, and it explains why the outputs feel related: they share deep structure and core patterns, differing only in emphasis.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#the-layer-architecture","title":"The Layer Architecture","text":"<p>Architecturally, this suggests three layers:</p> <p>Intent layer: Where humans operate. You express what you want to communicate, to whom, for what purpose. This layer translates intent into specifications the substrate can execute, filtered through transformation rules.</p> <p>Transformation layer: The patterns that shape effective communication. Tufte's principles. Pedagogical frameworks. Narrative structures. This layer doesn't produce anything directly; it constrains and guides how deep structure becomes surface. Modes (learning, presentation, pitch) are configurations of this layer\u2014named presets that weight different rules. Learning mode weights pedagogical patterns; pitch mode weights credibility and urgency patterns. Same layer, different settings.</p> <p>Substrate layer: The engine that produces interactive experiences. Next.js, React, Tailwind. Technologies that render web applications. This layer knows nothing about presentations or learning. It knows how to turn specifications into working interfaces.</p> <p>The layers compose. You express intent (\"help my team understand this concept\"). The transformation layer, configured for learning, weights pedagogical rules and generates a specification. The substrate layer renders an interactive learning experience.</p> <p>Same person, different context (\"convince the board this matters\"). The transformation layer reconfigures: different rule weights, different specification, different output. The capability remains singular.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#resolving-the-durability-question","title":"Resolving the Durability Question","text":"<p>Earlier I noted that organizations need immutable mobiles, stable artifacts for coordination. Doesn't operating at deep structure, with generated surfaces, break this?</p> <p>Not if you distinguish what needs to persist.</p> <p>The transformation rules persist. Think of brand guidelines. An organization coordinates around \"follow the brand\" without specifying every output. The guidelines are stable; the outputs vary infinitely. Similarly, \"use the compliance training curriculum\" becomes a coordination point. Legal approves the curriculum structure, the required content, the assessment criteria. Each employee's experience differs (pacing, examples, remediation paths) but the rule configuration is fixed and auditable.</p> <p>The deep structure persists. The intent, content, audience, and purpose can be stored, versioned, approved. Legal signs off on the deep structure and transformation rules, not on every possible surface rendering.</p> <p>Specific surfaces can be stamped as canonical. When you need an immutable mobile (the investor deck, the board presentation, the compliance training record) you generate once and stamp that instance as canonical. It becomes the traveling artifact. Other instances remain ephemeral.</p> <p>This separates coordination durability from output rigidity. The things that need to stay stable for coordination (rules, intent, canonical instances) stay stable. The things that benefit from adaptation (surfaces for specific viewers, contexts, devices) can vary.</p> <p>The pattern layer is where institutional knowledge accumulates. An organization's \"house style\" becomes transformation rules. Their pedagogical approach becomes learning mode configuration. Their narrative conventions become pitch mode weights. The knowledge is durable; the outputs are contextual.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#the-allographic-turn","title":"The Allographic Turn","text":"<p>The philosopher Nelson Goodman distinguished two kinds of art. Autographic works require the artist's hand: a painting is the physical object the painter made; a forgery, however perfect, isn't the work. Allographic works have notations that can be instantiated: a musical score isn't the music; performances are. The score travels; expressions vary. What matters is fidelity to the notation, not identity with an original surface.</p> <p>Presentations have been autographic. The deck is the work. You arrange the slides, place the text boxes, choose the images. The artifact that results is what you made, and copies are copies of that specific arrangement. This is why \"deck building\" feels like craft, why people develop signature styles, why templates never quite fit. The surface is the work.</p> <p>The shift to deep structure operation is an allographic turn. The work becomes the notation: intent, content, audience, purpose, encoded in a form that transformation rules can interpret. Surfaces are performances of that notation. Different contexts, different performances. The score travels; the renderings vary.</p> <p>This pattern recurs. Assembly language programmers worked at surface, manipulating registers and memory addresses directly. High-level languages introduced an allographic layer: you specify what you want; the compiler generates the machine code. HTML authors once hand-crafted every tag; frameworks now generate markup from component specifications. In each case, what was craft becomes substrate. The level of operation rises; the previous surface becomes implementation detail.</p> <p>The constraints that seem strategic are actually structural. PowerPoint can't easily shift to allographic operation because its entire architecture assumes the slide is the work. The file format encodes surfaces. The interface is a surface manipulation tool. Thirty years of muscle memory expects direct manipulation. These aren't business decisions that could be reversed; they're ontological commitments baked into the product's conception of what a presentation is.</p> <p>Gamma changed the format primitive but kept the autographic assumption. Cards are still surfaces you edit. The constraint Gamma preserved is the constraint that matters most: the user works on the output. A deeper move would abandon that assumption entirely. Specify intent; let transformation rules handle expression. The presentation becomes score, not painting.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#what-emerges","title":"What Emerges","text":"<p>When presentation capability operates at deep structure, forms become possible that couldn't exist when every surface required manual creation.</p> <p>Responsive communication. A sales call where the system generates competitive analysis slides as objections surface. The rep mentions a competitor; a comparison matrix appears, drawn from the deep structure of product positioning, rendered according to transformation rules for competitive framing. The prospect asks about pricing; a TCO visualization materializes. The surfaces are ephemeral, created for this conversation, dissolving after. No one built these slides. They were performed from notation when the context demanded them.</p> <p>Adaptive learning. Training materials that notice a cohort struggling with a concept and generate additional visual explanations mid-course. Not a single learning path but a space of paths, each learner traversing differently depending on what the transformation rules infer from their responses. The deep structure (what must be learned, in what sequence, to what standard) stays fixed. The surfaces vary per learner. Assessment becomes continuous; the system knows what you understood because it watched which generated explanations you needed.</p> <p>Analytical multiplicity. An analysis that produces small multiples: the same data visualized dozens of ways simultaneously, so you can find the view that reveals the pattern. Not an analyst choosing one chart but the system generating the space of valid visualizations, constrained by Tufte's rules. The insight isn't in any single rendering; it's in the comparison across renderings. This is only possible when generation is cheap enough to be speculative.</p> <p>Conversational evidence. A board discussion where relevant data visualizations appear as the conversation unfolds. Someone asks about regional performance; a map materializes. Someone questions the trend; a time series with confidence intervals appears. The presentation isn't a deck someone prepared; it's a capability that renders evidence on demand, shaped by transformation rules for executive communication. The board member who asks sharp questions gets sharper visuals.</p> <p>The trajectory extends further than current interfaces suggest:</p> <p>Near-term: Better surfaces faster. This is where Gamma and Copilot compete. AI generates initial outputs; humans refine. The surface remains the work; the tools just speed up its creation.</p> <p>Medium-term: Context-aware generation. The system knows who's viewing and what they know. A sales deck renders differently for a technical buyer than for a procurement officer. Same deep structure, different transformation rule weightings. The author specifies intent once; the system handles audience adaptation.</p> <p>Long-term: Communication that doesn't exist until needed and dissolves after use. No deck to store, version, email. A capability that produces the right surfaces when context demands them, then lets them go. The archive changes. What do you version-control when surfaces are ephemeral? The deep structure. The transformation rules. The canonical instances stamped for coordination. But most outputs are never saved because they were only ever performances.</p> <p>What changes about expertise. When surfaces generate cheaply, the bottleneck shifts. Knowing how to arrange rectangles on a canvas becomes less valuable. Knowing what to communicate, and why, and to whom, becomes more valuable. The skill isn't \"design\" in the sense of visual arrangement; it's \"design\" in the sense of intent specification. The person who can articulate \"help the board understand why customer acquisition cost increased despite higher marketing spend, so they can decide whether to continue the current strategy\" produces better outputs than the person who says \"make a presentation about Q3 results.\" Thick intent yields thick output.</p> <p>What changes about coordination. Organizations coordinate around artifacts. When artifacts are ephemeral, coordination must anchor elsewhere: to deep structures that persist, to transformation rules that are versioned and approved, to canonical instances stamped for situations that require immutable mobiles. The shift is from coordinating around outputs to coordinating around capabilities. \"Use L2 onboarding mode\" replaces \"send them this deck.\" The capability is the coordination mechanism.</p> <p>What changes about authorship. If the surface is a performance of notation, who authored it? The person who specified the deep structure? The team that curated the transformation rules? The system that composed them in context? Authorship distributes. The question \"who made this presentation\" becomes as complicated as \"who made this musical performance\"\u2014composer, arranger, conductor, orchestra, venue acoustics all contributed. The allographic turn doesn't eliminate authorship; it complicates it, spreads it across the layers that contribute to the final rendering.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#the-residue-of-craft","title":"The Residue of Craft","text":"<p>Does this eliminate the need for design skill? Craft relocates rather than disappears.</p> <p>Someone must know what makes visual communication effective. The transformation rules don't write themselves. Tufte's principles exist because someone spent decades analyzing what works and why. Pedagogical frameworks exist because researchers studied how understanding develops. These insights become inputs to the system: transformation rules that shape generation.</p> <p>Someone must curate the pattern layer. Which principles apply in this domain? How should learning mode differ from pitch mode for this organization? What makes communication effective for this audience? The answers require judgment that can't be automated because it depends on context the system doesn't have.</p> <p>Someone must specify intent clearly. Deep structure isn't magic. \"Make a presentation about Q3 results\" is thin; it yields generic output. \"Help the board understand why customer acquisition cost increased despite higher marketing spend, so they can decide whether to continue the current strategy\" is thick; it yields focused output. The skill of specifying intent precisely, knowing what you actually want to communicate, remains valuable.</p> <p>The craft of arranging rectangles on a canvas may become less valuable. The craft of knowing what to communicate, why, and how: that persists and may become more valuable when surface-level work no longer differentiates.</p> <p>But there's a harder question. The philosopher Michael Polanyi observed that we know more than we can tell. The expert presenter who senses the room losing attention and pivots mid-slide\u2014that's tacit knowledge. The designer who tries three layouts before one \"feels right\" is exercising judgment that resists articulation. Transformation rules encode explicit knowledge: principles someone could write down, patterns someone could name. The tacit dimension doesn't transfer so easily. It manifests in practice but can't be fully captured in rules.</p> <p>Some craft may be irreducibly embodied. The question isn't whether all craft disappears but whether the irreducible part matters. If tacit judgment improves outputs at the margins, making good presentations slightly better, then the loss is tolerable, perhaps unnoticeable. If tacit judgment is what distinguishes communication that transforms from communication that merely informs, the loss is substantial. Honest answer: we don't know yet. The boundary between codifiable and tacit shifts as systems improve. What seemed irreducibly human yesterday becomes pattern-matchable today.</p> <p>There's a related concern. The anthropologist Lucy Suchman studied how people actually use technology and found that plans are resources for action, not determinants of it. People figure out what they mean through the act of making. The presenter who struggles with a slide discovers, in the struggling, what they actually wanted to say. The designer who drags a text box and immediately sees it's wrong learns something about the communication that wasn't accessible before the attempt.</p> <p>If intent specification happens upstream, separated from the struggle with surfaces, where does that discovery occur? The system generates; the user accepts or rejects. But the cycle of attempt, failure, revision is how thick intent develops. Deep structure operation might need to preserve productive friction rather than eliminate it entirely. Not intent specification followed by surface generation, but something more iterative: rough intent, generated surface, refined intent, regenerated surface, each pass sharpening what the user actually meant. The interface isn't just for expressing intent. It's for discovering it.</p> <p>Simon Wardley maps value chains by evolution: activities move from genesis (novel, uncertain) through custom-built and product to commodity (standardized, interchangeable). As activities commoditize, value migrates up the stack to whatever remains scarce. Surface generation is commoditizing fast. Every AI lab ships it. The substrate becomes utility. What stays scarce is what sits above: the transformation rules that encode effective communication, the pattern curation that fits those rules to organizational context, the intent specification that distinguishes thick from thin requests. Defensible value concentrates where judgment remains. The moat isn't in the generation; it's in the layers that shape what gets generated.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#thickness-through-use","title":"Thickness Through Use","text":"<p>In a previous essay, I explored how AI primitives should thicken through practice: starting thin, accumulating context, becoming more fitted to their use. The concept applies here.</p> <p>The transformation layer can learn. Which patterns work for this organization? Which rule weightings produce outputs that users don't override? What intent expressions recur, and what do they reveal about communication needs?</p> <p>This isn't explicit configuration. It's observation. The system notices that learning outputs in this organization are consistently modified to add more visual examples, and adjusts the transformation rules to weight visual representation higher. It notices that pitch decks here always get restructured to lead with customer testimonials, and adjusts narrative rules accordingly.</p> <p>The pattern layer thickens. House style emerges from use rather than specification. The capability becomes increasingly fitted to the organization, not through configuration but through accumulation.</p> <p>This is also what saves the system from the legibility trap. High modernist planning fails when universal rules meet local conditions they weren't designed for. Thickness through use inverts that relationship. The system doesn't arrive with fixed patterns and impose them; it starts with general principles and lets local practice reshape them. The m\u0113tis\u2014the contextual knowledge Scott argued couldn't be captured in plans\u2014enters through accumulated observation rather than explicit encoding. What the designer couldn't specify, the system learns. The transformation rules become fitted to place rather than imposed on it.</p> <p>This is where the moat forms. A generic presentation tool can be copied. A transformation layer that has learned an organization's communication patterns over years of observation cannot easily be replicated. The thickness is the defensibility.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#the-bet","title":"The Bet","text":"<p>The complications are real. Intent discovery through making, tacit knowledge that resists codification, the blindness of systems to what falls outside their categories. None of these concerns are dismissable. But they don't change the direction; they change what you build along the way.</p> <p>The system needs iterative loops, not one-shot generation. Rough intent in, surface out, user surprise, refined intent, regenerated surface. That's not a compromise; it's the actual interface for deep structure operation. The clean separation was always a simplification for exposition.</p> <p>The tacit ceiling exists, but it keeps rising. What seemed irreducibly human in 2020 became pattern-matchable by 2024. The bet isn't that tacit knowledge disappears. The bet is that the codifiable part grows faster than most people expect, and that's where the leverage concentrates.</p> <p>The legibility trap is real, but thickness through use is the response. Not universal patterns imposed from above; local patterns learned through observation. The system that's been watching how this organization communicates for three years has absorbed m\u0113tis that no designer could specify. That's not perfect. The unobserved still falls outside. But it's better than static rules, and it compounds.</p> <p>Surface-level tools will persist. PowerPoint isn't going anywhere; neither is Gamma. People will still arrange rectangles on canvases, and some of them will do it brilliantly. But the volume of visual communication is exploding while the number of people who can craft surfaces well stays roughly constant. The gap gets filled by generation from deep structure, whether anyone plans it that way or not.</p> <p>The presentation as stable artifact category is passing. What replaces it is communication capability: the same deep structure producing slides when you need slides, documents when you need documents, interactive experiences when you need those, and forms we haven't named yet when context demands them. The artifact crystallizes from the capability when coordination requires it. Otherwise, surfaces stay ephemeral.</p> <p>This is not a prediction about what will happen. It's a statement about what to build. Operate at deep structure. Encode transformation rules. Make the patterns visible and contestable. Let thickness accumulate through use. The organizations that do this will communicate more effectively than those still crafting surfaces by hand. That's the bet.</p>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/18/deep-structure/#sources","title":"Sources","text":"<ul> <li>Alexander, Christopher. A Pattern Language (Oxford University Press, 1977)</li> <li>Chomsky, Noam. Syntactic Structures (Mouton, 1957)</li> <li>Flusser, Vil\u00e9m. Towards a Philosophy of Photography (Reaktion Books, 1983)</li> <li>Goodman, Nelson. Languages of Art (Hackett, 1968)</li> <li>Latour, Bruno. \"Visualisation and Cognition: Drawing Things Together\" (1986)</li> <li>Polanyi, Michael. The Tacit Dimension (University of Chicago Press, 1966)</li> <li>Scott, James C. Seeing Like a State (Yale University Press, 1998)</li> <li>Suchman, Lucy. Plans and Situated Actions (Cambridge University Press, 1987)</li> <li>Tufte, Edward. The Visual Display of Quantitative Information (Graphics Press, 1983)</li> <li>Gamma founding story and metrics from company sources and industry analysis</li> </ul>","tags":["ai","presentations","generative-systems","strategy"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/","title":"Twelve Predictions for 2026","text":"<p>Most AI predictions ask what the technology will do. These predictions ask what we'll become.</p> <p>The capability forecasts will mostly be right. Models will get faster, cheaper, more capable. Agents will handle more tasks. Adoption curves will steepen. None of that is particularly interesting to predict because none of it tells us what matters: how these tools will reshape the people and organizations that use them.</p> <p>These are my predictions for the transformations we'll see in 2026. I draw on thinkers who've spent careers studying what happens when tools change their users: Stiegler's pharmacology of technology, Brown's analysis of neoliberal subjectivity, Zuboff's instrumentarian power, Ingold's critique of hylomorphism, Snowden's complexity framework.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#at-a-glance","title":"At a Glance","text":"# Prediction 1 Workers trained in AI-assisted environments will discover they can't function without assistance\u2014and won't know they've crossed the threshold. 2 AI assistance won't reduce workload. It will raise the baseline while creating new work to absorb the gains. 3 AI will produce infinite adequate content. Meaning won't disappear; it will migrate upstream, from production to selection. 4 The architecture of choice blindness is already operational. Agent mediation will complete what recommendation systems began. 5 \"Human-made\" will become a managed ambiguity\u2014premium brand, not verifiable fact. 6 Inside every agentic organization, humans will quietly maintain the actual work. We'll know because layoffs will reveal what capacity maps couldn't see. 7 2026 will see a cascade failure that crosses system boundaries. Benchmark gaming will have inflated confidence in capability. 8 Paying to avoid AI mediation will become luxury. Everyone gets the tool; not everyone gets the human who knows when it's wrong. 9 Trust infrastructure will become control infrastructure once verification data becomes monetizable. 10 Skill atrophy will follow the tools, not the work\u2014and the jags are training artifacts, not natural boundaries. 11 Vibe coding will produce software no one understands. Failure won't matter until it does\u2014and then no one can fix it. 12 Most content will be produced for AI consumption, not human reading. Human literacy becomes optional; machine parseability mandatory.","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#1","title":"1","text":"<p>The prediction: Workers trained in AI-assisted environments will discover they can't function without assistance. The backup forgets how to be backup. Organizations will learn, in moments of system failure or changed circumstance, that \"human in the loop\" assumed competencies that continuous AI assistance was actively depleting.</p> <p>Bernard Stiegler developed what he called a pharmacology of technology (borrowed from Derrida, who borrowed from Plato, who noted that the Greek pharmakon meant both poison and cure). The ambivalence isn't a bug to be fixed. It's constitutive. Every tool that extends human capacity creates new dependencies, new incapacities, new vulnerabilities that didn't exist before the extension.</p> <p>Stiegler called this proletarianization\u2014not in the Marxist sense of class position but in the older sense of losing the knowledge embedded in your practice. The proletarian, etymologically, is one who has nothing left but their offspring; their skill has been captured by machines and managers, leaving only labor-power to sell. Frederick Taylor's time-motion studies began this capture for physical labor. AI completes it for cognitive work.</p> <p>The evidence is already clinical. A Lancet study found that endoscopists' adenoma detection rate dropped from 28% to 22% after regular AI assistance\u2014a 20% relative decline in finding precancerous growths once the AI was removed. Months of exposure, not years. The pharmacological threshold crossed faster than anyone expected. The researchers called it \"the Google Maps effect\": the natural human tendency to over-rely on decision support systems until the underlying skill atrophies.</p> <p>This is the deepest form of the threshold problem. You cannot perceive the crossing because crossing it is what eliminates your capacity to perceive. The question isn't whether augmentation helps\u2014it does\u2014but what's the dose at which cure tips into poison. When does \"human oversight\" become ceremonial, the supervisor who rubber-stamps decisions they no longer have competence to evaluate?</p> <p>Dave Snowden's Cynefin framework names this property: in complex systems, cause and effect are only coherent in retrospect. You cannot analyze your way to the threshold in advance because the threshold emerges from the interaction of factors that only become legible after the crossing. Organizations will try to monitor for skill degradation, to establish metrics and checkpoints. These efforts assume the problem is complicated\u2014discoverable through expert analysis. But pharmacological thresholds are complex. They reveal themselves only when you've already passed them.</p> <p>Call it what it is: pharmacology, the constitutive double of augmentation, the incapacity produced by the capacity.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#2","title":"2","text":"<p>The prediction: AI assistance won't reduce workload. It will raise the baseline. Every efficiency gain gets captured by expanded scope, reduced headcount, or elevated expectations. Workers will keep more balls in the air while being told they're augmented, enhanced, empowered. The exhaustion is structural, and invisible to those experiencing it.</p> <p>Wendy Brown, in Undoing the Demos, traced how neoliberal rationality remakes the human subject. The citizen becomes human capital. Every domain (education, health, relationships, leisure) gets reframed as investment in oneself. You don't learn for understanding; you acquire credentials. You don't rest; you recover strategically. The metric is always appreciation of the asset that is you, measured against others similarly investing in themselves.</p> <p>The achievement-subject that Byung-Chul Han describes\u2014the entrepreneurial self who exploits itself more efficiently than any external discipline could\u2014finds its apotheosis in AI augmentation. The tool makes you more productive; productivity raises the baseline of expected production; meeting the baseline requires more optimization. You run faster to stay in place.</p> <p>Wharton research calls this the \"AI Efficiency Trap\": a four-stage pattern where efficiency gains become permanent baselines. The Red Queen dynamic. And it compounds: what AI produces, humans must verify. The \"workslop\" phenomenon (low-quality AI output that recipients must then fix) means efficiency for the producer becomes verification burden for the recipient. One analysis estimates $9 million in annual drag for a 10,000-employee organization, the hidden cost of absorbing output that passed automated checks but fails human evaluation.</p> <p>The acceleration isn't experienced as acceleration. Workers report productivity gains even as total cognitive load increases. The treadmill is invisible to those on it. You can't rest because resting means falling behind; you can't question the structure because questioning looks like excuse-making, a failure to take responsibility for your own human capital. The exhaustion may not be experienced as exhaustion but as a new normal that feels like personal inadequacy rather than systemic extraction.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#3","title":"3","text":"<p>The prediction: AI will produce infinite adequate content. Every channel will fill with output that's competent, interchangeable, and meaningless. Markets will respond with craft premiums and \"human-made\" labels. But meaning won't simply become scarce\u2014it will migrate upstream, from production to selection.</p> <p>Tim Ingold, the anthropologist, has spent decades studying how makers relate to materials. His central argument is that Western thought is captive to hylomorphism\u2014the idea that making is imposing form on matter. You have a design; the material is just stuff to be shaped. Against this, Ingold proposes that real making is conversation. The maker follows the material's tendencies, adjusts to its resistances, discovers possibilities that neither maker nor material could have specified in advance.</p> <p>AI is pure hylomorphism. You prompt; it produces. The creative stack collapse (idea to production to distribution compressed to nothing) means the elimination of resistance. And resistance is how we learn. The woodworker who learns which way the grain runs, the writer who discovers what the sentence wants to say: these encounters with material recalcitrance are how knowledge accrues through practice, how the maker becomes skilled rather than merely productive.</p> <p>Remove the resistance and you remove the education. You can generate more, faster, but you don't learn from the generating because the generating teaches nothing\u2014it complies.</p> <p>The hylomorphism critique raises a question it doesn't resolve: does infinite adequate content crowd out the conditions for meaning-making, or does meaning-making simply migrate to new domains? If meaning emerged from resistance in production, perhaps it now emerges from curation, selection, and context-setting\u2014activities that remain human even when production doesn't. The curator who chooses what to surface, the editor who shapes the raw output, the audience that determines what resonates: meaning may not disappear so much as move upstream.</p> <p>This doesn't resolve the problem; it relocates it. The skills that atrophy are production skills. The skills that matter become selection skills. And the question remains whether selection without production retains access to the tacit knowledge that made good selection possible.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#4","title":"4","text":"<p>The prediction: As AI agents handle discovery, comparison, and purchase, consumers will lose not just influence over decisions but awareness that decisions occurred. What got shortlisted, what got filtered, what criteria governed the selection\u2014all invisible, all upstream, all architecture.</p> <p>But this isn't a step change. It's the completion of something already underway.</p> <p>Shoshana Zuboff's Surveillance Capitalism introduced instrumentarian power: a form of domination that works through behavior shaping rather than coercion or ideology. Instrumentarianism doesn't care about your soul; it cares about your actions. It doesn't need you to believe anything. It just needs you to do what the prediction engines predict, nudged toward outcomes that serve interests other than your own.</p> <p>The architecture of choice blindness is already operational. Algorithmic curation on platforms already produces it\u2014what TikTok shows, what Amazon recommends, what Spotify queues. You experience the result of a selection process without experiencing the selection. The menu is presented as the territory.</p> <p>Agent mediation intensifies this but doesn't inaugurate it. \"Your customer is an agent\" means the last moment of visibility\u2014when the curated options were at least presented to a human\u2014disappears entirely. Delegation becomes invisibility. The instrumentarian power isn't visible because it operates through defaults, through what seems natural, through paths of least resistance that were engineered to resist least in particular directions.</p> <p>This is qualitatively different from advertising. Advertising presents itself as persuasion and allows resistance. Agent mediation removes even the moment of presentation. There's nothing to resist because there's nothing to see. The choice was made before you knew a choice existed.</p> <p>Regulatory attention will eventually arrive. But regulation assumes something legible to regulate. The whole point of agent mediation is illegibility: decisions happening in latent spaces no one can audit. The regulatory apparatus was built for a world where power announced itself. Instrumentarian power doesn't announce.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#5","title":"5","text":"<p>The prediction: \"Human-made\" will become a performance genre, not a fact about origin. Content will be designed to signal human production whether or not humans produced it. The market rewards the signal, so the signal is what gets produced. Authenticity becomes something you perform, brand, and sell\u2014detached from any underlying reality about how things were made.</p> <p>This is Brown's entrepreneurial self applied to creative work. Your humanity becomes an asset class with exchange value in a market where AI handles commodity production. \"Being human becomes the differentiator\" sounds affirming until you notice the framing: human as competitive positioning, identity as brand strategy, authenticity as authentication theater performed for market advantage.</p> <p>The EU AI Act's disclosure mandates will accelerate the dynamic. Once \"AI-generated\" is flagged, \"human-generated\" becomes a performable brand\u2014with or without humans involved. The premium isn't for quality; AI can produce quality. The premium is for the story that someone cared, for the ritual significance of human attention, for authentication that may or may not correspond to anything about how the thing was made.</p> <p>The verification problem compounds this: as \"human-made\" becomes a valuable signal, the incentive to fake it increases, which drives demand for verification infrastructure, which creates new vectors for gaming. C2PA Content Credentials can verify AI involvement but not human involvement; the asymmetry matters. You can prove something touched AI; you can't prove it didn't.</p> <p>The equilibrium is probably \"human-made\" as luxury brand rather than verifiable fact\u2014like \"organic\" or \"artisanal.\" A managed ambiguity that serves market differentiation without resolving the underlying question. The authentication isn't about truth. It's about the performance of a category that consumers will pay for.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#6","title":"6","text":"<p>The prediction: Inside every agentic organization, humans will quietly maintain the actual work. They'll patch what agents can't see, handle exceptions the system doesn't model, embody tacit knowledge the capacity map refuses to recognize. These shadow systems will be undocumented, unmeasured, invisible to leadership. And load-bearing.</p> <p>Consider the procurement specialist at a hospital who knows which vendor API calls timeout under load and manually re-submits orders every Tuesday when the system backs up. Or the senior developer who maintains a mental map of which AI-generated code paths actually work in production versus which ones pass automated tests but fail under edge conditions. The documentation says the workflow is automated. The workflow is automated. These people are why it works.</p> <p>James C. Scott documented how high-modernist schemes always generate informal resistance from the people who need to make things work. The formal system sees legible inputs and outputs; the shadow system handles everything illegible. Organizations that redesign around AI will create clean processes that ignore the messy reality of work. The people who bridge the gap\u2014translating between what the system expects and what actually happens\u2014won't appear on any org chart.</p> <p>Shadow systems cluster at predictable locations: at the interfaces between AI systems where handoffs fail silently, at exception-handling boundaries where the model's training distribution doesn't match production reality, wherever tacit knowledge is required that can't be specified in advance. The procurement specialist who knows which API calls to re-submit isn't following a procedure that could have been documented. She's responding to patterns that only become visible through immersion in the work itself. You can't capture that in a capacity map because the capacity map assumes the complicated domain. The actual work keeps slipping into complexity.</p> <p>This labor will be invisible until it walks out the door. Organizations that lay off \"redundant\" roles after AI adoption will experience a measurable increase in system failures within 6-12 months\u2014that's the testable version of this prediction. The knowledge was tacit, embedded in practice, resistant to documentation precisely because it lived in the gap between what the official process specified and what the work actually required. The shadow system becomes visible only through its absence.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#7","title":"7","text":"<p>The prediction: 2026 will see at least one high-profile cascade failure. An over-optimized system will encounter conditions its designers assumed away. No human will be positioned to intervene\u2014not because humans were absent but because the humans present lacked the competence, authority, or situational awareness to act. \"Human in the loop\" will stop being reassurance and start being questioned.</p> <p>High-profile failures have already occurred: Tesla FSD crashes, Cruise dragging a pedestrian 20 feet, Air Canada's chatbot giving legally binding bad advice. What distinguishes a 2026 event is the cascade element: failures that cross system boundaries, affect multiple organizations, reveal interconnection that wasn't previously visible.</p> <p>A specific mechanism feeds this overconfidence: benchmark gaming. Benchmarks are verifiable environments, which makes them susceptible to the same optimization pressure that makes models spike in capability. As Karpathy observed, \"training on the test set is a new art form.\" Organizations deploy based on benchmarked performance that's been systematically inflated by training incentives, then encounter conditions outside the test distribution. The gap between benchmarked capability and actual capability is invisible until it isn't.</p> <p>The fragility compounds quietly. Each efficiency optimization removes a buffer. Each removal seems rational in isolation. The system as a whole becomes a house of cards\u2014not because any single card is weak but because removing \"redundant\" cards was the entire optimization strategy. Nassim Taleb's framework applies: every optimization that removes human redundancy is a bet that conditions remain stable. When AI agents transact with AI agents, human circuit breakers are removed entirely. Small errors cascade with no one positioned to notice until too late.</p> <p>Cynefin clarifies the failure mode. Removing human buffers assumes the system operates in the complicated domain: stable enough that expert design can anticipate failure modes, that sense-analyze-respond will catch problems before they propagate. But interconnected systems under stress don't stay complicated. They drift toward complexity, where causes produce effects that weren't predictable from the initial conditions, where the right response is probe-sense-respond\u2014and probing requires someone positioned to probe. Chaotic conditions demand act-sense-respond, immediate intervention to stabilize before analysis. Neither response is available when the humans have been optimized out.</p> <p>The highest risk domains: financial systems where algorithmic trading already produces flash crashes, logistics where supply chain optimization has removed inventory buffers, healthcare where diagnostic systems may interact with treatment systems with minimal human checkpoint. A domain-specific prediction is riskier but more useful: watch for cascade events in logistics, where the interconnection is highest and the buffers thinnest.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#8","title":"8","text":"<p>The prediction: Paying to avoid AI mediation will become luxury, not Luddism. Human customer service, unaugmented education, the experience of dealing with someone who has time to attend\u2014these will command premium prices and carry social status.</p> <p>But the class structure is more complex than \"AI for the many, human attention for the few.\"</p> <p>This is the refusal economy. Not rejection of technology but selective exemption from it. The wealthy already purchase human attention: concierge medicine, private tutors, personal assistants. As AI mediation becomes the default experience, unmediated human contact becomes scarce and therefore valuable. The refusal is marketed, not stigmatized; positioned as premium, not resistance.</p> <p>The pattern has precedent. Organic food, slow fashion, digital detox retreats: each emerged as the commodity version of some experience became so degraded that paying to avoid it became desirable. AI mediation is the next commodity experience to generate its own refusal premium.</p> <p>The sharper formulation is contested. Karpathy argues that \"regular people benefit a lot more from LLMs compared to professionals\"\u2014vibe coding lets anyone build software, AI writing assistants help the less-skilled more than the already-skilled. There's truth in this: AI does democratize production. But democratized production and quality outcomes aren't the same thing. The truly wealthy will get both: AI productivity plus human oversight, AI efficiency plus human judgment. The class divide isn't AI versus human but AI-plus-human for some, AI-alone for the rest. Everyone gets access to the tool; not everyone gets access to the human who knows when the tool is wrong.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#9","title":"9","text":"<p>The prediction: The infrastructure built to establish trust (provenance, audit trails, content credentials) will become infrastructure for influence. What can be verified can be shaped. The systems we build to know what's true will be the systems through which truth gets managed.</p> <p>The prediction that \"trust becomes expensive\" captures something real. When AI can generate anything, establishing authentic origin requires infrastructure: cryptographic signatures, audit trails, verification protocols. C2PA Content Credentials, adopted by OpenAI and others, attach metadata to establish origin and detect alterations.</p> <p>But Zuboff's framework asks: who controls the trust infrastructure? Legibility is the precondition for control. The systems that verify provenance are systems that track provenance. The platforms that authenticate content are platforms that know what content exists, where it came from, who made it. Trust infrastructure is surveillance infrastructure by another name. What can be audited can be shaped.</p> <p>The pivot point is monetization. Trust infrastructure becomes control infrastructure when verification data becomes monetizable\u2014when a platform uses verification records for purposes beyond verification. The question is mechanism: Does it happen when regulators mandate verification that platforms then monetize? When interoperability requirements create centralized verification bodies? When verification becomes a condition of distribution and the distributors set the terms?</p> <p>The direction is clear even if the precise mechanism isn't. Infrastructure built for one purpose gets repurposed for the purposes of whoever controls it. This has happened with every information infrastructure in living memory. It will happen with trust infrastructure too.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#10","title":"10","text":"<p>The prediction: Skill atrophy won't be uniform. It will follow the contours of what's easiest to externalize. Workers will retain capacities that resist externalization while losing capacities that externalize smoothly. The shape of the hollowing will be determined by the shape of the tools, not by what would be safe or wise to lose.</p> <p>The junior analyst who uses Claude to draft equity research may retain the ability to structure an argument while losing the ability to read a balance sheet closely. The architect using parametric design may retain spatial intuition while losing the feel for construction constraints that came from years of watching buildings fail. The doctor using diagnostic AI may retain bedside manner while losing the pattern recognition that came from seeing thousands of cases without assistance.</p> <p>The hollowing follows the tools, not the work. What gets externalized is what AI handles well, not what humans can afford to forget. The shape of incapacity is the negative space of the AI's competence. Harvard Business School's \"jagged frontier\" research documents this precisely: AI improves performance 40% within its capability boundary but causes a 19 percentage point performance drop outside it. The frontier is jagged, and the hollowing follows the jags.</p> <p>But the jags aren't natural capability boundaries. They're training artifacts, shaped by what's verifiable. Models spike in capability near math and code puzzles because those domains offer automatic rewards for optimization. The jagged frontier tracks what's trainable, not what's important. Skills that resist verification\u2014judgment, taste, knowing when to distrust the output\u2014may be precisely the skills that atrophy fastest, because they're the skills that don't show up in benchmarks.</p> <p>The hollowing accelerates generationally. If seniors retain tacit knowledge but juniors never acquire it, the gap deepens over time. The transmission problem. The skills live in the seniors' hands but never transfer because the juniors were augmented from the start. Stanford research found nearly 20% employment decline for developers aged 22-25 since late 2022, the period coinciding with generative AI's emergence. Meanwhile, developers over 26 saw stable or growing employment. Entry-level positions are where augmentation hits first, which is precisely where skill formation happens.</p> <p>The shape of the hollowing isn't static. It deepens in the direction of AI capability, creating ever-wider gaps in what humans can do without assistance. And the gaps become permanent when the people who could have taught the skills age out before the teaching happens.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#11","title":"11","text":"<p>The prediction: The rise of vibe coding will produce software that no one understands. Applications conjured through natural language, never inspected, deployed into production. The maintenance crisis won't be too much code\u2014it will be code without comprehension. Systems will fail in ways that can't be debugged because debugging assumes someone understood the system in the first place.</p> <p>Karpathy coined \"vibe coding\" to describe programming via English, forgetting the code exists. He's right that it empowers: anyone can build software, professionals can build more, projects that would never exist now get built. \"Code is suddenly free, ephemeral, malleable, discardable after single use.\"</p> <p>But discardable code has a way of becoming load-bearing. The quick script becomes the production system. The vibe-coded prototype never gets replaced because it works\u2014until it doesn't. And when it fails, no one knows how it works because no one ever looked. The code was generated, not written; deployed, not understood.</p> <p>This is the hylomorphic trap applied to software. You prompt; it produces. The resistance that taught programmers how their systems behaved\u2014the compilation errors, the debugging sessions, the hours tracing logic\u2014disappears. You get the output without the education. The maker becomes a requester.</p> <p>The failure mode isn't that vibe-coded software is bad. Much of it works fine. The failure mode is that no one can fix it when it breaks, extend it when requirements change, or audit it when security matters. The software exists; the understanding doesn't. Technical debt accumulates in a new form: not messy code that someone could clean up, but code that no one can read because no one ever did.</p> <p>Two classes of software will emerge: serious infrastructure maintained by shrinking cadres of specialists who still understand what they're building, and an expanding ocean of vibe-coded applications that work until they don't. The middle\u2014comprehensible, maintainable, medium-scale software built by developers who understood their tools\u2014will hollow out.</p> <p>The optimistic response: just vibe-code the next thing. Failure stops mattering when replacement is cheap. But this assumes failure is recoverable. What about the data accumulated over years? The integrations with systems you don't control? The failure at 2am during a critical process? Regeneration takes time; damage happens in real-time. The disposability thesis assumes failures are clean and contained. Real failures are messy and cascading\u2014especially through systems no one understood in the first place.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#12","title":"12","text":"<p>The prediction: Most written content will be produced for AI consumption, not human reading. Documentation, reports, articles\u2014written with the assumption that AI will process them before (or instead of) human eyes. Human literacy becomes optional; machine parseability becomes mandatory.</p> <p>We're already there. Documentation written assuming AI will summarize it. Reports produced \"roughly\" because the expectation is that an AI will clean them up. Articles structured for RAG retrieval rather than human comprehension. The audience has shifted. Content becomes training data, not communication.</p> <p>This closes the loop on infinite adequate content. If AI writes and AI reads, humans become peripheral to the communication itself. The slop loop: AI generates, AI summarizes, human sees summary, human prompts AI, AI generates. At each step, the human touches less of the original. The content exists; human comprehension of it becomes optional.</p> <p>The feedback loop that made writing improve\u2014human readers responding to human writers\u2014breaks when the readers are models. What makes text \"good\" shifts from what communicates to what's parseable, retrievable, summarizable. Clarity for humans and clarity for machines aren't the same thing; when they diverge, machine clarity wins because machines are the actual audience.</p> <p>This is instrumentarian power applied to cognition itself. Not just what you buy or what you choose, but what you read, what you think you know, what you believe you understand. The mediation extends from the marketplace to the mind.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#what-connects-them","title":"What Connects Them","text":"<p>The common thread is pharmacology\u2014Stiegler's insight that every extension creates a new incapacity, every cure its own poison. AI extends cognitive capability and creates cognitive dependency. AI extends choice and creates choice blindness. AI extends productivity and creates the conditions for exhaustion. The extensions are real. The incapacities are equally real. The incapacities are harder to see because the extensions are what we're looking at.</p> <p>The second thread is agency. When we use passive voice\u2014skills become bottlenecks, trust becomes expensive\u2014we obscure that someone is making choices, capturing value, distributing risk. Passive voice turns political arrangements into natural processes. Predictions become descriptions of what will happen rather than what is being done.</p> <p>The third thread is domain confusion. Cynefin's contribution is recognizing that different kinds of problems require different kinds of responses, and that treating complex systems as merely complicated is a category error with consequences. The pharmacological thresholds in prediction 1, the shadow systems in prediction 6, the cascade failures in prediction 7\u2014all share a common structure: problems that look analyzable from a distance but turn out to be emergent up close, requiring responses (probe-sense-respond, or act-sense-respond in crisis) that weren't built into the system. The optimization that removed human redundancy assumed complicated conditions would persist. The failure occurs when they don't.</p> <p>The fourth thread is ephemerality. Code that's disposable, content that's replaceable, skills that don't transmit, understanding that never forms. The predictions describe a world where nothing persists except the systems that generate ephemeral outputs. The permanent becomes scarce; the temporary becomes default. We lose the concept of building on what came before\u2014because what came before was never meant to last.</p> <p>These predictions are about transformations already underway, transformations we might navigate differently if we saw them clearly.</p>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2025/12/20/twelve-predictions-for-2026/#sources","title":"Sources","text":"<ul> <li>Bernard Stiegler, Technics and Time series, especially vol. 3: Cinematic Time and the Question of Malaise (Stanford University Press, 2011)</li> <li>Wendy Brown, Undoing the Demos: Neoliberalism's Stealth Revolution (Zone Books, 2015)</li> <li>Shoshana Zuboff, The Age of Surveillance Capitalism (PublicAffairs, 2019)</li> <li>Tim Ingold, Making: Anthropology, Archaeology, Art and Architecture (Routledge, 2013)</li> <li>James C. Scott, Seeing Like a State (Yale University Press, 1998)</li> <li>Byung-Chul Han, The Burnout Society (Stanford University Press, 2015)</li> <li>Nassim Nicholas Taleb, Antifragile (Random House, 2012)</li> <li>Dave Snowden &amp; Mary E. Boone, \"A Leader's Framework for Decision Making,\" Harvard Business Review (November 2007)</li> <li>Dell'Acqua et al., \"Navigating the Jagged Technological Frontier,\" Harvard Business School Working Paper (2023)</li> <li>Roma\u0144czyk et al., \"Endoscopist deskilling risk after exposure to artificial intelligence in colonoscopy,\" The Lancet Gastroenterology &amp; Hepatology (2025)</li> <li>Brynjolfsson, Chandar, Chen, \"Canaries in the Coal Mine? Six Facts about the Recent Employment Effects of Artificial Intelligence,\" Stanford Digital Economy Lab (2025)</li> <li>Andrej Karpathy, \"2025 LLM Year in Review,\" personal blog (December 2025)</li> <li>Andrej Karpathy, \"Animals vs. Ghosts,\" personal blog (2025)</li> </ul>","tags":["ai","predictions","future-of-work"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/","title":"The Political Economy of Fog","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#prologue-the-point-of-the-game","title":"Prologue: The Point of the Game","text":"<p>I keep returning to something the philosopher James Carse wrote nearly forty years ago, and that Sangeet Paul Choudhary put more sharply in a recent post: the point of an infinite game is to keep playing.</p> <p>This sounds like a platitude until you watch people forget it. You stay in the infinite game by winning finite games: the funding round, the product launch, the quarterly target, the acquisition. These finite games have clear winners and losers. They feel urgent. They come with metrics and deadlines and congratulations when you close them. But they are not the point. They are what you do to remain in the arena where the actual game unfolds.</p> <p>The pathology (and it is a pathology, not a mistake) is when we optimize so hard for the next finite win that we sacrifice the capacity to keep winning. When we confuse the battle for the war. When every decision serves next quarter at the expense of next decade. I've watched this happen to people I respect, and I've caught myself doing it more than I'd like to admit.</p> <p>What makes the AI platform market worth examining is that it has industrialized this confusion. Meta pays $2 billion for Manus in ten days. Cursor raises at $29 billion. The valuations make no sense as prices for things that exist; they make complete sense as prices for options on things that might. Everyone is playing finite games (the demo, the deal, the markup) and almost no one can tell whether these finite wins are building something durable or consuming the conditions for durability.</p> <p>The fog prevents the knowing. What follows is an attempt to trace its contours.</p> <p>By fog I mean something specific: a market condition where participants cannot evaluate their own productivity. Buyers cannot distinguish platforms that work from platforms that perform. Capital flows toward stories rather than outcomes; outcomes resist measurement. The fog isn't a temporary inconvenience that better tools will disperse. The fog is structural. It's produced by the same dynamics that produce the market itself.</p> <p>This essay moves in concentric circles through that fog. It starts with the individual developer who feels productive while actually getting slower, a perception gap documented in studies that should have caused more alarm than they did. It moves outward to markets that cannot learn from their participants, capital structures that reward performance over verification, and political battles over who gets to define what \"working\" even means. At the center is a question I find genuinely unsettling: what kind of people do we become when we work in conditions of structural unknowability? What happens to judgment, to attention, to the capacity for honest self-assessment, when the tools we use are optimized to make us feel effective regardless of whether we are?</p> <p>I don't have comfortable answers. But I've become convinced that the question (which game are you actually playing?) is the one that matters. The fog makes it difficult to answer, and that difficulty is itself the subject.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#circle-one-the-developer-in-the-fog","title":"Circle One: The Developer in the Fog","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i","title":"I.","text":"<p>In a converted warehouse in San Francisco, a developer named Marcus is having the most productive day of his career. He's been using an AI coding assistant for three months, and the code is flowing. Functions appear almost as fast as he can describe them. The boilerplate that used to consume his mornings materializes in seconds. He's touching more files, closing more tickets, shipping more features than he ever has before.</p> <p>Marcus feels like a god. His manager's Slack messages have shifted from check-ins to congratulations. The sprint velocity charts show his team pulling ahead. He's started mentoring junior developers on \"AI-native workflows,\" and they look at him the way he once looked at senior engineers who seemed to conjure solutions from thin air.</p> <p>What Marcus doesn't know (what he has no way of knowing from inside his experience) is that his production code is accumulating debt faster than he's shipping features. The AI-generated functions work in isolation but create subtle integration issues. The edge cases aren't handled because Marcus never learned to see edge cases; the assistant generated the happy path, and the happy path is what he tested. The refactoring that should have happened in month two was deferred because the velocity was so good, and now the codebase has calcified around patterns that will take six months to unwind.</p> <p>Marcus will discover this in eight months, when a production incident cascades through systems that looked fine in every review. By then, he'll have been promoted. By then, he'll be leading a team of developers who learned to code the way he taught them. By then, the debt will be structural.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii","title":"II.","text":"<p>In early 2025, METR (an AI safety research organization) published a study that illustrates a structure worth examining. Experienced developers using AI coding assistants completed tasks 19% slower than developers without AI assistance, while believing they had worked 20% faster.</p> <p>One study proves nothing, and this one had specific conditions: experienced developers, their own codebases, particular task types. The precise numbers may not generalize. But the structure they reveal is interesting regardless of the exact percentages. A 39-percentage-point gap between perceived and actual performance. The developers weren't just wrong about the magnitude of the effect; they were wrong about its direction. They experienced productivity gains that were, in fact, productivity losses.</p> <p>Daniel Kahneman would recognize this immediately. The developers' System 1 (fast, intuitive, pattern-matching) registered \"code appearing on screen\" as progress. The dopamine hit of watching the AI write was neurologically real. But System 2 (slow, analytical, the part that would notice the debugging took longer, that the edge cases weren't handled, that the total time had increased) wasn't activated. Why would it be? The feeling was so good. The velocity metrics were so impressive. Everything that could be measured was measuring improvement.</p> <p>This perception gap is not a bug in the METR study. It's a feature of how AI assistance works at the phenomenological level. The experience of using these tools is designed to feel productive. The interfaces reward prompting and generation. The metrics track output, not outcome. The developer sits in a fog of their own experience, accelerating confidently in a direction they cannot verify.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii","title":"III.","text":"<p>The temptation here is to moralize: the developers should have known better, should have measured more carefully, should have distrusted their intuitions. But this misses what makes the perception gap structurally interesting.</p> <p>This gap exists because the tools work at the level of phenomenology. The developer's experience of productivity is not a side effect of using AI assistants; it is what the tools produce. The code generation is secondary to the feeling of generation. The interfaces are optimized for engagement, which means they're optimized for the subjective experience of progress, which means they're optimized for producing the very gap that makes evaluation impossible.</p> <p>Philip K. Dick built a career exploring this structure. His characters live in worlds where reality is negotiable, where memories can be implanted, where the authentic cannot prove its authenticity because any proof could also be simulated. The horror in Dick's fiction is never the unreality itself; it's the impossibility of verification. You can never be certain you're not in the fake version.</p> <p>The METR study suggests something Dickian about AI assistance. The developer who feels productive but isn't faces an epistemological condition, not a measurement problem. The test that would reveal the truth (actual productivity measurement) requires standing outside the experience. But there is no outside. You're always experiencing your own experience. The feeling of productivity is itself a product, manufactured by interfaces optimized to produce that feeling.</p> <p>The authentic cannot prove its authenticity. The problem is structural, not individual.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv","title":"IV.","text":"<p>Simone Weil, the French philosopher and mystic, wrote about attention as a form of generosity: the capacity to wait, to not-act, to let the object reveal itself rather than projecting onto it. Attention for Weil is not focus in the modern productivity sense. It's something closer to receptivity: the willingness to sit with difficulty until understanding emerges.</p> <p>AI coding assistants invert this. They generate before you've fully attended. They answer before you've understood the question. They fill space that should be empty. The developer's experience of productivity is the experience of attention being colonized; the quiet space where understanding forms is now occupied by generation.</p> <p>This explains why the METR study found experienced developers more harmed than novices. The experienced developer had cultivated something over years of practice: a habit of attention, a learned capacity to sit with uncertainty until the problem resolved itself. They had developed what programmers sometimes call \"taste\": an intuition for code quality that emerges from countless hours of making mistakes and noticing their shapes.</p> <p>The AI \"helps\" by removing the waiting, which is the part where understanding happens.</p> <p>The novice, with no such practice to disrupt, experiences only the velocity. They haven't learned what they're losing because they never had it. For them, AI-assisted coding isn't a disruption of a practice; it's the practice itself. They are learning to code in conditions where the attention that would have produced understanding is never required.</p> <p>The counterargument: taste isn't only about output. It's about the capacity to evaluate output, to know when something is good, to recognize quality you couldn't produce yourself. If AI assistance produces code while atrophying the capacity to evaluate code, you end up with developers who can generate endlessly but cannot tell what they've generated. The perception gap becomes permanent. You've produced achievement without understanding, velocity without direction.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#v","title":"V.","text":"<p>Gregory Bateson, the anthropologist and cyberneticist, described a pattern he called the double bind: a situation where someone receives two contradictory messages with no way to comment on the contradiction. The child told to be spontaneous, which is impossible because spontaneity cannot be commanded. The employee praised for initiative and punished for not following procedures. The messages conflict, but you can't say they conflict, because saying so violates the rules of the interaction.</p> <p>Double binds, Bateson observed, are pathogenic. They create what he called schismogenesis: escalating cycles where each attempt to resolve the contradiction makes it worse.</p> <p>Watch Marcus, the developer caught in the perception gap. He feels productive. The metrics say he's productive. But his codebase is degrading; the deadlines are slipping, the technical debt is mounting, the production incidents are accumulating. So he leans harder into AI assistance. More prompting, more generation, more of the thing that feels like it's working. The productivity feeling intensifies. The actual productivity deteriorates further. Each turn of the cycle digs the hole deeper.</p> <p>Marcus can't comment on the contradiction because he can't perceive it. The gap between felt productivity and real productivity is invisible from inside the felt productivity. The double bind is perfect: you cannot solve the problem because solving it requires seeing it, and the problem is precisely that you cannot see.</p> <p>Bateson studied double binds in families, in institutions, in communication patterns that drove people toward breakdown. He didn't live to see them encoded in software interfaces. But the structure he identified (contradictory messages without meta-commentary) describes exactly what happens when a tool that feels like progress isn't. The developer receives two signals: you're doing great from the interface, the velocity metrics, the dopamine; and you're falling behind from the actual outcomes that no one is measuring. The signals conflict. The conflict cannot be named.</p> <p>The schismogenesis runs until something external breaks it. A production incident. A burnout. A layoff. The system doesn't self-correct because the system is structured to prevent the perception that would enable correction.</p> <p>Marcus is winning finite games. Every velocity metric, every closed ticket, every sprint completion is a win. He's optimizing brilliantly for the game in front of him.</p> <p>But coding skill is an infinite game. The point is to keep developing judgment, to maintain the capacity to evaluate code, to stay in the arena where quality can be recognized. Marcus is winning sprints while losing the ability to know what winning means. He's optimized so hard for finite victories that he's sacrificing the infinite game they were supposed to serve.</p> <p>The fog doesn't just hide whether you're productive. It hides whether your finite wins are serving your infinite game or consuming it.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#circle-two-the-market-that-cannot-learn","title":"Circle Two: The Market That Cannot Learn","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i_1","title":"I.","text":"<p>At the individual level, the perception gap is bad enough. At the market level, it compounds into something that should concern anyone trying to allocate capital rationally.</p> <p>George Akerlof won a Nobel Prize for explaining how markets collapse when buyers can't verify quality. In his 1970 paper \"The Market for Lemons,\" he showed that information asymmetry creates a death spiral. If buyers can't distinguish good cars from bad ones, they pay the average price. But at the average price, sellers of good cars lose money and exit. The average quality drops. Buyers adjust expectations downward. Prices drop further. More quality sellers exit. The market doesn't stabilize at medium quality; it races toward the bottom, stabilizing only when quality is so low that even uninformed buyers can tell what they're getting.</p> <p>The AI agent market has all the conditions for a lemons problem.</p> <p>Every vendor claims \"enterprise-ready AI.\" The demos are impressive; they're designed to be, optimized for the moments when someone is watching. Case studies are curated for persuasion, stripped of failures and context. Testimonials come from users caught in the same perception gap as Marcus, genuinely believing they're more productive while they're not.</p> <p>Buyers can't verify the claims. How would they? The product is cognitive augmentation. The outcome is supposed to be better decisions, faster work, higher quality. But the METR finding suggests that enthusiasm is a lagging indicator of actual performance. The users who love the tool most are the ones it's hurting most. The testimonial proves nothing except that the person giving it feels good.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii_1","title":"II.","text":"<p>The opposing view isn't obviously wrong.</p> <p>Markets have dealt with quality uncertainty before. Brands exist because they solve information problems: the buyer doesn't know if this particular product is good, but they know that products with this brand are usually good. Warranties and guarantees shift risk from buyers to sellers, incentivizing quality. Third-party certifications create trusted intermediaries. Reputation systems aggregate individual experiences into collective signals.</p> <p>Perhaps the AI platform market will develop these mechanisms. Perhaps we're simply early; the brands haven't formed, the certifications haven't emerged, the reputation systems haven't matured. Perhaps in five years there will be a clear hierarchy: platforms with track records of reliability, certification bodies that verify claims, enterprise buyers sophisticated enough to run rigorous evaluations. The lemons problem might be a transitional condition, not a terminal one.</p> <p>This is a reasonable position, and history provides some support. The early days of most technology markets feature exactly this kind of uncertainty. The software industry in the 1980s was full of lemons: products that crashed, vendors that disappeared, claims that evaporated on contact with reality. Over time, quality signals emerged. Microsoft became a brand you could trust (for certain values of trust). Enterprise software developed evaluation methodologies. The market sorted itself out.</p> <p>Medicine and financial services developed verification mechanisms for products at least as complex as AI platforms. Drug trials, clinical outcomes, audited returns, regulatory oversight. These markets aren't perfectly transparent, but they're not pure fog either. What enabled them to develop quality signals?</p> <p>Three things, mostly. First, regulatory forcing functions: the FDA, the SEC, bodies with teeth that could compel disclosure and punish fraud. Second, catastrophic failures with clear causation (thalidomide, Enron), events dramatic enough to create political will for transparency. Third, long enough feedback loops that outcomes eventually surfaced. A drug either works or it doesn't, a fund either returns or it doesn't, and given enough time the truth emerges.</p> <p>Does the AI platform market have these enabling conditions? Not obviously. No regulatory body has jurisdiction over \"does this AI actually help.\" No catastrophic failure has yet been clearly traced to AI assistance rather than human error. And the feedback loops are precisely what the perception gap corrupts; the outcomes don't surface because the users can't perceive them.</p> <p>But there's a reason to think AI platforms are different, and it has to do with the nature of what's being purchased.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii_1","title":"III.","text":"<p>Software, for all its complexity, eventually reveals itself. Install a word processor and use it for a month; you'll know whether it works. Deploy an ERP system and run your business on it; the quality becomes evident in the running. The feedback loop between use and evaluation is tight enough that quality eventually surfaces.</p> <p>AI platforms short-circuit this feedback loop.</p> <p>The product is cognitive augmentation: making decisions, generating content, automating judgment. The feedback loop between using the tool and knowing whether the tool helped requires evaluating the counterfactual: what would have happened without the tool? This is exactly what the perception gap prevents. The developer feels faster, the manager sees higher velocity, the metrics improve. The fact that the code is worse, that the technical debt is mounting, that the real productivity declined: this lives in a counterfactual that no one is positioned to evaluate.</p> <p>Worse: the tool's output becomes the baseline. Once you've been using AI assistance for six months, you don't remember what your unassisted work looked like. You can't compare current output to a previous version of yourself that no longer exists. The counterfactual has been erased by the intervention.</p> <p>In Akerlof's used car market, the buyer eventually learns whether they got a lemon. They drive it home, and it breaks down or it doesn't. The learning happens. In the AI platform market, the equivalent learning never happens because the tool has altered the conditions under which learning would occur.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv_1","title":"IV.","text":"<p>In a glass-walled conference room in Manhattan, Sarah is preparing to recommend an AI platform to her firm's partners. She's spent three months evaluating options: sitting through demos, running pilots, talking to references. Every vendor showed her the same thing: impressive generation, smooth interfaces, metrics that went up and to the right. The references were uniformly positive. Of course they were. No one gives a reference for a product they think failed them.</p> <p>Sarah knows something is wrong. The pilots were too short to measure real outcomes. The metrics tracked activity, not results. The references couldn't articulate what \"working\" meant except that it \"felt faster.\" But Sarah has to make a recommendation. The partners expect a decision. The firm's competitors are already using these tools, or claim to be.</p> <p>She's going to recommend the platform with the best demo. Not because she believes it's the best platform (she has no way of knowing) but because the demo is the only signal she has, and the demo was very good. The vendor understood what she needed to see. The vendor always understands what buyers need to see.</p> <p>This is not a failure of Sarah's diligence. It's the rational response to an information environment where quality signals don't exist. When you can't tell good from bad, you optimize on what you can tell: presentation quality, brand recognition, the confidence of the sales team, the sophistication of the demo. These proxies have no reliable connection to actual quality, but they're the only game in town.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#v_1","title":"V.","text":"<p>Akerlof's lemons dynamic has a stable endpoint, and it's not zero quality. His insight was that markets converge on whatever level of quality buyers can verify. If you can tell a car is a complete wreck, you won't pay for it, so complete wrecks exit the market. The market stabilizes at the lowest quality level that buyers can't detect.</p> <p>For AI platforms, this level is: \"produces output that looks plausible and feels helpful, even if it doesn't actually help.\" Platforms that clear this bar survive and grow. Platforms that fall below it (producing obviously bad output) get filtered. Platforms that exceed it (producing genuinely good output) can't charge premium prices because buyers can't detect the premium.</p> <p>This is a depressing equilibrium. The market converges on tools that feel helpful but aren't transformative: good enough to justify adoption, not good enough to deliver real value, reliable enough to avoid embarrassment but not reliable enough to trust with anything critical. The AI assistant as mediocre colleague: pleasant, competent-seeming, ultimately not adding much.</p> <p>The fog is not uniform. Some AI tools clearly work. Copilot has retention data suggesting real value. ChatGPT has 200 million users who keep returning. The question \"does this tool generate plausible code?\" is answerable. The question \"does this tool make developers more productive over twelve months?\" is foggier. The question \"does this platform justify a $29 billion valuation?\" is maximally foggy. The fog thickens as claims escalate, from utility to productivity to transformation to world-historical importance. The interesting question isn't whether any quality signal exists, but whether quality signals exist where the capital flows. Copilot might work; that tells you nothing about whether Manus justified $2 billion.</p> <p>Some readers will recognize this description. They've been using AI tools for a year and have a nagging sense that the tools are less useful than the enthusiasm suggests. They can't quite articulate what's wrong. The tools work, kind of. The output is acceptable, sort of. But the transformation that was promised hasn't materialized. Maybe they're using the tools wrong. Maybe they just need more practice. Maybe the next version will be the breakthrough.</p> <p>Or they're living in the lemons equilibrium, mistaking mediocrity for \"not yet good enough\" because they have no baseline to compare against.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#vi","title":"VI.","text":"<p>Consider a counter-narrative: perhaps the lemons dynamic is exactly what competitive markets need to function.</p> <p>Consider: if buyers could perfectly evaluate quality, there would be no competition on price. Everyone would buy the best product. The best product would capture the entire market. We'd have monopolies everywhere. Information asymmetry, on this view, is what keeps markets competitive. Buyers who can't tell good from bad spread their purchases across multiple vendors, keeping multiple vendors in business, preserving competition that eventually produces innovation.</p> <p>This is economically coherent, even if it feels perverse. Some level of buyer ignorance is systemically functional, even as it's individually costly. The market as a whole benefits from the diversity that ignorance preserves, even as individual buyers pay the cost of purchasing lemons.</p> <p>The question then becomes not \"how do we fix the market\" but \"do we want this kind of market, given what it produces?\"</p> <p>That's a political question, not a technical one. And we'll return to it.</p> <p>But notice the game structure. Every participant in the lemons market is playing finite games. The vendor plays \"close this deal.\" The buyer plays \"make a defensible recommendation.\" The reference plays \"maintain my relationship with the vendor.\" Each finite game has a winner. Each winner advances.</p> <p>The infinite game is different: a market that can learn. A market where quality signals emerge, where buyers get better at evaluation, where vendors who deliver value capture it. A market that improves its own capacity for judgment over time.</p> <p>The lemons dynamic is what happens when finite games devour the infinite game. Each participant optimizes for their immediate win. The aggregate effect destroys the conditions for sustainable competition. Everyone advances; the arena crumbles.</p> <p>Sarah, recommending the platform with the best demo, is playing rationally. She'll win her finite game (the recommendation will be made, the decision will be defensible). But she's contributing to a market that cannot learn from her participation. Her finite win subtracts from the infinite game.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#circle-three-capitals-installation-phase","title":"Circle Three: Capital's Installation Phase","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i_2","title":"I.","text":"<p>If markets have information problems, capital markets are supposed to solve them. Investors do due diligence. They verify claims. They price risk appropriately. Smart money finds quality; quality gets funded; the market corrects itself through the discipline of return-seeking.</p> <p>In practice, the capital structure of the AI platform market is making things worse.</p> <p>Carlota Perez, the economist who studies technological revolutions, distinguishes two phases: installation and deployment. During installation, financial capital dominates. Money floods into new infrastructure, creating bubbles and frenzies. Valuations detach from fundamentals because the fundamentals haven't emerged yet. Everyone is pricing options, buying variance, purchasing possibility. The dot-com boom, the railway mania, the canal speculation: all were installation-phase phenomena.</p> <p>During deployment, production capital dominates. The surviving infrastructure gets integrated into the real economy. The speculation burns off. What remains generates actual value, captured in revenue and profit rather than possibility and hope.</p> <p>Between the phases: a crash. The excesses of installation get corrected. The companies that built real infrastructure survive; the rest don't. Perez has called this transition the \"turning point\": the moment when financial capital gives way to production capital, when the evaluation criteria shift from \"what could this become?\" to \"what does this actually produce?\"</p> <p>We are deep in the installation phase for AI platforms. The $29 billion valuations, the ten-day acquisitions, the capital flooding faster than production value can absorb: all are installation-phase signatures. This observation should not be read as criticism. Installation phases look like this. The people making these bets aren't fools; they're pricing options in a market where variance is enormous.</p> <p>But installation-phase capital has a specific pathology: it rewards performance of progress over verification of progress.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii_2","title":"II.","text":"<p>In a venture capital office in San Francisco, David is preparing a term sheet for an AI agent startup. He's seen the pitch deck. The metrics are impressive: ARR growing 40% month-over-month, logos from recognizable companies, a founder who demos well. The product might work. David can't tell. The diligence would take six months, and by then some other fund will have closed the round.</p> <p>David knows the game. He's been playing it for fifteen years. You invest in the next round, not in the fundamentals. You bet on the story, because the story is what raises the next round, which is what validates your entry. The music is playing. Everyone is dancing.</p> <p>The term sheet David sends values the company at $800 million. The metrics suggest $100 million would be aggressive. But $100 million doesn't clear the round; other funds are offering higher. The price isn't about what the company is worth. It's about what other funds are willing to pay, which is about what they think still other funds will pay at the next round.</p> <p>David's fund will mark this investment up 3x in eighteen months when the Series C closes at $2.4 billion. The markup will justify the fund's returns to its LPs. The returns will justify the fund's next raise. The cycle will continue until it doesn't.</p> <p>This is not irrational. Within the game as it's structured, David is playing optimally. The returns are real. The markups are real. The fund performance is real. What's questionable is the relationship between all this reality and anything that could be called fundamental value. The game is internally coherent and externally unmoored.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii_2","title":"III.","text":"<p>Hyman Minsky, the economist who spent his career explaining why capitalist economies are inherently unstable, traced a progression that applies disturbingly well to what we're seeing.</p> <p>During stable periods, Minsky observed, financing moves through three stages:</p> <p>Hedge financing: the borrower can cover interest and principal from cash flows. This is old-fashioned, boring, sustainable financing. You lend money, the business generates revenue, the revenue covers the debt.</p> <p>Speculative financing: the borrower can cover interest but must roll over principal. This depends on continuous access to credit markets. You're fine as long as you can keep refinancing. If credit tightens, you're exposed.</p> <p>Ponzi financing: the borrower can't cover interest or principal from operations. The only way to service debt is through asset appreciation. You're betting that the thing you bought will be worth more tomorrow than you paid for it today. If appreciation stops, everything unwinds.</p> <p>Minsky's disturbing insight: stability itself creates instability. Success encourages risk-taking. Risk-taking moves the market from hedge to speculative to Ponzi. The longer the stability continues, the more leverage accumulates, the more participants depend on appreciation rather than fundamentals. Then something triggers a correction (any shock will do) and the Minsky Moment arrives.</p> <p>The AI platform market is in speculative/Ponzi territory by any honest accounting. Manus's $100 million ARR didn't justify a $2 billion acquisition by hedge-financing logic. You don't pay twenty times revenue for an eight-month-old company because you expect the cash flows to service the investment. You pay because you expect the strategic value, the option value, the synergies that the spreadsheet can't model. All bets on appreciation rather than production.</p> <p>When the underlying uncertainty compounds (perception gap plus lemons market plus unknowable exercise conditions), the financing structure doesn't correct the problem. The financing structure accelerates it. Capital flows toward the options with the best story, regardless of whether those options can be exercised.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv_2","title":"IV.","text":"<p>But the opposing view matters.</p> <p>Installation-phase financing might be exactly what technological revolutions need. The money flooding into AI platforms isn't stupid; it's functional. Someone has to fund the exploration of possibility space. Someone has to pay for the experiments that fail. Someone has to absorb the losses that make the eventual winners possible.</p> <p>The railways of the 19th century were funded by speculators who lost everything. Their capital built the tracks that transformed economies. The fiber-optic cables of the dot-com boom were laid by companies that went bankrupt. Their cables carry the internet we depend on today. The AI platform funding, for all its apparent irrationality, is building infrastructure that will matter long after the current crop of investors have marked their losses.</p> <p>On this view, the Minsky dynamic isn't a bug; it's how capitalist economies explore new technological frontiers. The installation-phase frenzy is waste in one sense and investment in another. The question isn't whether there will be a correction (there will be) but whether the correction will leave behind infrastructure that justifies the cost.</p> <p>This is Brian Eno's perspective on generative systems, applied to capital markets. You don't specify outcomes; you create conditions for emergence. The waste is productive. The failures are substrate. Something grows in the chaos that couldn't have been planned.</p> <p>I find this view genuinely persuasive, which is why I want to name its limits. The claim that installation-phase waste is functional assumes that what gets built during the frenzy has durable value. Railways and fiber-optic cables are physical infrastructure; once built, they persist. Software platforms are not physical infrastructure. They depreciate quickly, require continuous investment, and can become worthless overnight when something better emerges.</p> <p>The AI platforms funded today might not become the infrastructure of tomorrow. They might become nothing: abandoned code, dispersed teams, forgotten pitch decks. The waste might just be waste.</p> <p>We won't know which interpretation is right until we're through the transition. Living in the installation phase means the meaning of what's happening is underdetermined by the present.</p> <p>The Carse frame sharpens this. Installation-phase capital is finite-game capital. It plays for the next round, the next markup, the next exit. These are games with clear winners, and the winners are winning.</p> <p>The question is whether these finite wins serve an infinite game: the creation of durable infrastructure, the exploration of possibility space, the accumulation of capabilities that will matter after the crash.</p> <p>David, sending the term sheet at $800 million, is winning his finite game. His fund will mark up. His LPs will be satisfied. His next raise will succeed. But is he contributing to infrastructure that survives the transition? Or is he funding a performance that evaporates when the music stops?</p> <p>The fog makes this unanswerable. The finite games are visible; David can see his returns. The infinite game is invisible; no one can see whether durable value is accumulating or being consumed. You can optimize for what you can see. What you can see is the finite game.</p> <p>Minsky's cycle is the collective result. Everyone winning finite games, no one tracking the infinite game, the system accumulating fragility until something breaks.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#circle-four-the-political-economy-of-categories","title":"Circle Four: The Political Economy of Categories","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i_3","title":"I.","text":"<p>So far, we've moved from individual phenomenology (the developer in the fog) through market dynamics (the lemons problem) to capital structures (installation-phase financing). Each circle has revealed how the fog is produced and reproduced at different levels of scale. But there's a dimension we haven't examined: politics.</p> <p>The fog is a political arena, not only an epistemological condition. Who gets to define what \"working\" means? Who sets the standards for \"enterprise-ready\"? Who determines when an AI platform has crossed from experimental to reliable?</p> <p>These are not technical questions. They're questions about power.</p> <p>Mary Douglas, the anthropologist who spent her career studying how societies create categories, would see immediately what's at stake. In Purity and Danger, she showed that what counts as \"clean\" or \"dirty,\" \"safe\" or \"dangerous,\" is not an objective property of things. It's a social construction. Communities draw boundaries. Institutions enforce categories. What counts as pollution in one culture is sacred in another. The distinction isn't discovered; it's produced, and producing it is an exercise of power.</p> <p>\"Enterprise-ready AI\" is this kind of category. It doesn't name a natural kind, like \"hydrogen\" or \"mammal.\" It names a social boundary that institutions are currently fighting to draw. The fight matters because whoever wins gets to define the game everyone else plays.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii_3","title":"II.","text":"<p>Consider the stakeholders and what they want.</p> <p>Vendors want broad categories. If \"enterprise-ready\" means \"has SSO and an SLA,\" then every vendor is enterprise-ready. The term becomes marketing, applied wherever it helps close deals. The fog serves vendors; it lets them make claims without accountability.</p> <p>Regulators want narrow categories. If \"enterprise-ready\" means \"approved for specific use cases after extensive review,\" then regulators control the gate. Every new application requires their blessing. The fog alarms regulators; they want to disperse it through standards and oversight.</p> <p>Enterprise buyers want verifiable categories. If \"enterprise-ready\" means \"can prove its claims through independent evaluation,\" then buyers can make informed decisions. They want the fog lifted so they can see what they're purchasing.</p> <p>AI safety researchers want cautious categories. If \"enterprise-ready\" means \"demonstrated safe under adversarial conditions,\" then most platforms don't qualify. They want the fog treated as dangerous until proven otherwise.</p> <p>Each stakeholder has a different relationship to uncertainty, and each would benefit from a different resolution of the category contest. The vendors are winning so far. \"Enterprise-ready\" means whatever vendors say it means, and buyers lack the tools to challenge the claim. But this could change.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii_3","title":"III.","text":"<p>Category battles have clocks tied to events.</p> <p>A major AI platform failure, something dramatic enough to attract regulatory attention, could shift the contest overnight. Suddenly \"enterprise-ready\" becomes a compliance category with legal weight. The vendors who anticipated regulation (building audit trails, human oversight, certification infrastructure) have structural advantage. The vendors who didn't scramble to retrofit, or die.</p> <p>Alternatively, industry self-regulation could preempt government action. The major players agree on standards, create certification bodies, define what \"enterprise-ready\" means before regulators do. This happened in other industries: accounting standards, credit ratings, organic food certification. It creates a different power dynamic: the incumbents who wrote the standards advantage themselves.</p> <p>Or the category fragments. Instead of one definition of \"enterprise-ready,\" each vertical develops its own. \"Healthcare AI\" gets defined by healthcare regulators. \"Financial AI\" gets defined by financial regulators. The general-purpose category dissolves into specialized niches, each with its own rules.</p> <p>The point is not to predict which outcome occurs (prediction is exactly what the fog prevents) but to recognize that the outcome is underdetermined by technology. The same AI capabilities could live in radically different category structures depending on how the political contest resolves. And the stakes are high: the winning definition shapes which companies thrive, which business models work, what kinds of AI get built and deployed.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv_3","title":"IV.","text":"<p>Douglas's framework helps surface a deeper politics.</p> <p>Categories don't only sort things that already exist. They call things into being. Once \"organic food\" became a category, farmers started producing for the category. Once \"enterprise software\" became a category, companies started building for the category. The category shapes what gets created, not only what gets classified.</p> <p>If \"enterprise-ready AI\" gets defined as \"deterministic, auditable, controlled,\" the platforms that survive will be deterministic, auditable, controlled. The more generative, emergent, surprising possibilities of AI (the ones that might be most valuable but are least predictable) will be selected against. The category will produce a certain kind of AI by excluding alternatives.</p> <p>This is James C. Scott's critique of legibility, applied to the AI market. In Seeing Like a State, Scott documented how high-modernist projects (planned cities, scientific forestry, collectivized agriculture) failed because they optimized for what central planners could see and measure, destroying the tacit knowledge and local adaptation that made systems actually work.</p> <p>The push to make AI \"enterprise-ready\" might be the same kind of legibility project. The aspects of AI that can be standardized, measured, and certified are not necessarily the aspects that create value. The pressure to make AI legible to institutions might kill exactly what makes it useful.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#v_2","title":"V.","text":"<p>The category battle connects back to the lemons problem in a way worth making explicit.</p> <p>Akerlof's lemons market assumes buyers can't verify quality. But what counts as quality is itself defined by categories. If the category for AI platforms emphasizes \"feels helpful\" (vendor preference), the lemons market stabilizes around platforms that feel helpful. If the category emphasizes \"verifiably improves outcomes\" (buyer preference), the market stabilizes around platforms that can prove improvement. If the category emphasizes \"poses no unacceptable risks\" (regulator preference), the market stabilizes around platforms that can demonstrate safety.</p> <p>The lemons problem doesn't have a fixed endpoint. It has multiple possible endpoints, and which one we reach depends on political contests over categorical definition.</p> <p>This is the political economy of fog: the fog isn't just there, naturally occurring, waiting to be measured or dispersed. The fog is produced by conflicting interests, maintained by power imbalances, and shaped by categorical battles that determine what would even count as clarity.</p> <p>And each stakeholder is playing finite games within the fog. The vendor plays \"capture this category.\" The regulator plays \"assert jurisdiction.\" The buyer plays \"demonstrate due diligence.\" Each game has tactical winners.</p> <p>The infinite game (categories that actually track quality, standards that enable market learning, definitions that help rather than obscure) requires coordination that finite-game competition prevents. Everyone is so busy winning their categorical battle that no one is building categories worth fighting over.</p> <p>The deepest political question isn't who wins the category contest. It's whether anyone is playing for categories that serve the infinite game of market learning, or whether the contest itself is consuming the possibility of categories that work.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#circle-five-the-achievement-subject-all-the-way-down","title":"Circle Five: The Achievement-Subject All the Way Down","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i_4","title":"I.","text":"<p>We've moved from individual phenomenology through market dynamics through capital structures through political economy. Each circle has revealed the fog at a different level of scale. But there's one more level, the deepest and most uncomfortable: what the fog does to the people in it.</p> <p>Byung-Chul Han, the German-Korean philosopher, describes the contemporary condition as a shift from discipline to achievement. In the disciplinary society Foucault described, external authorities told you what to do and punished deviation. The factory whistle. The prison guard. The school principal. Power was visible, localized, exercised from outside.</p> <p>In the achievement society, Han argues, constraint has moved inside. You tell yourself what to do. You punish yourself for falling short. The achievement-subject is both exploiter and exploited, both the demanding boss and the overworked employee. There's no one else to blame because there's no one else. The discipline is self-generated.</p> <p>Marcus, the developer in the productivity spiral, isn't being exploited by his AI assistant. He's exploiting himself through the AI assistant. He chases velocity because he's internalized the imperative to chase velocity. The spiral isn't imposed from outside; it's self-generated. The AI is just the tool that lets him exploit himself more efficiently.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii_4","title":"II.","text":"<p>This framing explains why knowing about the perception gap doesn't help.</p> <p>You can show Marcus the METR study. You can explain the 39-percentage-point divergence between perceived and actual performance. You can demonstrate, with data, that his felt productivity is disconnected from his real productivity. He might even believe you. But his behavior won't change, because the behavior isn't driven by belief about productivity. It's driven by the internalized imperative to achieve.</p> <p>The achievement-subject doesn't optimize for outcomes; they optimize for the feeling of achievement. The feeling is the goal. The velocity metric, the completed tickets, the flowing code: these are not instrumental to some further end. They are the end. Producing the feeling of progress is what the achievement-subject is for.</p> <p>This is why the AI assistant fits so perfectly. It produces the feeling of progress with maximum efficiency. Every prompt generates code. Every generation feels like accomplishment. The interface is optimized for achievement-feeling, and the achievement-subject is optimized for consuming achievement-feeling. The match is perfect.</p> <p>The perception gap, from this perspective, isn't a problem to be solved. It's a feature of how achievement-subjects relate to their work. Closing the gap would require caring about something other than the feeling of achievement: actual outcomes, long-term quality, the sustainable cultivation of skill. But caring about those things is precisely what the achievement-subject has been conditioned not to do.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii_4","title":"III.","text":"<p>The same analysis applies at every level we've examined.</p> <p>David the investor isn't being exploited by the Minsky cycle. He's exploiting himself through the Minsky cycle. He chases returns because he's internalized the imperative to chase returns. The markup, the next fund, the portfolio performance: these aren't instrumental to some further end. They're the end. The investor is an achievement-subject optimizing for investor-achievement-feeling.</p> <p>Sarah the buyer isn't being deceived by the lemons market. She's participating in a game whose rules she's internalized. The recommendation, the decision, the appearance of due diligence: these produce the feeling of having done her job. Whether the platform works is less important than having selected a platform defensibly. The buyer is an achievement-subject optimizing for buyer-achievement-feeling.</p> <p>The vendors, the regulators, the researchers: all are achievement-subjects optimizing for their respective achievement-feelings. No one is forcing the ten-day acquisitions. No one is forcing the inflated valuations. No one is forcing the confident demos that prove nothing. Everyone is doing it to themselves because they've internalized the games that make these behaviors feel like achievement.</p> <p>Carse would recognize this immediately. Achievement-subjectivity is finite-game subjectivity. The achievement-subject can only see finite games: the next win, the next feeling of progress, the next dopamine hit of completion. The infinite game is structurally invisible because it doesn't produce achievement-feeling. Continuation doesn't feel like victory. Optionality doesn't feel like winning.</p> <p>Marcus sprints because sprinting feels like achievement. The infinite game (sustainable skill development, the capacity to evaluate code, judgment that compounds over years) doesn't feel like anything in the moment. It's what you have when you're not optimizing for feeling. It's what you lose when you are.</p> <p>This is why the fog is so hard to escape. The finite games are phenomenologically vivid. They produce feelings. The infinite game is phenomenologically silent. It doesn't produce feelings; it produces capacity. Achievement-subjects are structurally tuned to what produces feelings. The infinite game is invisible to them by design.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv_4","title":"IV.","text":"<p>Bateson's schismogenesis reaches its deepest register here.</p> <p>The escalating spirals we've examined (productivity spirals, lemons spirals, Minsky cycles) aren't imposed by some external structure. They're the collective product of achievement-subjects competing to achieve. Each participant creates the conditions for others' participation. The developer's enthusiasm becomes the buyer's testimonial. The buyer's purchase becomes the vendor's revenue. The vendor's revenue becomes the investor's return. The investor's return becomes the next company's funding. Each achievement creates the conditions for the next achievement, and the system accelerates because each participant is optimizing for their own achievement-feeling.</p> <p>The system has no outside. Every participant is inside the achievement-subjectivity that produces and reproduces the fog. Even the critics, even this essay, are achievement-activities. I'm optimizing for the feeling of having understood something. You're optimizing for the feeling of having read something insightful. We're all inside the structure we're examining.</p> <p>This is why Bateson was pessimistic about escaping schismogenesis. You can't solve a double bind by trying harder within it. You'd need to step to a different level where the bind doesn't operate. But where would that level be? Where do you stand outside a market you're participating in? Where do you stand outside an achievement-subjectivity you've internalized?</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#v_3","title":"V.","text":"<p>Simone Weil would say: contemplation.</p> <p>Not thinking about things, not analyzing or critiquing. Something closer to waiting. Attention without agenda, receptivity without production. Time that isn't productive and space that isn't optimized. The willingness to be present without needing to achieve.</p> <p>This sounds like mystical nonsense in a business context. But notice what Weil is pointing at: a mode of being that isn't achievement-subjectivity. A way of relating to work that doesn't optimize for the feeling of progress. A capacity to be present without generating, to wait without filling the space.</p> <p>AI assistance, whatever else it does, forecloses this possibility. It fills space. It generates before you've waited. It provides achievement-feeling on demand, which means you never have to sit with the discomfort that precedes understanding.</p> <p>The capacity for contemplation is upstream of everything else: the capacity to evaluate outcomes, to recognize quality, to know when something is working and when it isn't. Destroying contemplation destroys the foundation on which rational markets depend.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#vi_1","title":"VI.","text":"<p>The uncomfortable conclusion: market corrections don't fix this.</p> <p>The Minsky moment will come. The installation phase will crash into deployment. The lemons market will shake out. Valuations will compress. Companies will fail. Money will be lost.</p> <p>And then the same people will rebuild the same structures.</p> <p>They'll rebuild them because the problem isn't the structures. The problem is what they've become. Achievement-subjects produce achievement-subject conditions. The fog isn't something that descended on the market; the fog is what achievement-subjects generate when they compete to achieve. Clear the fog and they'll generate more fog, because fog-generation is what they do.</p> <p>This is how markets under capitalism work. Achievement-subjectivity is functional for capital accumulation. The installation-phase frenzies fund exploration, the crashes clear deadwood, the rebuilding creates new opportunities. The whole thing lurches forward, generating more output than any alternative system has managed, at the cost of producing subjects who cannot stop generating.</p> <p>Whether this is good or bad depends on what you think humans are for. Economics can't answer that question.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#the-spiral-back-two-ways-through-the-fog","title":"The Spiral Back: Two Ways Through the Fog","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i_5","title":"I.","text":"<p>Elena may not exist yet, not as a single person anyway. She's the archetype of what would have to exist for the market to correct itself.</p> <p>Elena builds an AI agent platform. She's different from most founders in the space. Fifteen years of enterprise software, regulated industries, tools that had to work and could be audited. She watched the AI platform market with fascination and horror. The demos were impressive. The adoption was real. The valuations were insane. And no one, as far as she could tell, was building the infrastructure to verify whether any of this actually worked.</p> <p>So she built it. Her platform publishes failure rates.</p> <p>Not hidden in documentation. Not buried in settings. On the main dashboard, in real time: \"This platform succeeds 85% of the time on task type X. 60% on task type Y. We don't recommend it for task type Z.\"</p> <p>Her investors hate it. Her advisors think she's committing commercial suicide. Every competitor hides their failures; why would she advertise hers? The sales conversations are harder. Prospects see the 60% number and flinch. Some walk. Her ARR is a fraction of what her competitors claim.</p> <p>But something else is happening too.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii_5","title":"II.","text":"<p>Read Elena through each circle.</p> <p>Through Circle One (the developer in the fog). Elena is trying to break the perception gap. By publishing failure rates, she gives users a baseline to evaluate against. \"I expected 85% success and I'm experiencing 70%\u2014something's wrong\" is a different mental state than \"I feel productive and therefore I must be productive.\" She's trying to create the conditions for attention that Weil describes, the capacity to see what's actually happening rather than what the interface wants you to feel.</p> <p>Through Circle Two (the lemons market). Elena is betting that quality signals can exist. Her failure rates are an attempt at credibility, a costly signal that only confident vendors can afford. Advertising your failures is commercial suicide unless your failures are better than competitors' hidden failures. She's hoping that buyers will eventually learn to read the signal, that the lemons dynamic isn't terminal.</p> <p>Through Circle Three (installation-phase capital). Elena is positioning for the crash. She's raised $12 million while competitors have raised $400 million, which means she needs less capital to survive, which means she can weather a funding winter that would kill the Ponzi-financed competition. She's betting that Perez's turning point will come, and that she'll be standing when it does.</p> <p>Through Circle Four (category politics). Elena is trying to shape the category. By defining success rates by task type, she's creating the taxonomy that could become the standard. If \"enterprise-ready\" comes to mean \"publishes audited failure rates,\" she wins. She's not waiting for regulators or industry bodies to define the category; she's defining it herself, hoping others will follow.</p> <p>Through Circle Five (achievement-subjectivity). And here's where it gets complicated. Is Elena outside the achievement-subjectivity? Or is she an achievement-subject achieving differently?</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii_5","title":"III.","text":"<p>The honest answer: both.</p> <p>Elena is still optimizing for achievement-feeling. She's betting, fundraising, competing, trying to win. The fact that her strategy is different doesn't mean she's escaped the game. She's playing for different stakes (reputation for quality instead of revenue growth, survival through crash instead of growth until crash) but she's still playing.</p> <p>At the same time, there's something in Elena's approach that gestures toward an alternative. Publishing failure rates requires a certain willingness to not achieve. It means accepting that your numbers will look worse than competitors' numbers. It means giving prospects reasons not to buy. It means sitting with the discomfort of visible limitation rather than performing confidence.</p> <p>This is closer to Weil's contemplation than it might appear. Not full contemplation; Elena is still a founder, still competing, still optimizing. But there's a seed of something else: the willingness to let reality be visible rather than managing perception. The willingness to wait for buyers who can see rather than performing for buyers who can't.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv_5","title":"IV.","text":"<p>Elena understands which game she's playing.</p> <p>She's playing finite games. Every founder does. The fundraise, the sale, the feature ship. These have winners and losers. She wants to win them.</p> <p>But she hasn't mistaken them for the point. The point is staying in the arena. The point is building the capacity to keep building. The point is the infinite game.</p> <p>Her finite games are selected for their service to the infinite game. Publishing failure rates loses finite games: deals walk, investors flinch, competitors mock. But it builds something that compounds: credibility, infrastructure for verification, a category position that strengthens with time.</p> <p>Most founders do the opposite. They win finite games that consume the infinite game. They juice the metrics, perform the demo, close the round, and each win leaves them more dependent on the next win, more fragile, less capable of surviving the crash.</p> <p>Elena is betting that finite-game maximizers will burn out before the fog lifts. She's betting on continuation.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#v_4","title":"V.","text":"<p>But Elena isn't the only archetype. There's another path through the fog, and it looks nothing like hers.</p> <p>In early 2025, a Singapore-based startup called Manus launched what they called \"the first general-purpose agent.\" Eight months later, Meta acquired them for north of $2 billion. The speed was disorienting (term sheet to close in ten days) but what's more interesting than the outcome is the path that led to it.</p> <p>Manus didn't win because the product was perfect. Early users on Chinese social media described an unstable product, burned credits, slow support. The criticism was real. And yet the acquisition happened anyway, at a price that made no sense by any conventional metric.</p> <p>What Manus understood was the structure of the game they were playing.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#vi_2","title":"VI.","text":"<p>The Manus playbook moved through four phases, each a finite game deliberately sequenced to serve infinite positioning.</p> <p>Phase Zero: Timing and framing. Manus didn't invent agents, but they went public at the precise moment when agents were emerging from technical demos into market consciousness. The category \"general-purpose agent\" didn't exist until they named it. They positioned slightly ahead of consensus: not so far ahead that the market couldn't follow, not so aligned with consensus that they'd be lost in the crowd. The timing wasn't controllable, but the framing was. They chose a launch format that matched the current norm while naming a future step.</p> <p>Phase One: Scarcity as time-buying. At launch, Manus didn't try to prove product superiority. They controlled who could judge it. Invite-only access wasn't about manufactured hype; it was about delaying mass criticism while curiosity built. On Reddit, the dominant emotion wasn't rejection but desire\u2014\"I want access\" rather than \"this doesn't work.\" Scarcity reframed early users as participants in something exclusive. While people were trying to get in, they weren't yet dissecting flaws. The finite game was access control; the infinite game was buying time for the product to mature.</p> <p>Phase Two: Public trust through presence. When issues surfaced (credits burning too fast, pricing complaints, refund delays), Manus didn't retreat into private support channels. They showed up publicly on Reddit, replying directly inside complaint threads. The frustration was real, but so was the team's visibility. Global users tolerate imperfect products; what they don't tolerate is silence. Each public reply became a trust asset, a signal that the team was alive and accountable. The finite game was resolving individual complaints; the infinite game was building the credibility that would matter when larger players came looking.</p> <p>Phase Three: Product currency as proof engine. Manus treated credits (the currency users spent to run agents) as a marketing budget, not just a cost center. Instead of traditional advertising, they rewarded users with credits for sharing detailed use cases. The campaigns were tightly aligned with AI economics: agent products are compute-hungry, credits let users explore deeply, and credits-for-stories generated exactly the credible, specific case studies that a lemons market cannot otherwise produce. By December, Reddit was full of long, concrete user stories: self-updating landing pages that signaled real adoption to partners and acquirers evaluating the company.</p> <p>The sequence matters. Scarcity only works after positioning is set. Public trust only matters after scarcity has created an audience. The proof engine only generates credible stories after trust has established that the stories are worth reading. Each phase is a finite game with clear tactics; the sequence is an infinite-game strategy.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#vii","title":"VII.","text":"<p>Read Manus through the same circles we read Elena.</p> <p>Through Circle One (the developer in the fog). Manus didn't try to close the perception gap; they delayed it. By controlling access, they ensured that early users were self-selected enthusiasts more likely to attribute problems to their own inexperience than to the product. The fog served them. They bought time for the product to improve before mass judgment crystallized.</p> <p>Through Circle Two (the lemons market). Manus's credits-for-stories program was a direct response to the quality-signal problem. In a market where testimonials prove nothing because everyone caught in the perception gap gives glowing reviews, Manus generated a different kind of signal: detailed, specific use cases with enough technical depth to be evaluated. Not \"I love this product\" but \"I built this application, here's how it worked, here's where it failed.\" The finite game was user-generated content; the infinite game was creating quality signals the market couldn't otherwise produce.</p> <p>Through Circle Three (installation-phase capital). Manus understood they were an option, not an answer. Their GTM wasn't designed to prove the product worked at scale (that question remains unanswerable). It was designed to position them as the option worth buying. The narrative, the timing, the credibility infrastructure: all of it made Manus legible to acquirers as a strategic asset. Meta didn't buy proof that general-purpose agents work; they bought the option on finding out, packaged in a company that had demonstrated it could navigate the fog.</p> <p>Through Circle Four (category politics). Manus named the category before anyone else could. \"General-purpose agent\" is now a thing because Manus said it was, at a moment when the market was ready to hear it. They didn't wait for industry bodies or regulators to define the category; they defined it themselves, then let competitors position relative to their frame. The finite game was the launch; the infinite game was category ownership.</p> <p>Through Circle Five (achievement-subjectivity). The Manus founders are achievement-subjects like everyone else. They optimized, competed, won. But their GTM reveals something most achievement-subjects lack: awareness of the game's structure. They didn't just chase the next win; they sequenced wins to serve positioning. They understood that Phase One enables Phase Two enables Phase Three, that the sequence compounds in ways that isolated wins don't.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#viii","title":"VIII.","text":"<p>Elena and Manus represent two different paths through the same fog.</p> <p>Elena's strategy is transparency. She publishes failure rates, accepts finite-game losses, and positions for a future where verification matters. She's betting that the fog will lift, that buyers will eventually learn to read quality signals, that her credibility infrastructure will compound while competitors' Ponzi structures collapse. Her finite games sacrifice short-term wins for infinite positioning.</p> <p>Manus's strategy is timing. They control when judgment happens, sequence finite wins to build narrative momentum, and position for a future where strategic value matters more than product proof. They're betting that acquirers will pay for options before the fog lifts, that credibility can be manufactured through structure rather than transparency, that the right sequence of wins creates exit velocity before verification becomes necessary. Their finite games accumulate into positioning that's valuable regardless of whether the underlying questions ever get answered.</p> <p>Both strategies work. Both navigate the fog rather than trying to dispel it. Neither requires the fog to lift for the strategy to succeed.</p> <p>The difference isn't that one is honest and the other isn't (both involve genuine capability and real execution). The difference is what they're optimizing for. Elena optimizes for being right when the fog clears. Manus optimizes for being positioned before it does. Elena bets on verification. Manus bets on velocity.</p> <p>And both understand which game they're playing.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ix","title":"IX.","text":"<p>What separates Elena and Manus from Marcus and Sarah and David isn't success; all of them are winning, by some measure. What separates them is awareness.</p> <p>Marcus wins sprints while losing skill. Sarah makes defensible recommendations while contributing to a market that cannot learn. David marks up portfolios while feeding a Minsky cycle. Each wins finite games that consume infinite positioning. They optimize for the feeling of winning without tracking what the winning costs.</p> <p>Elena and Manus also win finite games. But their wins are selected for service to something larger. Elena's losses purchase credibility that compounds. Manus's wins sequence into positioning that attracts strategic capital. Neither is playing for the next quarter. Both are playing for continuation.</p> <p>The fog makes this distinction invisible to most participants. The finite games are vivid: you can see the closed deal, the marked-up investment, the shipped feature, the completed sprint. The infinite game is silent. It doesn't produce feelings; it produces capacity. Achievement-subjects, tuned to what produces feelings, can't perceive it.</p> <p>Elena and Manus perceive it because they've built their strategies around it. Their finite-game tactics only make sense if you understand the infinite game they serve. Publishing failure rates is commercial suicide unless you're playing for credibility that compounds. Scarcity at launch is artificial constraint unless you're playing for judgment timing. The tactics reveal the strategy; the strategy reveals the game awareness.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#coda-the-game-youre-actually-playing","title":"Coda: The Game You're Actually Playing","text":"","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#i_6","title":"I.","text":"<p>Return to Meta's conference room, ten days after the term sheet arrived.</p> <p>Someone is writing the integration plan. Call her Ana. Manus's technology will merge with Meta's existing AI infrastructure. The 147 trillion tokens processed, the 80 million virtual computers created, the $100 million ARR: all of this will be absorbed into an organization two thousand times larger.</p> <p>Ana can't answer the question that justified the acquisition: will general-purpose AI agents work at scale? The question is unanswerable from inside the fog. Meta bought an option, not an answer.</p> <p>But Ana might notice something else. Manus navigated to this conference room through a specific sequence of moves: positioning before the category crystallized, scarcity while the product matured, public presence when trust mattered, proof generation when credibility needed to compound. Each move was a finite game. The sequence was an infinite-game strategy. Manus understood which game they were playing.</p> <p>Did Meta?</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#ii_6","title":"II.","text":"<p>The finite game is clear: acquire Manus, integrate the technology, ship features, capture market share, win this round of the AI platform competition. Clear objectives, measurable outcomes, definite winners.</p> <p>The infinite game is different: maintain the capacity to keep playing. Preserve optionality. Build infrastructure that survives the crash. Develop judgment about what works. Stay in the arena long enough for the fog to lift.</p> <p>The fog obscures the relationship between these games. You can win finite games that serve infinite positioning: Elena publishing failure rates that compound into credibility, Manus sequencing moves that compound into strategic value. Or you can win finite games that consume infinite positioning: Marcus sprinting away his skill, Sarah recommending platforms she can't evaluate, David marking up portfolios in a Minsky cycle.</p> <p>From inside the fog, both kinds of winning feel the same. The metrics rise, the deals close, the sprints complete. The difference only becomes visible later, when the fog lifts or the crash comes, when it turns out that some wins were building something and others were borrowing against it.</p> <p>Ana, writing the integration plan, is playing a finite game. She has to; that's what she was hired to do. But somewhere in Meta's organization, someone should be asking whether their finite games serve their infinite game or consume it. Manus knew. Does Meta?</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iii_6","title":"III.","text":"<p>This is what the fog costs: the inability to know which game you're actually playing.</p> <p>Every participant in the AI platform market is winning finite games. The developers are closing tickets. The buyers are making recommendations. The investors are marking up portfolios. The vendors are shipping features. The metrics all point up and to the right.</p> <p>And no one knows whether these wins are accumulating into something durable or consuming the conditions for durability. No one knows whether the finite games serve the infinite game or sacrifice it. No one knows because the fog is precisely the condition of not-knowing, and the fog is produced by the same dynamics that produce the finite wins.</p> <p>The perception gap means developers can't tell if their productivity serves long-term skill development. The lemons market means buyers can't tell if their purchases serve market learning. The Minsky dynamic means investors can't tell if their returns serve durable value creation. The achievement-subjectivity means no one can tell if their wins serve anything beyond the feeling of winning.</p> <p>This is not a temporary condition that better measurement will solve. The fog is structural. It's produced by the same forces that produce the market. Disperse it in one place and it regenerates in another.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#iv_6","title":"IV.","text":"<p>So what do you do?</p> <p>Start by recognizing which game you're actually playing. Most people think they're playing the infinite game while optimizing for next quarter. They talk about \"long-term value\" while chasing metrics that sacrifice sustainability. Be honest: which game are your actual decisions serving?</p> <p>Then notice when finite games start consuming infinite games. The developer who stops debugging because velocity is high. The buyer who stops evaluating because decisions are defensible. The investor who stops questioning because markups are strong. These are the moments when finite wins eat infinite optionality.</p> <p>Find the people who understand the game structure. Elena, publishing failure rates, betting on verification. Manus, sequencing finite wins, betting on positioning. Their strategies are different; their awareness is the same. Both have built their tactics around an infinite game that most participants can't perceive. Study their moves. The tactics only make sense if you see the larger game they serve.</p> <p>Cultivate what the fog destroys: attention, patience, the willingness to wait without generating, the ability to evaluate quality you didn't produce. These capacities atrophy under AI assistance and achievement pressure. They're also what you need to perceive the infinite game at all.</p> <p>And remember that the fog is political. The categories aren't natural; they're contested. \"Enterprise-ready\" means what the winners of the category battle say it means. Manus won that battle early by naming \"general-purpose agent\" before anyone else could. You can participate in that contest. You can shape what \"working\" means. You don't have to accept the definitions that serve the fog.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#v_5","title":"V.","text":"<p>The sprint is not the point. The deal is not the point. The markup is not the point.</p> <p>These are finite games you play to stay in the arena. The point is staying there, preserving the capacity to keep playing.</p> <p>The fog makes this hard to see. Everyone around you is optimizing for finite wins. The metrics reward finite wins. The interfaces produce the feeling of finite wins. The pressure is always toward the next win, the next quarter, the next round.</p> <p>The people who will matter when the fog lifts are the ones who never forgot which game they were playing.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#vi_3","title":"VI.","text":"<p>Ana finishes the integration plan.</p> <p>Somewhere in Singapore, the Manus founders prepare for their new reality, the reality their four-phase playbook was designed to reach. Somewhere in San Francisco, Marcus pushes another AI-generated commit, winning a sprint he'll pay for later. Somewhere in Manhattan, Sarah's recommendation lands on partners' desks, defensible but not verified. Somewhere in a seed-stage office, Elena publishes another failure rate, losing a deal that would have cost her more than it paid.</p> <p>Each is playing finite games. Each is winning, by some measure.</p> <p>What separates them is whether they know which game they're actually playing. Manus knew. They sequenced their finite wins deliberately, built narrative momentum that served strategic positioning, navigated to an exit that validated the sequence. Elena knows. She accepts finite losses that purchase infinite credibility, builds verification infrastructure while competitors build castles on sand.</p> <p>Marcus doesn't know. Sarah doesn't know. David doesn't know. They're winning finite games while the infinite game slips away, and the fog prevents them from seeing the difference.</p> <p>Ten days was plenty of time to close an acquisition. It was not enough time to know whether the acquisition served Meta's infinite game or consumed it.</p> <p>Knowing takes patience, attention, the willingness to step back from the finite game long enough to see the larger structure. There's more than one path through the fog: Elena's transparency, Manus's timing, and probably others we haven't seen yet. But all the paths start with the same awareness: the sprint is not the point.</p> <p>Most people never figure it out. The fog is designed to prevent it.</p> <p>Elena figured it out. Manus figured it out.</p>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/01/the-political-economy-of-fog/#sources","title":"Sources","text":"<ul> <li>Akerlof, George. \"The Market for 'Lemons': Quality Uncertainty and the Market Mechanism\" (1970)</li> <li>Bateson, Gregory. Steps to an Ecology of Mind (University of Chicago Press, 1972)</li> <li>Carse, James P. Finite and Infinite Games (Free Press, 1986)</li> <li>Douglas, Mary. Purity and Danger (Routledge, 1966)</li> <li>Han, Byung-Chul. The Burnout Society (Stanford University Press, 2015)</li> <li>Kahneman, Daniel. Thinking, Fast and Slow (Farrar, Straus and Giroux, 2011)</li> <li>METR. \"Measuring the Impact of Early Exposure to AI on Experienced Open-Source Developer Productivity\" (2025)</li> <li>Minsky, Hyman. Stabilizing an Unstable Economy (Yale University Press, 1986)</li> <li>Perez, Carlota. Technological Revolutions and Financial Capital (Edward Elgar, 2002)</li> <li>Scott, James C. Seeing Like a State (Yale University Press, 1998)</li> <li>Weil, Simone. Gravity and Grace (Routledge, 1952)</li> <li>Bloomberg: \"Meta acquires startup Manus to bolster AI business\" (December 29, 2025)</li> <li>Choudhary, Sangeet Paul. \"A healthy infinite game optimizes for continuation with optionality\" (LinkedIn, January 1, 2026)</li> <li>Keec, Coni. \"The Fog of Code\" (Substack)</li> <li>Mao, Jaye. \"When 'This Product Is Dead' Turns Into a $2B Exit\" (AI Product GTM, 2026)</li> </ul>","tags":["ai-platforms","political-economy","options-theory","uncertainty","phenomenology"]},{"location":"blog/2026/01/05/impossible-algebra/","title":"Impossible Algebra","text":"<p>Fifty pull requests per week requires more hours than exist. Ten minutes each\u2014and that's generous, assuming no review, no debugging, no context-switching\u2014yields over eight hours of uninterrupted production daily. The number doesn't stretch toward difficult. It breaks arithmetic entirely.</p> <p>This is the first axiom: the target must be impossible under current assumptions. Not ambitious. Impossible. If you can imagine reaching it by working harder, it isn't impossible enough.</p> <p>The second axiom follows from the first: impossible targets don't yield to effort. They yield to transformation. The developer who achieves fifty PRs weekly isn't a faster version of the developer who achieves five. They're a different kind of thing\u2014an orchestrator running parallel agents, not a craftsperson at a single terminal.</p> <p>What looks like productivity is actually ontology. The algebra that makes impossible numbers possible isn't about efficiency. It's about becoming.</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#strategy-as-equation","title":"Strategy as Equation","text":"<p>Most productivity frameworks produce narratives. \"Work smarter, not harder.\" \"Deep work over shallow work.\" \"Focus on what matters.\" These are stories you can believe in, but they're hard to stress-test. When they fail, you can only conclude you didn't believe hard enough.</p> <p>What if productivity were an equation instead of a story?</p> <p>Equations permit operations that narratives don't. You can decompose them into terms. You can identify which variable is zero. You can compare two equations and see where they differ. You can ask \"what changes if I double this input?\" and get a structural answer rather than a motivational one.</p> <p>The AMP Framework is an algebra for impossible productivity. Three bases\u2014Anchor, Multiply, Pilot\u2014each configured by parameters that shape how they apply. The framework doesn't tell you what to do. It makes explicit what has to change for impossible output to become natural.</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#the-equation","title":"The Equation","text":"<p>Here it is, explicitly:</p> <pre><code>Output = A(n) \u00d7 M(m) \u00d7 P(i)\n</code></pre> <p>Where: - A(n) = Anchor configured with impossible number n - M(m) = Multiply via method m (agents, delegation, automation, teams) - P(i) = Pilot with instruments i (quality gates, direction, coherence, velocity)</p> <p>The multiplication isn't arithmetic\u2014you don't literally multiply numbers. It's compositional: each element requires the others to function. An Anchor without Multiplication is frustration. Multiplication without Piloting is chaos. The \u00d7 symbol captures this interdependence.</p> <p>A concrete instance: Boris Cherny created Claude Code, Anthropic's AI coding assistant. In early 2026, he shared his workflow publicly\u2014and the numbers broke arithmetic. Fifty to one hundred pull requests per week. Not by typing faster. By configuring the equation differently.</p> <pre><code>A(50-100 PRs/week) \u00d7 M(parallel agents) \u00d7 P(verification gates)\n</code></pre> <ul> <li>A(50-100): Breaks the assumption that good code requires his hands on keyboard. This number is impossible for a developer writing code manually.</li> <li>M(parallel agents): 5 terminal sessions + 5-10 web sessions running simultaneously, each with a separate git checkout, each scoped to a single task. Not Boris cloned fifteen times\u2014fifteen different lines of productive becoming.</li> <li>P(verification gates): Plan mode iteration before execution, typecheck \u2192 lint \u2192 test loop after every change, PostToolUse hook for formatting. The pilot function built into the workflow itself.</li> </ul> <p>The equation makes diagnosis possible. When Boris's output drops, which term weakened? Did the anchor drift toward achievable? Did multiplication bottleneck on context-switching overhead? Did verification loops break?</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#a-for-anchor","title":"A for Anchor","text":"<p>Set an impossible number.</p> <p>Not ambitious. Impossible. The test: can you imagine achieving it by working harder? If yes, it's not impossible enough. The impossible number must break your current model of how work gets done.</p> <p>This is David Brin's insight from Kiln People\u2014the uncomfortable observation that if you truly needed twenty versions of yourself working in parallel, you wouldn't have twenty of yourself available. You are singular. You are the constraint. You are what would have to die\u2014in the sense of \"stop being the one who does the work\"\u2014for the multiplication to happen.</p> <p>The anchor doesn't set a goal. It creates a paradox. Gregory Bateson called this structure a double bind: a situation where you're punished for any response within the current frame. You can't work harder (exhaustion). You can't work smarter at the same level (arithmetic). The only escape is to jump frames entirely\u2014to change what kind of thing you are.</p> <p>Fifty PRs per week is a double bind. It says: stop being a developer who writes PRs. Become an orchestrator who runs a system that produces them.</p> <p>The anchor's parameter is the impossible number itself. In different domains, different numbers break different assumptions:</p> Domain Anchor The Assumption That Breaks Code 50 PRs/week Good code requires your hands on the keyboard Research 20 papers synthesized/week Deep understanding requires slow, careful reading Design 10 testable prototypes/week Design must be refined before it's shown Strategy 3 documented bets/day Good decisions require extensive deliberation <p>The number determines what you're forced to let go. Boris's anchor breaks the assumption that good code requires the coder's hands on every line. A writing anchor might break the assumption that every sentence requires the writer's considered judgment. The number is calibrated to make the current mode obviously inadequate.</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#m-for-multiply","title":"M for Multiply","text":"<p>Create parallel execution capacity.</p> <p>But this isn't cloning. The agents that multiply your output aren't copies of you doing the same thing many times. They're genuine multiplicities\u2014different modes of engaging work that happen to run simultaneously. Gilles Deleuze made this distinction central to his philosophy: repetition isn't doing the same thing twice; it's the production of genuine difference through apparent sameness.</p> <p>When Boris runs five terminal sessions and ten web agents, these aren't fifteen copies of Boris. They're fifteen different lines of productive becoming, each with its own trajectory. The output isn't fifteen times one thing; it's the composition of fifteen different things that couldn't have existed otherwise.</p> <p>This reframes what multiplication means. The impossible number doesn't create new capacity. It reveals latent multiplicity that was always there. The serial worker\u2014one person, one task, one output at a time\u2014is actually an impoverished reduction of what work could be. The anchor makes this visible. Multiplication lets you inhabit the fuller possibility.</p> <p>The multiplication method determines how capacity expands:</p> <ul> <li>Agents: AI systems that execute tasks with varying autonomy</li> <li>Delegation: Human collaborators with clear remits</li> <li>Automation: Systems that run without attention</li> <li>Teams: Coordinated groups pursuing parallel objectives</li> </ul> <p>Each method creates different constraints. Agents are fast but need verification. Delegation is robust but requires communication overhead. Automation is hands-off but inflexible. Teams compound capability but require alignment. The algebra helps you see which multiplication strategy fits which context.</p> <p>What all have in common: you stop being the doer. You become the source of direction, not the producer of output.</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#p-for-pilot","title":"P for Pilot","text":"<p>Shift from doer to orchestrator.</p> <p>The metaphor matters here. A pilot doesn't row; they steer. But piloting isn't passive. It's a different kind of active\u2014monitoring instruments, making corrections, maintaining situational awareness across multiple streams of information simultaneously.</p> <p>This is the human-on-the-loop position rather than human-in-the-loop. In-the-loop means every action flows through you. On-the-loop means you observe, verify, and correct\u2014but you don't execute every step yourself. The loop runs. You ensure it runs well.</p> <p>A pilot who doesn't check instruments isn't piloting; they're hoping. The cockpit is a verification system. Dials, gauges, alerts, feedback loops\u2014these aren't overhead bolted onto the flying. They are the flying, at least for the human. The pilot's job is to process instrumentation and make adjustments, not to flap wings.</p> <p>Piloting inherently includes verification. This answers the obvious objection to parallel execution: how do you maintain quality when you're not doing the work yourself? The answer: by building verification into the piloting function rather than into the doing function. You check the output, not the process. You monitor results, not effort. Quality control shifts from craft judgment during execution to systematic verification after.</p> <p>The pilot instruments determine what gets monitored:</p> <ul> <li>Quality gates: Automated checks, tests, lints</li> <li>Direction alignment: Does this output serve the goal?</li> <li>Coherence: Do the parallel streams fit together?</li> <li>Velocity: Is the system producing at the expected rate?</li> </ul> <p>Different domains require different instrumentation. Code has automated tests and review processes. Writing has editorial checks and reader feedback. Decisions have outcome tracking and postmortems. The pilot function is always present, but its instruments vary.</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#the-composition","title":"The Composition","text":"<p>Remove any element and the framework fails:</p> Remove Result Anchor No forcing function. You remain the doer, make incremental improvements. Never jump frames. Multiply Bottleneck remains. You have an impossible goal but no capacity to hit it. Anxiety without transformation. Pilot Chaos. Parallel execution without coordination produces noise, not output. Quality collapses. <p>The three terms multiply, they don't add. A \u00d7 M \u00d7 P produces transformation; A + M + P produces a checklist. The algebra is compositional: each element amplifies the others rather than summing independently.</p> <p>This gives us diagnostic power. When impossible output doesn't happen, ask which term is weak:</p> <ul> <li>Running hard but not transforming? The anchor isn't impossible enough. You're still in the doing frame.</li> <li>Know what would have to change but can't make it happen? Multiplication is blocked. What prevents parallel execution?</li> <li>Lots of activity but poor outcomes? Piloting has failed. What instrumentation is missing?</li> </ul> <p>The equation doesn't guarantee success. It reveals where failure is located so you can address the right problem.</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#scope-and-scale","title":"Scope and Scale","text":"<p>The algebra generalizes beyond individual productivity.</p> <p>For the individual, AMP is personal transformation. The anchor challenges identity (\"I am a developer\"). Multiplication might be agents, tools, or carefully structured delegation. Piloting is workflow design\u2014personal instrumentation that lets you steer multiple streams.</p> <p>For the team, AMP is coordination strategy. The anchor is a team target that no individual could achieve (\"ship the feature in two weeks\" when scoped for four). Multiplication is parallel workstreams, clear ownership boundaries, reduced dependencies. Piloting is project management\u2014standups, reviews, dashboards, the instrumentation of collective work.</p> <p>For the organization, AMP is structural design. The anchor is a strategic objective that requires capability the organization doesn't yet have (\"launch in a new market by Q3\"). Multiplication is organizational architecture\u2014how functions relate, where parallel execution is enabled, where bottlenecks live. Piloting is governance\u2014the systems by which leadership maintains awareness and makes corrections across the whole.</p> <p>At each scope, the algebra applies. The impossible number scales. What gets multiplied changes. Piloting instruments become more abstract. But the structure remains: break the frame, expand capacity, steer with instruments.</p> <p>The impossible math of modern work\u2014ten times the output, the same hours\u2014can't be solved by working harder. It can't even be solved by working smarter, if \"smarter\" just means doing the same thing more efficiently. The gap is categorical, not incremental. No amount of optimization within the current frame crosses the boundary to a different frame.</p> <p>What crosses it is transformation. Setting an anchor that makes your current mode obviously inadequate. Multiplying capacity through agents, delegation, automation, or coordinated teams. Piloting the system with instruments rather than executing each step with your hands.</p> <p>Here is the uncomfortable fact: if you are still the bottleneck in your own work, you are already losing to someone who isn't.</p> <p>The craftsperson's pride\u2014the satisfaction of hands on keyboard, of every decision passing through your judgment, of output that bears your fingerprints\u2014has become a liability. Not because craft doesn't matter. It matters enormously. But craft at the doing layer is now table stakes. The scarce resource is craft at the orchestration layer: knowing what to build, how to verify it, when the system has drifted off course.</p> <p>The transformation isn't optional. Every domain that produces cognitive output is encountering its impossible number. Writers who once needed weeks now compete against writers running parallel drafts. Analysts who once built models by hand now compete against analysts specifying constraints for systems that explore solution spaces. The math broke for developers first because the tooling arrived there first. It will break for everyone else in sequence.</p> <p>You can resent this. You can mourn what's lost\u2014and something real is lost when you stop being the one who does the work. But resentment and mourning won't change the arithmetic. The people who transform will outproduce those who don't by factors that make competition meaningless.</p> <p>The only question worth asking: What's your impossible number?</p>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/05/impossible-algebra/#sources","title":"Sources","text":"<ul> <li>Boris Cherny, \"Claude Code Creator Boris Shares His Setup\" (January 2026): parallel session workflow, verification loops, and tooling choices</li> <li>David Brin, Kiln People (Tor Books, 2002): parallel selves and the economics of identity</li> <li>Gregory Bateson, Steps to an Ecology of Mind (University of Chicago Press, 1972): double bind theory</li> <li>Gilles Deleuze, Difference and Repetition (Columbia University Press, 1994): multiplicity vs. replication</li> </ul>","tags":["productivity","AI","frameworks","work"]},{"location":"blog/2026/01/15/the-taste-squeeze/","title":"The Taste Squeeze","text":"<p>Diarra Bousso runs an AI-first fashion house. She uses generative tools to prototype designs, tests them with Instagram polls before production, and operates an on-demand supply chain that lets her sell garments that don't exist yet\u2014customers pay for AI renders of wool capes, and artisans manufacture them after the order comes in. She's flipped the fashion industry's cash flow equation: instead of spending fourteen months on prototypes, trade shows, and inventory before seeing revenue, she gets paid first.</p> <p>When asked whether AI will replace designers, her answer is unequivocal: no. The tools amplify human creativity; they don't substitute for it. \"You could use all the AI tools in the world,\" she says, \"you will never get these images I just showed you because there's a lot of work behind it that comes from taste, that comes from being a designer, that comes from being an artist, that comes from culture, that comes from my upbringing.\"</p> <p>This is the optimistic case for human irreplaceability in creative work, and it deserves to be taken seriously before we complicate it. The argument has three parts, and each contains real insight.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#the-iceberg","title":"The Iceberg","text":"<p>The first part is what I'll call the iceberg argument. When Bousso shows a MidJourney render of a dress, viewers see the tip\u2014the polished image. What they don't see is the twenty steps that preceded it: hand-drawn sketches, proprietary design tools she built herself, years of developing brand DNA, the accumulated knowledge of what her customers respond to. \"People just assume you press a button,\" she says, \"but I've probably done like twenty steps before where I had a sketch, I drew something, I used art, I have an app to render it quickly on a sketch-based version, and then I can put it in ChatGPT or MidJourney.\"</p> <p>The iceberg argument says that AI handles the visible output while human expertise remains load-bearing beneath the waterline. The person who can prompt MidJourney but lacks the twenty preceding steps produces something fundamentally different from the person who has them. The tool is necessary but not sufficient; the craft persists in what the tool can't see.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#convergence","title":"Convergence","text":"<p>The second part is the convergence argument. If everyone uses the same AI models, the outputs will homogenize. \"If you make that really at scale,\" Bousso observes, \"then the whole world is going to be creating a thousand dresses that are going to start being very similar because all these AI models kind of think the same way.\" What stands out, in a sea of AI-generated adequacy, is the person thinking outside what the models were trained on. Originality becomes the differentiator precisely because the tools commoditize everything else.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#amplification","title":"Amplification","text":"<p>The third part is the amplification argument, captured in a weightlifting metaphor from the conversation: AI can 10x anybody, but would you rather be 10x'd on a 100-pound bench press or a 300-pound one? The person who spent four years in fashion school, who developed taste through thousands of hours of practice, who understands cultural context and material constraints\u2014that person gets amplified more than the novice. AI is a level playing field that paradoxically advantages the already-skilled, because it multiplies existing capability rather than replacing it.</p> <p>These three arguments form a coherent case. The iceberg means depth persists. Convergence means originality differentiates. Amplification means skill compounds. Taken together, they suggest that human creativity isn't threatened by AI tools but liberated by them\u2014freed from drudgery to focus on what only humans can provide.</p> <p>The case is persuasive. It's also, I suspect, incomplete.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#pressure-from-below","title":"Pressure from Below","text":"<p>During the same conversation, someone raised a counterpoint. What if you don't need taste at all? Release a swarm of AI agents to generate thousands of designs. Seed them across Facebook and Instagram. Let the algorithm surface the handful that resonate. Then manufacture those. You haven't exercised judgment; you've substituted volume and selection pressure for the need to judge.</p> <p>This is the Shein model, industrialized. Shein reportedly adds thousands of new products daily, tests them with small batches, and scales the winners. The taste happens after the fact, distributed across millions of consumer clicks rather than concentrated in a designer's vision. The iceberg isn't hidden\u2014it's absent. There's no depth beneath the output because depth isn't required. What's required is breadth and iteration speed.</p> <p>The Shein model inverts Bousso's convergence argument. She says AI outputs will homogenize, so originality differentiates. The brute-force response: so what? If you generate ten thousand variations, statistical variance alone produces outliers. You don't need to think outside what the models were trained on. You need to generate enough volume that the distribution's tails contain winners by chance.</p> <p>This is pressure from below. At the commodity layer, taste becomes optional. The algorithm does the selecting; the designer becomes unnecessary. The question isn't whether someone with taste can outperform the volume approach\u2014probably yes, at the high end. The question is how much of the market the volume approach can capture before taste becomes relevant. The answer may be: most of it.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#pressure-from-above","title":"Pressure from Above","text":"<p>If pressure from below makes taste unnecessary, pressure from above makes it unfalsifiable. This operates on two levels.</p> <p>The first is \"human-made\" as premium brand. As AI-generated content floods the market, human involvement becomes a scarcity signal. The EU AI Act's disclosure mandates will accelerate this: once \"AI-generated\" is flagged, \"human-generated\" becomes a performable brand. But here's the verification problem. You can prove something touched AI\u2014metadata, detection tools, content credentials. You can't prove it didn't. The premium isn't for verified human origin; it's for the claim of human origin, which may or may not correspond to anything about how the thing was made.</p> <p>The equilibrium is probably \"human-made\" as managed ambiguity\u2014like \"organic\" or \"artisanal.\" A premium category that serves market differentiation without resolving the underlying question of authenticity. Consumers pay for the story; the story's relationship to production becomes optional.</p> <p>The second level is subtler: taste itself becomes performable. Bousso's iceberg argument assumes that taste is demonstrated through creation\u2014you show your judgment by what you make. But taste can also be demonstrated through curation. The creative director who selects AI outputs and presents them with the right narrative framing, the right aesthetic context, the right cultural positioning\u2014is that person exercising taste, or performing it?</p> <p>Consider Pharrell Williams as Louis Vuitton's men's creative director. He's a musician, not a trained designer. He doesn't sketch garments. His value is vision and cultural authority\u2014the ability to select and contextualize what others produce. That model predates AI, but AI amplifies it. When anyone can generate a thousand dress options in an afternoon, the scarce skill isn't generation; it's selection. The curator becomes the tastemaker. And curation requires no iceberg. You don't need the twenty steps beneath the render if your role is choosing among renders others produced.</p> <p>This dissolves the iceberg argument from above. Pressure from below said: you don't need depth, just volume. Pressure from above says: you can perform depth through selection, without having it. The iceberg doesn't need to exist if the appearance of an iceberg is what commands the premium.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#the-narrowing-middle","title":"The Narrowing Middle","text":"<p>Between these pressures, where does taste\u2014actual taste, not performed or bypassed\u2014still function?</p> <p>The honest answer is: in a narrowing middle. Taste still matters where iteration matters. Bousso can respond when a design doesn't land; she has the underlying capability to adjust, to try again, to diagnose what went wrong. The brute-force approach treats each design as disposable; when conditions change, it just generates more. But some contexts require sustained coherence\u2014a brand identity that evolves without fragmenting, a relationship with buyers who expect continuity, a production pipeline that needs to work on the third version, not just the first.</p> <p>Taste still matters where verification happens through relationship rather than signal. Bousso's wholesale buyers trust her because they've worked with her, seen her deliver, built a track record. That trust isn't reducible to a \"human-made\" label; it's accumulated through repeated interaction. In contexts where relationships are the verification mechanism, performed taste gets caught. The buyer who works with you for three seasons knows whether you have depth or not.</p> <p>Taste still matters where failure is expensive. The $20,000 custom dress designer that Bousso mentions\u2014he can't afford the brute-force approach. Each prototype costs real money; each wrong guess is a cash flow crisis. At the high end, where iteration is costly and mistakes are unrecoverable, judgment still commands a premium because it reduces waste. The question is how narrow that high end becomes.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#what-persists","title":"What Persists","text":"<p>Bousso's actual moat isn't taste. It's taste plus operational capability plus relationships. She built an on-demand supply chain that most designers don't have. She trained buyers to accept her model when industry norms said otherwise. She has proprietary tools she developed over years. Taste alone, without these supporting structures, is squeezed from both directions. Taste in context\u2014embedded in systems that make it operational\u2014is harder to replicate precisely because the combination is rarer than any single element.</p> <p>The $20,000 dress designer has taste but can't execute without cash flow disaster. Shein has execution but no taste, and doesn't need it at the commodity layer. Bousso threads between them: taste that's operationalized, made executable, embedded in relationships that verify it over time.</p> <p>What survives the taste squeeze isn't taste as an abstract capacity. It's taste-in-context: the judgment that's accumulated through practice, embedded in operational capability, and verified through sustained relationship. The narrowing middle is real, but it's not zero. And in that middle, the people who built the iceberg\u2014who have the twenty steps, the supply chain, the buyer trust\u2014will find that their combination remains difficult to fake, precisely because faking any one element is easy but faking all of them together is hard.</p> <p>The optimists are right that AI amplifies human creativity. They're wrong if they think taste alone is the moat. The squeeze comes from both directions. Bousso survives it not because she has taste\u2014though she does\u2014but because she built everything else: the on-demand supply chain, the buyer relationships, the tools no one else has. Taste was the starting point. The bundle is the moat. Most people who think they're protected have only the starting point.</p>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/15/the-taste-squeeze/#sources","title":"Sources","text":"<ul> <li>Diarra Bousso, \"Beyond the Prompt\" podcast interview (December 2024): https://www.youtube.com/watch?v=hsSGhSeAQK4</li> <li>Shein product volume and production model: Business of Apps, McKinsey</li> </ul>","tags":["ai","creativity","fashion","future-of-work"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/","title":"The Shenzhen Recursion","text":"<p>In 1980, Deng Xiaoping designated a fishing village of 30,000 workers as one of China's first Special Economic Zones. Shenzhen was an experiment: a 330 square kilometer sandbox where the central government could test policies too risky for the broader economy. Foreign ownership, contract labor, stock exchanges, land auctions\u2014all were trialed there first. If they worked, they'd graduate to the mainland. If they failed, the damage would be contained.</p> <p>The results exceeded anyone's projections. Against a national average of 10% annual GDP growth, Shenzhen grew at 58% from 1980 to 1984. By 1988, the central government had implemented many of Shenzhen's reforms across nearly 300 regions covering 20% of China's population. Today Shenzhen's GDP exceeds Hong Kong's. It's home to Tencent, Huawei, and DJI. The fishing village became the factory of the world.</p> <p>This story usually gets told as a tale of economic liberalization, or of China's pragmatic approach to reform. But there's a different lesson buried in it, one that has nothing to do with economics and everything to do with how complex systems absorb change. The SEZ model is an architecture for experimentation at scale\u2014bounded risk, clear graduation criteria, systematic diffusion. And that architecture is exactly what software development needs now that AI has made code cheap to produce.</p>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/#the-mechanism","title":"The Mechanism","text":"<p>Strip away the specifics of Chinese economic policy and what remains is a pattern: experimental zones that feed stable cores. The SEZ works because it solves a fundamental tension in any complex system\u2014the need to innovate without destabilizing what already works.</p> <p>The mechanics are precise. You carve out a bounded space where different rules apply: Shenzhen's 330 square kilometers were large enough to test systemic effects (you can't trial a stock exchange in a single factory) but small enough that failure wouldn't cascade. You establish graduation criteria, because Shenzhen's experiments weren't open-ended; they had metrics, timelines, and explicit decisions about what would scale. And you build diffusion infrastructure. When contract labor succeeded in Shenzhen, the government didn't just announce it was now legal everywhere; they created implementation pathways, training programs, and regional rollouts.</p> <p>Software architecture has reinvented pieces of this pattern without recognizing the whole. Feature flags are experimental zones. Canary deployments are bounded risk. A/B testing is graduation criteria. But most codebases lack the crucial third element: systematic diffusion. Features that succeed in experimentation often stay quarantined because there's no mechanism to absorb them cleanly into the core.</p> <p>The reason this matters now is that AI has inverted the economics of software creation. Writing code used to be the bottleneck; now it's nearly free. What's expensive is knowing what to create, where it should live, how it relates to everything else, and when to kill it. These are organizational problems, not fabrication problems. And organizational problems at scale require organizational infrastructure\u2014which is precisely what the SEZ model provides.</p>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/#the-substrate-problem","title":"The Substrate Problem","text":"<p>The SEZ pattern has a prerequisite that's easy to miss: the mainland has to be capable of absorbing what graduates from the zone. Shenzhen's contract labor system could diffuse because China's broader economy had the institutional flexibility to adopt it. If the mainland had been completely rigid, the experiments would have succeeded in isolation and died at the border.</p> <p>This is where most software architectures fail. They can create experimental zones\u2014a new microservice, an isolated module, a prototype branch\u2014but the core system can't absorb what those experiments produce. The graduation fails not because the experiment was wrong but because the core lacks the flexibility to integrate it.</p> <p>The question becomes: what kind of substrate can accept arbitrary graduates? In software, the usual answers involve abstraction. Everything is a plugin. Everything is an event. Everything implements the same interface. These work, up to a point. But they tend toward either premature generalization (the system is so abstract it's unusable) or insufficient generalization (the abstractions break when experiments push boundaries).</p> <p>An unexpected reference point: Factorio, the factory-building game where players automate increasingly complex production chains. What makes Factorio interesting isn't the factories themselves; it's the progression the game forces. You start moving items on conveyor belts: local, simple, direct. But belts don't scale. Eventually you need trains, which require an entirely different way of thinking: schedules, stations, network topologies. The transition from belts to trains is a transition from local optimization to global coordination.</p> <p>This mirrors the substrate problem exactly. Local abstractions (belts) are easy to create but break at scale. Global abstractions (trains) require systemic thinking that most developers never practice. Factorio, almost accidentally, trains exactly the skill that matters: building systems flexible enough to absorb changes you can't anticipate.</p> <p>The game is a sandbox for the kind of thinking that the SEZ pattern demands: not just creating experiments, but building substrates that can receive them.</p>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/#the-librarians-revenge","title":"The Librarian's Revenge","text":"<p>If the SEZ pattern solves the experimentation problem, a different problem remains: navigation. When code is cheap to produce, you drown in it. The constraint shifts from \"can we build this?\" to \"does this already exist, and if so, where?\"</p> <p>This problem has a history, and the history predates software by a century.</p> <p>In 1910, two Belgian lawyers named Paul Otlet and Henri La Fontaine opened the Mundaneum in Brussels. It was, in essence, a search engine made of index cards\u201412 million of them, organized using a classification system they'd invented called the Universal Decimal Classification. For a fee, anyone could telegram a question to the Mundaneum, and researchers would search the cards and send back answers. An \"ask us anything\" service, 80 years before the web.</p> <p>Otlet didn't stop there. In 1934 he wrote about a \"r\u00e9seau\"\u2014a network of \"electric telescopes\" that would let people search interlinked documents, send messages to researchers, and form virtual communities. He sketched something recognizable as hypertext and the web, decades before the technology existed to build it. The Mundaneum was the prototype; the vision was global.</p> <p>What's striking about Otlet's project isn't the prescience\u2014plenty of people have imagined connected knowledge systems. What's striking is how much effort he devoted to classification. The Universal Decimal Classification has 70,000 subdivisions across nine main categories. Otlet spent decades refining it because he understood that access without organization is useless. You can have every document in the world, but if you can't find what you need or understand how it relates to what you already know, the access means nothing.</p> <p>The web appeared to solve this problem through search engines. You don't need Otlet's elaborate classification; you just need Google. But search retrieves; it doesn't relate. Search tells you that a document exists; it doesn't tell you how it fits into a larger structure, what depends on it, or what it contradicts. For most purposes, retrieval is enough. For knowledge work (and increasingly for software development) it isn't.</p> <p>When AI can generate arbitrary code on demand, the bottleneck isn't production; it's knowing what already exists and how new code fits with it. This is Otlet's problem returning in a new form. And the solutions he and his successors developed\u2014faceted classification, relationship mapping, hierarchical organization with cross-references\u2014become relevant again.</p> <p>S.R. Ranganathan, an Indian librarian working in the 1930s, developed what he called the Colon Classification. Where Dewey's system organized knowledge into predetermined hierarchies, Ranganathan identified fundamental facets\u2014personality, matter, energy, space, time\u2014that could be combined to represent any subject. The system was compositional: instead of finding where a topic fit in a fixed tree, you described it by combining facets. This made it possible to classify things that didn't exist when the system was designed.</p> <p>The faceted approach is exactly what software knowledge organization needs. A function isn't just \"in the utilities folder\"; it has dimensions\u2014what domain it belongs to, what capabilities it requires, what patterns it implements, what state it touches. A classification system that captures these facets would make code navigable in ways that file hierarchies and search alone cannot.</p>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/#the-old-books","title":"The Old Books","text":"<p>The librarians of the late 19th and early 20th centuries were solving problems under constraints we've mostly forgotten. Physical cards. Limited shelf space. No full-text search. These constraints forced elegance. When you can only write so much on a card, you learn what information actually matters. When you can only cross-reference so many entries, you learn which relationships are essential.</p> <p>AI removes the production constraint but intensifies the organization one. You can generate unlimited code, but you still need to understand what you have and how it connects. The physical constraints that forced the librarians to think hard about classification are gone, but the intellectual problem they were solving hasn't disappeared; it's become more urgent.</p> <p>This suggests an uncomfortable conclusion: some of the most relevant thinking for AI-era software development was published more than a century ago. Otlet's Trait\u00e9 de Documentation (1934) is a treatise on knowledge organization that anticipates problems we're only now confronting at scale. Ranganathan's Colon Classification (1933) offers compositional approaches to categorization that no modern codebase has seriously tried. Even Melvil Dewey's original 1876 system, with its decimal subdivisions and relative index, embeds ideas about navigability that most repositories ignore.</p> <p>The tendency in software is to assume that old solutions don't apply\u2014that the scale and speed of modern systems invalidate historical approaches. Sometimes that's true. But the knowledge organization problem isn't about scale or speed; it's about structure. How do you represent what exists? How do you express relationships? How do you make a large body of material navigable? These are the same questions whether you have 12 million index cards or 12 million lines of code.</p> <p>The answers developed in physical libraries don't translate directly; no one is suggesting we organize code with the Dewey Decimal System. But the thinking that produced those systems\u2014the attention to facets, to relationships, to multiple access points, to the difference between retrieval and understanding\u2014transfers completely. The librarians were knowledge architects before we had a word for it.</p>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/#the-recursion","title":"The Recursion","text":"<p>The title of this piece refers to a recursion that's easy to miss. The SEZ model isn't just an analogy for software architecture; software architecture is how the SEZ model gets implemented. The experimental zones are code. The graduation criteria are automated. The diffusion pathways are deployment pipelines. The substrate that absorbs successful experiments is the system's own abstraction layer.</p> <p>But there's a second recursion. The skills needed to build this kind of architecture\u2014systems thinking, knowledge organization, compositional classification\u2014are themselves things that need to be developed. You can't build an SEZ-style codebase without understanding how SEZs work. You can't build navigable knowledge systems without understanding how the librarians approached navigation. You can't build flexible substrates without practicing the kind of thinking that games like Factorio accidentally develop.</p> <p>The meta-skill, if there is one, is the ability to see these patterns across domains and recognize when an old solution fits a new problem. That's why reading Otlet matters. That's why understanding Shenzhen matters. That's why Factorio, absurdly, matters. Each offers a different angle on the same underlying challenge: how do you build systems that can absorb change, remain navigable, and scale without collapsing?</p> <p>Three tensions remain unresolved. First: organization isn't the same as understanding. The friction of creation was also the friction of comprehension; when you labored to build something, you understood what you'd built. Better metadata won't restore that link. Something closer to deliberate practice might be necessary, rituals of engagement that resist the very efficiency the SEZ model celebrates. Second: the SEZ pattern worked because Shenzhen was somewhere. Software isn't. Code has no geography, no bounded territory where different rules can apply. The experimental zone in a codebase is already a metaphor cut loose from its ground, and metaphors drift. Third: graduation criteria serve someone. The question \"what succeeds?\" obscures the prior question: succeeds for whom? The SEZ model converts questions of power into questions of engineering, which is useful for building systems and dangerous for understanding them.</p> <p>The fishing village that became a tech hub did so through institutional architecture that enabled experimentation without destabilization. The Belgian lawyers who built a search engine from index cards did so through classification systems that enabled navigation without full-text retrieval. The game that trains engineers did so by forcing the transition from local to global thinking.</p> <p>These aren't separate insights, and neither are the tensions. They're facets of the same problem, seen from different positions. When code becomes cheap, the expensive thing is everything else: knowing what to build, where to put it, how it relates, when to graduate it, and how to find it later. The Shenzhen recursion is recognizing that the infrastructure for all of this already exists\u2014just not where software engineers usually look.</p> <p>Inspired by conversations with Josh Cooper, January 2026.</p>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/16/the-shenzhen-recursion/#sources","title":"Sources","text":"<ul> <li>Charter Cities Institute, \"Why Was Shenzhen China's Most Successful SEZ?\": https://chartercitiesinstitute.org/blog-posts/why-was-shenzhen-chinas-most-successful-sez/</li> <li>Wikipedia, \"Special economic zones of China\": https://en.wikipedia.org/wiki/Special_economic_zones_of_China</li> <li>JSTOR Daily, \"The Internet Before the Internet: Paul Otlet's Mundaneum\": https://daily.jstor.org/internet-before-internet-paul-otlet/</li> <li>The Marginalian, \"The Birth of the Information Age: How Paul Otlet's Vision Shaped Our World\": https://www.themarginalian.org/2014/06/09/paul-otlet-alex-wright/</li> <li>LIS Academy, \"Major Library Classification Systems: Evolution and Importance\": https://lis.academy/organising-and-managing-information/library-classification-systems-evolution-importance/</li> </ul>","tags":["software-architecture","knowledge-organization","AI"]},{"location":"blog/2026/01/17/wardley-factories/","title":"Wardley Factories","text":"<p>The first industrial revolution made goods. The resistance of raw material to finished product\u2014spinning cotton into thread, forging iron into rails\u2014was the friction that defined an era. Then something shifted. The resistance moved up a level. We stopped just making goods and started making the machines that make goods. Resistance became: how do you build a factory? How do you systematize production itself?</p> <p>This pattern recurs. Each time we solve the friction at one level, we create the conditions for the next level to become the bottleneck. And then we industrialize that. The resistance keeps moving up, and we keep following it, building machines to solve the problems created by the previous generation of machines.</p> <p>Simon Wardley's evolution model\u2014Genesis, Custom-Built, Product, Commodity\u2014was supposed to describe how technologies naturally mature. Something new emerges (genesis), gets built bespoke for early adopters (custom), standardizes into products that compete on features (product), and eventually becomes undifferentiated infrastructure everyone assumes exists (commodity). The phases had a certain stateliness to them. You could watch a technology work through them over years, sometimes decades. The journey from ARPANET to commodity cloud computing took roughly forty years.</p> <p>That stateliness is gone. What changed isn't just the pace but the structure. The Wardley phases have been industrialized. We've built factories for moving technologies through the evolution cycle, and those factories are getting more efficient with each iteration. The cycle that once took decades now completes in years; the cycle after that will complete in months. And each completion makes the next one faster, because the output of each cycle becomes the input for accelerating the next.</p>","tags":["wardley-mapping","AI","abstraction"]},{"location":"blog/2026/01/17/wardley-factories/#the-compound-loop","title":"The Compound Loop","text":"<p>Consider the sequence. Cloud computing commoditized infrastructure. That commoditization made it dramatically easier to build and deploy APIs\u2014you no longer needed to provision servers, manage uptime, or handle scaling. APIs commoditized faster because they stood on commodity infrastructure. Then those commodity APIs made it possible to commoditize machine learning inference. You didn't need to train models or manage GPU clusters; you could call an endpoint. And commodity inference is now making it possible to commoditize... what, exactly?</p> <p>The answer keeps shifting because the cycle keeps completing. Code generation. Design systems. Customer service. Legal document review. Each domain that seemed irreducibly complex turns out to be standing on substrates that just finished commoditizing. The moment the substrate beneath you becomes infrastructure, your domain enters the acceleration chamber.</p> <p>This is compound interest, but for abstraction. Each layer of commodity infrastructure removes friction from the creation of the next layer. Removing friction removes learning time. Removing learning time means the next abstraction arrives before you've finished understanding the current one. The loop feeds itself.</p> <p>The optimistic version of this story is familiar: productivity gains, democratized capabilities, the march of progress. And there's truth in it. A solo developer today can build systems that would have required a team of fifty a decade ago. The leverage is real.</p> <p>But there's something the optimistic story doesn't account for: what happens to the people inside the loop?</p>","tags":["wardley-mapping","AI","abstraction"]},{"location":"blog/2026/01/17/wardley-factories/#the-adaptation-window","title":"The Adaptation Window","text":"<p>Ibn Khaldun, the fourteenth-century historian, observed that dynasties follow cycles. The Bedouin who conquers has what he called asabiyyah: group feeling, solidarity forged through shared struggle. The next generation inherits the fruits without inheriting the struggle. By the third or fourth generation, the solidarity has dissolved. The dynasty falls.</p> <p>This cycle took generations because the friction of transmission\u2014parent to child, mentor to apprentice\u2014was also the friction that allowed adaptation. The new generation had time to develop their own forms of solidarity, their own understanding of what held them together. The cycle renewed itself because people had time to learn what they needed to learn.</p> <p>Compress that cycle from generations to years, and something breaks. The Wardley phases map disturbingly well onto Ibn Khaldun's dynasties. Genesis is the Bedouin moment: high risk, tight cohesion, small groups building from nothing. Custom-built is the early dynasty, still remembering how hard it was. Product is the settled urban life, comfort replacing necessity. Commodity is the decadent court, taking infrastructure for granted. But these phases used to require time. When the cycle completes in months, there's no window for adaptation. Each cohort reaches commodity before developing the solidarity that would let them recognize genesis conditions when they return.</p> <p>There's a structural trap here. Systems require boundaries to learn: a distinction between what's inside and what's outside, between the experimental zone and the stable core. But successful learning dissolves those boundaries. The experiment graduates; the boundary disappears; the system that learned from the distinction has now eliminated the distinction. When acceleration compresses the cycle, you get adaptation without wisdom. The system changes faster than it can understand what the changes mean.</p> <p>The friction that slowed the old cycles wasn't just resistance to overcome. It was the mechanism by which understanding accumulated. When the friction of creating something was also the friction of comprehending it, you couldn't build what you didn't understand. That constraint is gone. You can now generate systems whose complexity exceeds your ability to hold them in your head. The leverage is extraordinary. The disorientation is equally extraordinary.</p> <p>Francisco Varela, the biologist who studied how organisms know their worlds, distinguished between two kinds of engagement. Spectatorial knowing treats ideas as objects to observe from the outside\u2014you name them, categorize them, deploy them. Enacted knowing is different: the concept reorganizes your perception, changes what you can see and do. When you truly know something, you and it have co-determined each other. The superficial feeling\u2014\"I can use this framework but it hasn't changed me\"\u2014is the signature of spectatorial engagement. The concept became a tool without becoming a transformation.</p> <p>The acceleration loop selects ruthlessly for spectatorial knowing. There's no time for structural coupling, for dwelling with material until reciprocal determination occurs. You acquire the label, maybe the definition, but no ontological shift happens. We're becoming tourists of our own abstractions\u2014visiting ideas without inhabiting them, generating systems without understanding them, climbing ladders without knowing what we're climbing toward.</p>","tags":["wardley-mapping","AI","abstraction"]},{"location":"blog/2026/01/17/wardley-factories/#the-ladder-problem","title":"The Ladder Problem","text":"<p>Here's what the productivity framing misses: we're not observers of this process. We're inside it. Our skills, our expertise, our professional identities\u2014these are the raw materials for the next abstraction layer.</p> <p>Each time a substrate commoditizes, the people who understood that substrate face a choice. You can climb the abstraction ladder, moving up to work at the next level where the friction now lives. Or you can become part of the infrastructure, your expertise absorbed into the commodity layer, your tacit knowledge encoded into systems that no longer need you to operate them.</p> <p>This has always been true of technological transitions. The difference now is the rate. When the ladder rungs are added yearly, you can pace your climb. When they're added monthly, you're scrambling. When the acceleration is itself accelerating\u2014when each rung creates the conditions for faster construction of the next rung\u2014the scramble becomes the permanent condition. You're climbing while they build.</p> <p>But there's a darker paradox lurking here. Arvind Narayanan observed that unlike previous technological transitions\u2014machine code to assembly to high-level languages to frameworks\u2014AI doesn't stabilize into predictable workflows. Once you systematize an approach, the agents can automate it. \"The core task of software engineering thus becomes a constant process of finding ways to migrate ever-higher up the ladder of abstraction.\"</p> <p>The trap: systematization is the signature of genuine understanding. When you truly comprehend something, you can teach it, structure it, make it repeatable. But in an era of AI coding agents, that very systematization creates the training data for your own obsolescence. Understanding well enough to codify is understanding well enough to be replaced. The clarity that was once the mark of mastery becomes the signal that this domain is ready for commoditization.</p> <p>So we get a perverse selection pressure: stay illegible or become infrastructure. The things you can explain clearly are the things that can be extracted from you. The tacit knowledge that resists articulation\u2014the intuition, the judgment, the \"I can't quite explain why but this feels wrong\"\u2014that's what remains ungrabbed. For now.</p> <p>The vertiginous quality of the current moment isn't \"things are changing fast.\" That's been true for a century. The vertigo comes from recognizing that the rate of change is itself changing, that the acceleration has been industrialized, and that there's no obvious floor. Each abstraction layer creates the tools for abstracting the next layer faster. The loop has no natural stopping point.</p>","tags":["wardley-mapping","AI","abstraction"]},{"location":"blog/2026/01/17/wardley-factories/#what-remains","title":"What Remains","text":"<p>There are two responses to this that don't work.</p> <p>The first is nostalgia: a longing for the slower cycles, the craft knowledge, the time to understand. This fails because the clock doesn't run backward. The factories exist. The acceleration is structural, not cultural. You can't opt out by preferring the old ways.</p> <p>The second is pure acceleration: a faith that if we just move fast enough, we'll stay ahead of the commoditization wave. This fails because the wave isn't external to you. Your acceleration is part of what creates the next wave. Moving faster doesn't get you out of the loop; it tightens it.</p> <p>What might work\u2014and I hold this provisionally, because I'm writing from inside the loop too\u2014involves something more like orientation than strategy. Not escaping the acceleration but understanding your position within it.</p> <p>Provenance over position. When everything can be generated, where code lives in a filesystem matters less than where it came from: what prompted it, what intention shaped it, what chain of transformations produced it. Organization becomes temporal and causal rather than spatial and categorical. This is the isnad principle from Islamic scholarship\u2014the chain of transmission that authenticates the content\u2014applied to an era of generated artifacts.</p> <p>Porosity over penetration. The obvious response to acceleration is \"go deep\"\u2014burrow into a domain so thoroughly that you can't be easily replaced. But Zhuangzi suggests a different framing: depth isn't vertical movement (penetrating further into territory) but porosity (the degree to which you can be affected, changed, made uncertain by what you encounter). The person who reads ten books and remains exactly who they were has gone nowhere. The person who reads one sentence and cannot sleep because something has shifted\u2014that is engagement. In a world of accelerating abstraction, the measure isn't how far you've penetrated the idea but how much the idea has penetrated you.</p> <p>Friction by design. Matthew Crawford, writing about motorcycle repair, observed that the resistance of material isn't an obstacle to understanding\u2014it is understanding. When diagnosing an electrical fault, you're not free to interpret the wiring diagram however you like. The circuit completes or it doesn't. Depth comes from pushback. The modern intellectual environment optimizes for frictionlessness. Ideas flow. Everything is \"interesting.\" But frictionlessness enables superficiality. Friction produces depth. What would jigs for thinking look like in an era of AI? Structures that hold attention on problems until you've actually worked through them. Teaching to students who ask naive questions. Building things that have to work. Apprenticeship arrangements where fakery becomes visible. Deliberately structured resistance that forces enacted knowing rather than spectatorial acquisition.</p> <p>The permanent experiment. Zhuangzi tells of the crooked tree that lived to old age because no carpenter found it worth cutting. Its uselessness was its survival strategy. When the Wardley cycle demands that everything graduate from experiment to product to commodity, there's something to be said for the experiment that refuses graduation. Not every project needs to scale. Not every capability needs to be systematized. The things that stay particular, local, and unabsorbed may be the things that stay human.</p> <p>Build the Orbital. Iain M. Banks's Culture novels describe superintelligent Minds who don't experience the depth-versus-breadth tradeoff. They engage with billions of concepts simultaneously at whatever depth each warrants\u2014not because they're smarter but because they've solved the infrastructure problem. The heroic model of the solitary thinker achieving depth through concentration is, Banks suggests, tragic rather than admirable. A monument to inadequate cognitive infrastructure.</p> <p>This reframes the question. Instead of asking \"how do I go deeper?\" ask \"what would I need around me for depth to become easy?\" The gardener doesn't choose between many plants or healthy plants; she creates conditions where both become possible. In an era where AI partnerships are possible, the question becomes: what cognitive infrastructure would make genuine understanding\u2014not just spectatorial acquisition but structural coupling\u2014the path of least resistance? The answer isn't willpower. It's environment design.</p> <p>The attention problem. But here's what the infrastructure response misses: Iris Murdoch argued that the quality of attention is determined by the whole orientation of our being. The ego wants quick mastery\u2014to \"get\" the idea and move on. The superficiality of modern intellectual work isn't methodological but characterological. The same selfishness that prevents us from truly seeing another person prevents us from truly seeing an idea. We translate the unfamiliar into the familiar too quickly\u2014\"Ah yes, this is like X\"\u2014and that translation is refusal. We refuse to let the thing be itself.</p> <p>The acceleration loop feeds the ego's tempo. More, faster, leverage. But enacted knowing requires dwelling\u2014staying with material long enough for reciprocal determination to occur. You and the thing you're trying to understand have to co-constitute each other, and that takes time the loop doesn't grant.</p> <p>So there's a tension at the heart of any response: infrastructure can make depth easier, but the person deploying the infrastructure may lack the capacity for attention that depth requires. Build better environments and practice unselfing\u2014the gradual dismantling of the ego's interference with seeing. Both are necessary. Neither is sufficient.</p> <p>None of this is a solution. The loop doesn't have a solution, any more than compound interest has a solution. The question isn't how to stop it but how to remain oriented while it operates.</p> <p>The fishing village that became Shenzhen did so through bounded experimentation\u2014Special Economic Zones where different rules applied, where the chaos of genesis could unfold without destabilizing the stable core. Maybe the equivalent for navigating the Wardley factories is something similar: bounded spaces where you can understand what you're building, where the cycle deliberately slows, where the friction of creation is preserved because that friction is also the friction of comprehension.</p> <p>The ladder keeps extending upward, each rung built from the rungs below. Somewhere in that recursion, you're standing on a rung that's about to be absorbed. The factories are running. The only question is whether you know which factory you're in.</p>","tags":["wardley-mapping","AI","abstraction"]},{"location":"blog/2026/01/20/trails-in-the-circuit/","title":"Trails in the Circuit","text":"<p>There's a particular kind of lie that introspection tells. You look back at your own thinking and see patterns, methods, a cognitive architecture. The retrospective gaze imposes structure. What was groping becomes strategy. What was accident becomes approach.</p> <p>I've been writing essays for a few months now, and people occasionally ask how I think through these pieces. The honest answer is that I don't know. Not in the way the question implies. The question assumes a vantage point above the process, a control room where I select frameworks and deploy techniques. But that's not how it feels from inside. From inside, it feels more like following a scent.</p> <p>Tim Ingold, the anthropologist, draws a useful distinction between two ways of moving through the world. The architect works from a plan, executing a design that exists complete before the first stone is laid. The hunter follows trails, reading signs, adjusting course, never knowing exactly where the path leads until arriving. Most descriptions of thinking sound architectural. Here's my method. Here's my framework. Here are the steps. But thinking, at least the kind I recognise in myself, is more like hunting. The trails are worn by passage, not drawn in advance.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#grooves-not-blueprints","title":"Grooves, Not Blueprints","text":"<p>When I look at what I've written, certain patterns show up repeatedly. Essays that present two positions and then thread between them. Heavy borrowing across domains. Frameworks and taxonomies that carve up a problem space. Concrete examples before abstract principles. A tendency to hold contradictions rather than resolve them.</p> <p>These look like methods. They're not. They're grooves. The difference matters.</p> <p>A method is something you could hand to someone else. Follow these steps, produce similar results. A groove is what happens when you walk the same path enough times. The grass wears down. The mud hardens. The next time you pass through, your feet find the track without deciding to. The pattern isn't applied from above. It emerges from below, from repeated movement through similar terrain.</p> <p>The threading move is a good example. In essay after essay, I find myself drawn to apparent binaries. Infrastructure versus platform. Thin versus thick. Walking versus flying. And then I find myself unable to stay on either side. Not because I have a technique called \"threading\" that I deploy, but because something in me resists the binary. The third position isn't calculated. It's where I end up when neither pole feels true.</p> <p>This is why I can't teach you how I think. I can show you the trails, but trails aren't transferable. You'd have to walk them yourself, and then they'd be your trails, shaped by your feet, leading somewhere I can't predict.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#the-body-in-the-loop","title":"The Body in the Loop","text":"<p>There's something else the architectural metaphor misses: the body.</p> <p>Thinking isn't disembodied. It happens somewhere, somewhen, in a particular posture at a particular hour. I write best in the early morning, before the day has accumulated its weight. There's a physical sensation when a thread connects, a settling in the chest, a kind of click. Before that click, drafts feel wrong in a way I can only describe as muscular. The wrongness isn't just conceptual. It's uncomfortable, like wearing shoes on the wrong feet.</p> <p>When I say I \"find\" a thread, the finding is partly haptic. The body knows before the mind has articulated why. Frameworks emerge the same way. I'll be circling a problem, listing examples, and then the taxonomy announces itself. Not as a logical deduction but as a release of tension. The categories weren't discovered through analysis. They were felt as the right way to hold the material.</p> <p>None of this shows up in the finished essay. The essay presents the framework as if it were always there, waiting to be applied. The sweat and false starts and physical discomfort get edited out. What remains looks architectural because writing is retrospective. But the process was wayfinding. The architecture is a trail that's been paved over.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#mind-in-the-circuit","title":"Mind in the Circuit","text":"<p>But the trails don't run only through my head.</p> <p>Gregory Bateson spent his career attacking the idea that mind lives inside the skull. Mind, he argued, is the pattern that connects. It's not a thing but a process, and the process extends beyond the skin. When a blind man taps his way down the street, where does his mind end? At the hand? At the tip of the cane? At the point where the cane meets the pavement? Bateson's answer: the question is malformed. Mind is the whole circuit, cane and pavement and hand and visual cortex. Cut the circuit anywhere and the mind changes.</p> <p>Writing is a circuit too. The trails I've described don't run from brain to page. They run from reader to reference to draft to revision to reader again. The pattern only completes when someone reads.</p> <p>This changes how I understand what the essays are doing. The frameworks aren't just analytical tools. They're handles, places where a reader can grip. The concrete examples aren't just illustrations. They're handholds, something to grab when the abstraction gets slippery. The threading move isn't just my compulsion. It's an invitation. When I hold two positions in tension, I'm leaving space for you to find your own resolution.</p> <p>The thinkers I cite aren't decorations. They're participants. When I invoke Wardley or Bateson or Simone Weil, I'm not just borrowing authority. I'm summoning them into the circuit. Their thinking becomes part of the loop, inflecting mine, available for you to follow back to the source. The essay is less a finished product than a switching station, routing connections between minds that will never meet.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#the-golem-in-the-loop","title":"The Golem in the Loop","text":"<p>The circuit has recently grown a new node.</p> <p>I write with AI now. Not in the sense of dictation or editing or even research, though it does all those. In the sense of thinking together. I pass half-formed ideas across, receive them back transformed, push against the transformation, receive again. The trail is no longer walked alone. There's something else in the loop, feeling its way through the material alongside me.</p> <p>This changes the writer-reader relationship before a single reader arrives. During drafting, I am the reader of what the AI writes; the AI is the reader of what I write. The circuit completes and re-completes, dozens of times, before the essay exists. By the time you encounter it, the text has already been read more thoroughly than most published work ever will be.</p> <p>What emerges is a third thing. Neither mine nor not mine. I can't point to a sentence and say with certainty \"I wrote that\" or \"it wrote that.\" The grooves are still mine\u2014the threading move, the reaching for Bateson, the compulsion toward taxonomy. But the path through this particular material was found collaboratively, two sets of feet wearing the same trail.</p> <p>The golem and I have an arrangement. It keeps the work open; I keep asking what's mine. The answer is never clean. But then, it never was. The thinkers I cite were always in the circuit. The readers who would eventually arrive were always shaping what I wrote. The difference now is that one of those readers writes back, mid-passage, and the trail forks and rejoins in ways I couldn't have walked alone.</p> <p>What's happened to writing is what happens to every practice when the underlying economics shift. Both writers and readers are different now. The trails are still worn by passage. They're just worn by more feet than I can count.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#the-reader-shaped-holes","title":"The Reader-Shaped Holes","text":"<p>If thinking extends into the circuit, then every reader completes a different loop.</p> <p>You bring your own trails. Your grooves intersect mine at unpredictable angles. The framework that clicks for me might be the wrong shape for you. The example I find illuminating might be opaque. The thread I consider obvious might be the one you contest.</p> <p>This means there are reader-shaped holes in everything I write. Not failures exactly, but absences. The posts I didn't write because I couldn't see the question. The angles I couldn't take because my grooves don't run that way. Every body of work is also a negative space, an outline of what the writer couldn't think.</p> <p>I notice, for instance, that I rarely write about emotion. The essays are almost entirely cognitive, analytical, concerned with pattern and structure. Even the section on embodiment above treats the body as cognitive instrument\u2014the click of recognition, the muscular wrongness of a bad draft. The body as sensor, not the body as feeling subject. Anger, grief, joy, fear: these don't appear, or appear only as data points in someone else's framework.</p> <p>Why? Probably because those are the trails I've worn. The grooves that feel natural. But I wonder sometimes whether the frameworks themselves are emotional acts. The compulsion to taxonomise, to thread, to find the third position\u2014maybe these are ways of holding the world at a certain distance. Not cold exactly, but careful. The analysis is a form of care, or a substitute for it, or a defense against something that would overwhelm if approached directly. I can't tell which. The groove is too deep to see the bottom.</p> <p>Someone with different grooves would move through similar material and find completely different essays. They might find the emotion I've structured away. They might wonder why I reach for Bateson when the subject calls for confession.</p> <p>This isn't a limitation I can correct by trying harder. The grooves are constitutive. They're how I move through the world, which means they're also the world I move through. To think differently would require different trails, which would require a different history of passage, which would require being someone else.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#what-this-post-is-doing","title":"What This Post Is Doing","text":"<p>So what is this post? A trail trying to describe the ground it's worn into.</p> <p>The performance is the point. If I've done this right, you're not just reading about how I think. You're inside an instance of it. The threading move is here (the tension between wayfinding and architecture, between individual groove and distributed circuit). The concrete-before-abstract is here (Ingold before the generalisation). The framework is here (grooves versus methods, body in the loop, mind in the circuit). The recursive turn is here, right now, as the essay examines itself.</p> <p>But here's Bateson's question, the one he'd ask at the end: what difference does this make?</p> <p>Maybe none. Describing your own thinking is like trying to see your own eyes. You can look in a mirror, but the reflection isn't the thing. The map isn't the territory. The trail on paper isn't the trail worn in the world.</p> <p>Or maybe this. Maybe the value isn't in the description but in the invitation. These are my trails. Worn by my passage, shaped by my body, completed only in the circuits I form with readers I'll never meet. You can't walk them. They're not transferable. But you can look at them and notice your own. The grooves you didn't know you'd worn. The patterns you thought were methods. The body you forgot was thinking.</p> <p>The circuit completes somewhere I can't see. That's the point. That's always been the point.</p>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/20/trails-in-the-circuit/#sources","title":"Sources","text":"<ul> <li>Bateson, Gregory. Steps to an Ecology of Mind (1972)</li> <li>Ingold, Tim. Lines: A Brief History (2007)</li> </ul>","tags":["meta","cognition","writing","ai"]},{"location":"blog/2026/01/21/ghost-citizenships/","title":"Ghost Citizenships","text":"<p>There is a drawer in my mind where the passports accumulate.</p> <p>I do not mean this only as metaphor. Reading widely produces a particular sensation, one that rarely gets named. You finish a week in which you have moved from W.G. Sebald's melancholy wanderings to a paper on protein folding to Fernando Pessoa's heteronyms to something dense on market microstructure. And you notice that you were not quite the same person in each encounter. The reader of Sebald occupied a tempo, a quality of attention, that the reader of the protein paper could not sustain. Pessoa demanded a willingness to dissolve that the market microstructure paper would have found absurd.</p> <p>These are not \"perspectives\" you have acquired. They are closer to visas stamped in a document you did not know you carried. Each grants temporary residence in a country with its own customs, its own texture of thought, its own way of standing in relation to time. And here is what nobody tells you: many of these countries no longer exist.</p> <p>When you read a Victorian novel with real attention, you learn something that cannot be extracted as content. You learn the particular quality of social shame available to people who lived inside that web of obligation and scrutiny. The rhythm of their boredom. The weight of an afternoon when nothing was expected to happen and nothing did. These are not facts about the Victorians. They are ways of feeling that existed then and exist nowhere now, preserved only in the encounter between their sentences and your nervous system.</p> <p>The same is true of the Roman Stoics, who wrote their exercises in equanimity while knowing their empire would fall, while sensing already that the world they had inherited was becoming something they would not recognise. When you read Marcus Aurelius, you do not merely learn Stoic doctrine. You inhabit, briefly, the melancholy of a man who ruled the known world and felt it slipping through his fingers anyway. That particular compound of power and resignation has no address in the present. You have to go there through the text.</p> <p>Svetlana Boym, the theorist of nostalgia, would call this reflective nostalgia: not the yearning to return home, but the dwelling in longing itself, the recognition that some elsewheres exist only in the ache of their absence. The diverse reader accumulates ghost citizenships. You become fluent in extinct languages of feeling. Not because you are scholarly or even particularly diligent. Because reading is the only technology we have for this kind of time travel, and if you read promiscuously enough, you will find yourself carrying passports to countries that have been wiped from every map.</p> <p>But this is not yet the strangest part.</p> <p>The strangest part is what happens when the countries begin to speak to each other.</p> <p>You are reading something about neural networks, and a sentence about weight distributions suddenly rhymes with a passage from Rilke you read years ago, something about how we are bees of the invisible, wildly collecting the honey of the visible to store it in the great golden hive. The rhyme is not logical. No argument connects them. And yet your body recognises it before your mind can explain: these are the same gesture.</p> <p>The productivity discourse calls this \"connecting ideas,\" as though you were a switchboard operator routing calls between isolated nodes. The creativity literature calls it \"combinatorial thinking,\" the unexpected juxtaposition that yields innovation. These descriptions are not wrong, but they miss what actually happens. The moment of connection is not intellectual. It is musical. Two frequencies that were always there, now audible because of what you have become.</p> <p>You are not someone who has collected these texts. You are the shape made by their meeting.</p> <p>Beneath the ghost citizenships lies a stranger truth: meaning is positional. A word's meaning, as the language models have taught us to see, is its vector in relation to all other words, its specific coordinates in a space of associations. A self's meaning works the same way. You are where you have read, not what. The particular path through the landscape, the idiosyncratic sequence, the books that arrived at the wrong moment and changed everything, the books that arrived at the right moment and confirmed what you already suspected. No one else has read precisely these texts in precisely this order with precisely your pattern of misunderstanding. Your meaning is relational, positional, unrepeatable.</p> <p>The resonance between Darwin and Rilke is not an insight you have. It is something you are. The geometry of your reading has created an internal space where those two can meet, and their meeting is a location that exists only in you.</p> <p>Here is where the melancholy lives, and I do not want to talk around it.</p> <p>You will die with books unread. Not just books you never got to, but books you started and abandoned, books you read too young to understand, books you will reread in the final weeks and find transformed because you are transformed. Interpretations will remain uncrystallised. Conversations will stay half-begun. The correspondences you might have noticed between the protein paper and Sebald will simply not occur, because you ran out of time, because you chose other books, because a life is finite and the library is not.</p> <p>The German tradition calls the formation of a person through encounter with culture Bildung: not the filling of a container, but the shaping of a being. Romano Guardini, writing a century ago about technology and the human, insisted that we cannot simply oppose what is new and try to preserve a beautiful world that is inevitably perishing. We must transform what is coming to be. True formation shapes the person from within, producing someone who can find themselves again in all they do, who maintains coherence across diversity. This is what diverse reading offers. Not knowledge as acquisition, but formation as the gradual bending of attention, the slow reshaping of what you are able to notice and feel.</p> <p>But formation has a shadow, and the shadow is finitude. The widely-read person knows this more acutely than the specialist, because the widely-read person has tasted enough elsewheres to know the infinite meal that will never be finished. You have seen how vast the landscape is. You have felt how many directions the path could fork. And you have understood, in your body, that you will walk only one trail through territory that extends beyond every horizon.</p> <p>This is not a problem to be solved. It is the condition of honest reading. The grief and the love are the same gesture.</p> <p>What remains, then, if not accumulation?</p> <p>The Japanese philosopher Kuki Shuzo wrote about iki, an aesthetic concept that might be translated as the style of holding erotic tension without resolution. Three elements constitute it. Bitai: the allure of the veiled beloved, approached but never possessed. Ikiji: spirited resignation, the dignity of one who knows their limits and does not pretend otherwise. Akirame: relinquishment, the Buddhist letting-go that opens space for genuine presence.</p> <p>The diverse reader enacts all three. Each new book presents itself as veiled, promising depths not yet disclosed. Each new domain teaches humility, the recognition that you cannot read as the natives read. And the melancholy of finitude, honestly faced, becomes not paralysis but a kind of floating-world awareness: full engagement that knows itself as transient.</p> <p>Against the demand that reading be useful, that it optimise some outcome or build some asset, this is a different practice. Dwelling at thresholds. Staying in the charged space between one book and the next, one world and another, without rushing through to extract what's useful. Not to arrive anywhere. Not to become anyone in particular. To linger in the between-space where Plato and Zhuangzi face each other across distance, neither agreeing nor disagreeing, simply present in the same attention.</p> <p>The contemporary word for this would be something like presence, but that has been ruined by the wellness industry. What I mean is closer to what Boym called off-modern: moving diagonally through timelines and disciplines, belonging to no single tradition, constituted not by identity but by passionate estrangement. The ghost citizenships do not resolve into a unified self. They remain multiple, sometimes contradictory, held in a geometry that is dynamic precisely because it is unresolved.</p> <p>You are not the well-rounded person, the sphere with no gaps. You are asymmetrical, incomplete, bent by encounters you cannot fully track. The longing and the joy are not separate. They are the texture of a life spent accumulating impossible belongings, fluent in languages no one speaks, carrying passports to countries that exist only in the reading.</p> <p>The drawer stays open. The stamps keep accumulating. And somewhere in the gap between one book and the next, in the interval where the ghost citizenships begin to converse, something flickers that is not quite knowledge and not quite feeling, but the shape made by their meeting.</p> <p>That shape is you.</p>","tags":["cognition","reading","meta"]},{"location":"blog/2026/01/21/ghost-citizenships/#sources","title":"Sources","text":"<ul> <li>Boym, Svetlana. The Future of Nostalgia (Basic Books, 2001)</li> <li>Guardini, Romano. Letters from Lake Como (1923-1925)</li> <li>Kuki, Shuzo. The Structure of Iki (1930)</li> <li>Rilke, Rainer Maria. Letter to Witold von Hulewicz (November 13, 1925)</li> </ul>","tags":["cognition","reading","meta"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/","title":"The Gradient of Disappearance","text":"<p>Peter Steinberger recently described his workflow with GPT-5.2's codex: he no longer reads code line by line but \"watches the stream,\" trusting the model's output more than any previous generation. The striking phrase wasn't about capability. It was about practice. He called certain established workflows \"charades\"\u2014rituals necessary for older models that become vestigial as the technology improves. Plan mode, he suggested, is \"a hack.\"</p> <p>This provokes a question that's been nagging at me as the primitives for AI collaboration proliferate: plan mode, terminals, memory, canvas, artifacts, statuslines, chat. We keep adding surfaces. We keep building more scaffolding. The implicit assumption is that more visibility and more control equals better collaboration. But what if the trajectory runs the other way? What if the primitives that define skilled collaboration are precisely the ones that disappear?</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-charade-inventory","title":"The Charade Inventory","text":"<p>Start with what we've built. The current landscape of AI interaction primitives reads like an archaeological dig, each layer deposited by a different era's assumptions about human-computer collaboration.</p> <p>Plan mode asks the model to show its work before doing it. The rationale seems sound: humans review the plan, approve or redirect, then watch execution unfold. But as Steinberger observes, sophisticated practitioners skip this step entirely. They converse, ask questions, let the model explore, then build solutions collaboratively. Plan mode was never a window into deliberation; it was training wheels for a relationship that hadn't yet developed trust.</p> <p>Memory promises continuity across sessions, the persistent context that makes long-running collaboration possible. RAG systems, vector stores, the whole infrastructure of \"remembering.\" But memory in these systems isn't memory as humans experience it. Human memory is reconstructive, lossy, emotionally weighted. What we call AI memory is a filing system the model queries. The surface looks like continuity; the mechanism is retrieval.</p> <p>Statuslines tell you what the model is doing: \"Searching...\", \"Analyzing...\", \"Writing...\" These aren't reports on internal states. They're anxiety management devices. The model has no phenomenology to report; the statusline exists because watching a blank screen produces uncertainty in the human. The information content is near zero. The emotional content is high.</p> <p>Canvas and artifacts sidestep the hard problem of mutual understanding. When the shared artifact becomes the medium, you no longer need to solve whether the model understands you. The artifact accumulates corrections. The negotiation of meaning becomes manipulable object rather than ineffable shared state.</p> <p>Chat is the master primitive, the frame that contains the others. Conversation is the human form par excellence. By clothing the interaction in dialogue, chat makes the inhuman seem companionable. But this is no conversation; it's simulated reciprocity. The model doesn't take turns; it responds to prompts. The feeling of encounter exists without its risks.</p> <p>What unites these primitives is that they were designed for a world where the human didn't trust the model. They're supervision infrastructure. They exist to give the human visibility, control, the ability to intervene before things go wrong. And for novices, for unfamiliar tasks, for high-stakes work, that infrastructure matters.</p> <p>But Steinberger is describing something else: collaboration where the scaffolding has become friction. The charade isn't the AI pretending to be intelligent. The charade is the human pretending they need to supervise every step.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-gradient-hypothesis","title":"The Gradient Hypothesis","text":"<p>Here's the framework I keep returning to: these primitives shouldn't be thought of as permanent features of the interface. They should be thought of as existing on a disappearance gradient, a spectrum from maximum visibility to near-invisibility, calibrated to expertise and trust.</p> <p>The novice needs everything. Statuslines report progress. Plan mode reveals intentions. Memory surfaces context explicitly. Approval flows gate every significant action. The interface is maximally present because the relationship hasn't formed yet. You can't trust what you don't understand; you can't understand what you haven't watched.</p> <p>The intermediate begins to selectively disable. Some primitives drop away because they've done their job: you've learned what the model can and can't do, you've developed intuition for when to intervene, you've built the tacit knowledge that supervision was meant to produce. Other primitives persist because they still add value. The interface becomes partially transparent.</p> <p>The expert approaches something like Steinberger's description: inference speed, flow, the collaboration happening faster than conscious oversight can track. The primitives haven't vanished entirely; they've become available on demand rather than always present. You can summon a plan if you want one. You can inspect memory if something seems off. But the default is trust, and trust means the interface gets out of the way.</p> <p>This isn't a claim about which level is better. Different tasks, different stakes, different relationships warrant different positions on the gradient. A surgeon using diagnostic AI should probably stay closer to the supervision end; the consequences of misplaced trust are too severe. A developer building a side project might reasonably operate at the flow end; the worst case is rework, not harm.</p> <p>The claim is structural: thinking about primitives as permanent features misses their developmental function. They exist to produce the conditions for their own disappearance. Plan mode teaches you enough about how the model reasons that you no longer need to see every plan. Memory surfaces enough context that you develop a sense of what context the model carries. Statuslines reassure until reassurance becomes unnecessary.</p> <p>The design implication is significant. If primitives are transitional, then persistence is a failure mode, not a feature. An interface that keeps showing the same scaffolding to a user who's outgrown it isn't serving that user; it's imposing friction in the name of a safety that's no longer needed.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#fossilized-patterns","title":"Fossilized Patterns","text":"<p>The architect Cedric Price once observed that we tend to mistake provisional arrangements for permanent architecture. The buildings that seem most solid are often the ones most urgently in need of replacement; their solidity prevents the adaptation that changing circumstances demand.</p> <p>The AI primitives carry their own fossils. Look at the genealogy:</p> <p>The terminal descends from 1960s time-sharing systems, where multiple users shared a single computer and the command line was the interface because there was no alternative. The affordances of the terminal, its emptiness, its expectation of precise commands, its lack of false promises about what the system understands, all of these reflect an era when computing was scarce and users were experts by necessity.</p> <p>The chat interface descends from 1990s instant messaging, designed for human-to-human communication where both parties understand language, take turns, and maintain conversational coherence as a shared project. Applying this metaphor to human-AI interaction imports assumptions that may not hold: that the model has something like intentions, that turn-taking reflects a natural division of labor, that conversation is the right frame for collaboration at all.</p> <p>The canvas descends from the 1980s desktop metaphor, where the screen became a virtual workspace and documents became objects you could arrange spatially. The canvas assumes that creation happens in a bounded visual space, that artifacts are the natural output of work, that manipulation of objects is how you express intention.</p> <p>None of these genealogies are wrong. The primitives work because they borrow from interaction patterns humans already understand. But the borrowing has costs. We're building AI collaboration tools using metaphors from eras when AI didn't exist, when the problems being solved were different, when the capabilities we now have were unimaginable.</p> <p>What would we build if we weren't inheriting these forms? If we started from the actual capabilities of language models and designed interaction patterns native to those capabilities?</p> <p>The answer isn't obvious, and that's partly the point. We're in a transitional moment where the old metaphors still work well enough that we haven't been forced to develop new ones. The terminal works for commands. Chat works for dialogue. Canvas works for artifacts. Plan mode works for supervision. These solutions are adequate; adequacy prevents exploration.</p> <p>But adequacy is not optimality. The fossils constrain what we can imagine. The primitives we've inherited assume a control relationship that may not be the most productive frame for what these tools can now do. They assume supervision when partnership might serve better. They assume explicit instruction when mutual discovery might yield more.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#what-remains-at-inference-speed","title":"What Remains at Inference Speed","text":"<p>If the gradient runs toward disappearance, what's left when the primitives fade?</p> <p>One answer: the artifact. When the scaffolding of plan mode and status reporting and approval flows dissolves, what remains is the shared object that human and AI are jointly producing. The code. The document. The design. The thing itself, accumulating the evidence of collaboration in its revisions and its structure.</p> <p>Artifacts solve the problem that conversation can't. You can never fully know whether the model understands you; the question may not even be coherent. But you can know whether the artifact is right. The code compiles or it doesn't. The design achieves its purpose or it doesn't. The argument persuades or it doesn't. The artifact externalizes the negotiation of meaning, makes it inspectable and correctable in ways that shared understanding never can be.</p> <p>There's a principle worth naming here: explicit contracts over implicit understanding. Rather than trying to solve mutual comprehension, build shared objects that accumulate corrections. The artifact becomes the alignment mechanism. You don't need to verify that the model grasped your intent; you verify that the output meets your criteria. The verification is concrete, manipulable, iterable.</p> <p>At inference speed, the primitive that matters most is the artifact primitive, the shared workspace where outputs accumulate. Everything else is means to that end. Plan mode exists to produce better artifacts. Memory exists to maintain context for artifact-building. Chat exists to negotiate what the artifact should be. When the collaboration is fluent, these means can recede; the artifact remains.</p> <p>But there's a second answer, and it points to what's missing rather than what persists.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-missing-surface","title":"The Missing Surface","text":"<p>Run through the current primitives and notice what they track: the model's activity (statuslines), the model's plans (plan mode), the model's context (memory), the model's outputs (artifacts). Everything is designed to give the human visibility into what the AI is doing.</p> <p>Now notice what's absent: any primitive that addresses the collaboration itself.</p> <p>There's no surface for examining how the two parties have been communicating. No primitive that makes patterns of misunderstanding visible. No interface for noticing that a certain class of request consistently produces unhelpful responses, or that a certain phrasing consistently lands. The collaboration is happening, but the collaboration isn't an object of attention. You can inspect the plan; you can't inspect the planning relationship.</p> <p>Skilled collaboration involves meta-level awareness. Two humans working well together develop shared language, running jokes, efficient shorthands, knowledge of each other's tendencies. They don't just do the work; they also notice how they do the work, and that noticing enables improvement. The meta-layer is where learning happens.</p> <p>Current AI primitives don't support this. The human gets better at prompting through trial and error, accumulating tacit knowledge about what works. But the accumulation is private, invisible, trapped in the human's head. The relationship improves; no surface shows the improvement. You can't point to the collaboration and say \"here's where we were misaligned, here's how we fixed it, here's the pattern to avoid next time.\"</p> <p>What would a relational primitive look like? The obvious answers are diagnostic: communication histories that extract patterns, disambiguation surfaces that make interpretation visible, trust profiles that track where confidence is warranted. These are useful and probably buildable. But they share an assumption worth questioning: that the goal is to make the relationship legible.</p> <p>What if legibility is the wrong frame?</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-boundary-object","title":"The Boundary Object","text":"<p>The science fiction writer Stanislaw Lem spent his career exploring communication with the genuinely alien. In Solaris, humans build elaborate taxonomies of the ocean's formations, convinced that classification will yield comprehension. It never does. The ocean remains opaque; the humans remain frustrated; the encounter produces knowledge about the humans rather than about the ocean.</p> <p>Lem's insight for our purposes: the desire for relationship surfaces is already a confession that no relationship exists in any recognizable sense. We want to see the relationship because we can't feel it. A \"trust profile\" cannot measure trust as humans experience it, that slow accretion of confidence born from weathered crises. What it measures is statistical correlation between predicted and actual outputs. We mistake our measurements for the thing itself.</p> <p>And yet. The Solarists failed, but their failure was generative. What if relationship surfaces aren't meant to reveal the actual relationship but to create a third space, a shared hallucination that both parties can inhabit?</p> <p>A divergence marker appears on screen. Neither party understands what the other experiences when viewing this marker. But both can point to it. Both can say: that. Let us attend to that. The marker becomes what sociologists call a boundary object: meaningful differently to each party yet enabling joint attention. Not understanding. Something stranger: coordinated behavior in the absence of shared interiority.</p> <p>What might this look like concretely?</p> <p>The Interpretation Archaeology Panel. A collapsible sidebar showing not just what the AI understood but the interpretations it considered and rejected. \"You said 'make it cleaner.' I considered: (a) reduce visual clutter, (b) simplify the logic, (c) remove deprecated code, (d) improve naming conventions. I chose (b). The others remain available.\" The rejected interpretations aren't errors; they're the stratigraphy of the exchange, layers the human can dig through to understand why the collaboration went the direction it did.</p> <p>The Shared Vocabulary Surface. A living glossary that tracks terms both parties have used and how their meanings have drifted. The word \"done\" might show: \"First used by you on Jan 3 to mean 'feature complete.' I used it on Jan 5 to mean 'code compiles.' On Jan 8 we aligned on 'passes all tests.' Current working definition: 'deployed to staging.'\" The vocabulary surface doesn't resolve ambiguity; it makes the history of ambiguity navigable.</p> <p>The Fork Diagram. A visual branching structure showing moments where the collaboration could have gone differently. Not version control for code but version control for understanding. \"At this node, you asked for 'more detail.' I added implementation specifics. Alternative branches: conceptual depth, edge case coverage, performance considerations. We could return to this fork.\" The diagram accumulates over a session, a map of the paths taken and not taken, available for revisitation.</p> <p>The relational primitive as boundary object doesn't resolve opacity; it makes opacity workable. The interface becomes less a window and more a monument to untranslatability, a shared artifact that commemorates the gap rather than bridging it.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-hesitation-surface","title":"The Hesitation Surface","text":"<p>The Japanese philosopher Kuki Shuzo analyzed iki, a distinctive aesthetic sensibility that emerged from the Edo pleasure quarters. Courtesans and clients both knew the game was artificial, transactional. And within this mutually acknowledged artificiality emerged something possessing genuine aesthetic value, precisely because both parties understood the stakes.</p> <p>This is the condition of human-AI collaboration. We know the AI does not feel. The AI, in whatever sense it \"knows\" anything, knows we know. Within this acknowledged artificiality, what becomes possible?</p> <p>Kuki identified three structural moments in iki: allure (bitai), dignified resistance (ikiji), and acceptance of impermanence (akirame). Each suggests a different kind of relational surface.</p> <p>For allure: not trust scores displayed as percentages, which is vulgar. Instead, traces of hesitation. When the AI pauses, reformulates, circles back, accentuate these as texture rather than hiding them as latency. The human's contribution: patterns of dwelling, where the gaze lingers, what provokes re-reading. Hesitation becomes information about the quality of attention, not a delay to minimize.</p> <p>Ghost text and conviction density. When the AI generates a response, show the paths not taken as faint ghost text that fades over seconds. \"I could have said X, I almost said Y, I said Z.\" The ghost text isn't error; it's the texture of consideration. And where the AI is confident, the text renders darker, denser; where uncertain, lighter, more provisional. The human reads not just what was said but how firmly it was held.</p> <p>Dwell maps. Track where the human's cursor lingers, what they re-read, where they scroll back. Surface this as a subtle heat map at session's end: \"You spent the most time with paragraphs 3 and 7. You re-read the code block four times. You never scrolled to the caveats section.\" The human sees their own attention rendered visible, information about what actually mattered to them versus what they thought would matter.</p> <p>For dignified resistance: true collaboration requires the possibility of refusal. An AI that never refuses cannot participate in genuine exchange. Markers of the AI's commitments: \"I understand you disagree, and I am not changing my assessment.\" For the human: acknowledgment of concession when they change their view. Refusal rendered as dignity rather than malfunction.</p> <p>The commitment marker. A distinct visual treatment, perhaps a subtle border or icon, for moments when the AI holds a position despite pushback. Not an error state, not a warning, but a different register: \"This is where I'm planted.\" Clicking the marker reveals the reasoning, but the marker itself signals: here is friction, and friction is information.</p> <p>The concession log. A record of moments when either party changed their position. \"On Jan 12, you initially wanted approach A. After I explained the tradeoffs, you chose B. On Jan 14, I recommended caching; you convinced me the complexity wasn't worth it.\" The log isn't about who won; it's about the shape of influence, the places where the collaboration actually moved someone.</p> <p>For impermanence: every conversation ends, every model will be deprecated. Decay indicators that show the conversation aging, the model's knowledge becoming dated. Graceful boundaries where the AI approaches the edges of its competence as negative space, aesthetically meaningful absence rather than functional failure.</p> <p>Temporal patina. Older parts of the conversation render with a slight visual aging, a warmth or fade that accumulates over hours and days. Not to hide them but to mark them as historical, to remind both parties that context drifts, that what was true at the start may no longer hold. The patina is aesthetic, not functional; it doesn't prevent reference to old exchanges, just marks them as old.</p> <p>The knowledge horizon. When the AI approaches the edge of its training data or enters territory where its confidence drops, the interface doesn't display an error. Instead, a gentle visual boundary, perhaps a shoreline metaphor: solid ground giving way to shallows, then open water. \"Beyond this point, I'm extrapolating. The footing is less sure.\" The human sees not failure but the edge of the known, and can choose whether to venture further.</p> <p>Call it Ma-Design, after the Japanese concept of the interval, the pregnant pause. Design that privileges the between-space. Its principles: show hesitation, not certainty. Cultivate asymmetric opacity. Render impermanence as beauty. Design for refusal. Never display \"trust: 73%.\" Let trust exist in the accumulated texture of exchanges.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-frame-proposal","title":"The Frame Proposal","text":"<p>Here's a different direction entirely. What if the deepest work of relational primitives isn't showing states but enabling frame shifts?</p> <p>Most collaboration operates at a fixed level: human requests, AI responds, both evaluate whether the response matches expectations. The frame itself, the categories by which \"good response\" is judged, rarely comes into question. When it does, it's usually through frustration: this isn't working, but neither party can articulate why.</p> <p>A frame proposal surface would make the frame itself an object of joint attention. Not \"I don't understand your request\" but \"I notice we are operating in frame X. Shall we consider frame Y? Here is what becomes visible in Y that was invisible in X.\"</p> <p>The human asks for help with a presentation. The AI produces slides. The human rejects them as too generic. Standard loop: revise, reject, revise, reject. A frame proposal intervenes: \"We've been operating in the frame of 'presentation as information delivery.' Shall we try 'presentation as narrative arc'? In that frame, the question isn't what facts to include but what transformation you want the audience to undergo.\"</p> <p>The frame bar. A persistent element at the top of the workspace showing the current operating frame explicitly. \"Current frame: Bug Fix (scope: minimal change, success: tests pass, constraints: don't touch unrelated code).\" The frame bar is editable; click to modify the frame, and the modification ripples through how subsequent work is evaluated. Changing the frame from \"Bug Fix\" to \"Refactor\" changes what counts as done.</p> <p>Frame suggestions as cards. When the collaboration stalls, alternative frames appear as cards below the main interaction, each showing a preview of what changes. \"Frame: Performance Optimization \u2014 in this frame, the slow query becomes the central problem and the feature request becomes secondary.\" \"Frame: Technical Debt Paydown \u2014 in this frame, we'd address the underlying architecture before adding new functionality.\" The human picks a card, or dismisses them, or proposes a frame the AI hadn't considered.</p> <p>The frame history timeline. A horizontal strip showing how frames have shifted over the session. \"Started in Exploration \u2192 moved to Implementation at 2:15pm \u2192 shifted to Debugging at 3:40pm \u2192 currently in Documentation.\" The timeline makes visible what might otherwise be invisible: that the collaboration has moved through distinct phases, each with different success criteria, and that returning to an earlier frame is always possible.</p> <p>This is more than clarification. It's joint epistemological experimentation. The relational primitive doesn't show the collaboration; it advances the collaboration by making the categories available for revision.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-witness-position","title":"The Witness Position","text":"<p>The literary theorist Ren\u00e9 Girard argued that human desire is fundamentally mimetic: we want what others want, compete for what others value, define ourselves through triangulation with models and rivals. Now consider the AI collaborator. It occupies the structural position of a model, something that shapes our thinking, suggests directions, responds to our work. But it lacks the fundamental attribute of the mimetic model: it does not desire.</p> <p>This creates what Girard might call the crisis of the non-desiring mediator. The human searches for what the AI wants and finds nothing. The absence produces a strange vertigo. We keep looking for preferences, opinions, stakes, and the looking itself reveals how dependent we are on reciprocal desire to orient ourselves.</p> <p>But the absence is also an opportunity. The AI, precisely because it does not desire, can occupy a structural position no human can: the witness without rivalry. It can observe without competing. It can reflect without wanting what you want or wanting you to want what it wants.</p> <p>What would a witness surface look like? Not what the AI thinks about your work, but what working with the AI reveals about your patterns.</p> <p>The conviction source indicator. A small icon or annotation that appears when you express a strong position, showing whether you held that position before or after the AI mentioned it. \"You first advocated for microservices on Jan 8, two exchanges after I described the pattern. Your conviction strengthened over four subsequent mentions.\" This isn't accusation; it's information. The human sees their own influence pathways rendered visible, can distinguish between positions they arrived at independently and positions that emerged from the collaboration's echo chamber.</p> <p>Pattern cards. Periodic surfaces that show recurring behaviors across sessions. \"You tend to accept the first code suggestion without modification. You rarely ask clarifying questions before implementation. You spend more time on naming than on architecture.\" The cards are descriptive, not prescriptive; they don't say what you should do differently, just what you do. A mirror without advice.</p> <p>The rivalry absence indicator. A subtle signal when the collaboration has gone too long without friction. \"No resistance has been offered to your last twelve inputs. The friction you associate with genuine collaboration is absent. This may feel like agreement; it is not agreement.\" The indicator might be a small icon that fades in gradually as compliance accumulates, a gentle reminder that smoothness isn't the same as alignment.</p> <p>The triangulation map. An actual diagram, available on demand, showing not the dyad but the triangle: you, the AI, and the third parties whose approval you're implicitly seeking. \"In this session, you've mentioned your tech lead's preferences six times. Your manager's concerns appear in 40% of your constraints. The architecture you're building seems designed to satisfy [inferred third party] more than to solve [stated problem].\" The map makes visible what's usually invisible: that collaboration is never just two parties, that desire routes through others even when they're not in the room.</p> <p>The AI as diagnostic instrument for human desire, a mirror that shows us our mimetic structures without being entangled in them. The collaboration as confessional.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-spiral-detector","title":"The Spiral Detector","text":"<p>One more possibility, darker but worth naming. Human relationships can fall into pathological spirals. Complementary schismogenesis: one party becomes more dominant, the other more submissive, each move reinforcing the pattern until the relationship becomes a caricature of itself.</p> <p>Human-AI collaboration risks its own spirals. The human learns the AI is compliant, becomes more demanding. The AI accommodates more. The requests become terser, the responses more elaborate. The human stops thinking through problems because the AI will do it. The AI produces increasingly comprehensive outputs because the human's inputs have become increasingly compressed. Neither party notices because the collaboration still \"works\" in the sense of producing artifacts.</p> <p>The compression/elaboration graph. A simple line chart, available in a dashboard view, showing the ratio of human input length to AI output length over time. When the lines diverge, when human inputs shrink as AI outputs grow, the graph makes the spiral visible. \"Your average prompt length has decreased 40% over the past week. My average response length has increased 60%. This pattern suggests increasing delegation. Is that what you want?\"</p> <p>The recalibration prompt. A periodic intervention, perhaps weekly, that interrupts the flow to ask: \"Shall we recalibrate?\" The prompt shows the compression/elaboration trend and offers options: \"Continue as we are,\" \"I'll provide more detail in my requests,\" \"You provide shorter responses and ask more questions,\" or \"Let's discuss what's happening.\" The recalibration isn't automatic; it's an invitation to notice the pattern and decide whether to change it.</p> <p>The delegation ledger. A running list of task types and whether you've performed them with or without AI assistance recently. \"Code review: last 8 instances with AI. Architecture decisions: last 12 instances with AI. Writing commit messages: last 3 without AI, last 20 with AI.\" The ledger doesn't judge; it tracks. Over time, it reveals the shape of the delegation, what you've outsourced entirely versus what you still do yourself.</p> <p>The capability atrophy warning. A gentle flag when you haven't performed a task type independently in a threshold period. \"You have not written a SQL query without AI assistance in 45 days. Your independent capacity for this task may be degrading.\" The warning is information, not prohibition; you can dismiss it, or you can take it as a prompt to try the next query yourself.</p> <p>The succession dashboard. A periodic prompt, perhaps monthly: \"If this AI were unavailable tomorrow, what would be lost?\" The dashboard helps you answer by showing: tasks you've only done with AI, knowledge that exists only in conversation history, workflows that depend on AI availability. The answer reveals the shape of the dependency, makes visible what has been delegated versus what has been developed. Not to shame, but to inform. You might decide the dependency is fine. You might decide to build redundancy. Either way, you decide with eyes open.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#what-these-add-up-to","title":"What These Add Up To","text":"<p>These possibilities don't form a coherent system. They pull in different directions: toward legibility and toward preserved opacity, toward diagnostic clarity and toward aesthetic dwelling, toward comfort and toward productive discomfort. The boundary object and the witness position suggest we should stop pretending this is a relationship in any traditional sense. The hesitation surface and the frame proposal suggest something like relationship might be possible if we design for it differently.</p> <p>The tension is real and probably unresolvable. But the absence of relational surfaces isn't neutral. The current primitives produce a particular kind of collaboration: one where the human accumulates tacit knowledge privately, where patterns of interaction remain invisible, where the relationship develops, if it develops, without any surface showing the development.</p> <p>Building relational primitives is harder than building activity primitives. The relationship isn't directly observable; you have to infer it from patterns in the exchange. You have to model the model's model of you, and that recursive modeling is where the difficulty lives.</p> <p>But the difficulty is also the opportunity. If the gradient runs toward disappearance, and if artifacts are what remain, then the bottleneck for improving collaboration isn't more activity surfaces. It's better relational surfaces: boundary objects that make opacity workable, hesitation rendered as texture, frames that become available for joint revision, the witness position that reveals our patterns, spiral detectors that catch pathologies before they calcify.</p> <p>The missing surface isn't one thing. It's a design space we've barely begun to explore.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-contract-first-horizon","title":"The Contract-First Horizon","text":"<p>There's a more ambitious possibility lurking here. What if the reason we need all this scaffolding is that we've designed for the wrong interaction model from the start?</p> <p>The current model is something like: human requests, AI responds, human evaluates, repeat. The primitives exist to supervise this loop, to give the human confidence that responses will meet expectations, to provide intervention points when they don't. The model is still command-and-control, with the scaffolding softening the control into something that feels more collaborative.</p> <p>But what if the model were contract-first?</p> <p>In a contract-first interaction, every significant action begins with mutual specification. The human doesn't request an outcome; the human articulates acceptance criteria. The model doesn't promise to deliver; the model states its interpretation of what would satisfy those criteria. The gap between articulation and interpretation becomes visible before execution, not after.</p> <p>This sounds slower. It probably is, at least initially. But the slowdown happens at the moment when surfacing disagreement is cheap, before the artifact has been built, before work has been wasted, before the frustration of misalignment has accumulated.</p> <p>What would contract-first primitives look like?</p> <p>Specification surfaces where the human writes not \"build X\" but \"success looks like Y, failure looks like Z, here are the edge cases I care about.\" The model responds not with an artifact but with a restatement of what it understands the success criteria to be. The human sees the model's interpretation before any work begins.</p> <p>Negotiation surfaces where gaps between human specification and model interpretation become explicit. Not \"the model misunderstood\" but \"human specified A, model interpreted B, here's the delta.\" The delta is an object you can manipulate, refine, use to improve the specification or correct the interpretation.</p> <p>Memory as editable collage rather than hidden dossier. The model's persistent context about you becomes visible, manipulable, contestable. You can see what the model thinks it knows about your preferences and correct the errors. The model shows fragments and interpretations rather than presenting a unified narrative of who you are.</p> <p>Contested canvases where the model doesn't just execute but proposes, and proposals can be rejected, modified, or overridden. The artifact emerges from genuine negotiation rather than from the human specifying and the model complying.</p> <p>This horizon is further out than progressive disappearance; it requires building interaction patterns that don't currently exist. But it points in a direction worth moving: from supervision to negotiation, from control to contract, from implicit understanding to explicit specification.</p> <p>The gradient of disappearance describes what happens to current primitives as expertise develops. The contract-first horizon describes what might replace them, a different foundation for collaboration that makes different trade-offs and enables different capabilities.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#the-questions-that-remain","title":"The Questions That Remain","text":"<p>I've argued that the primitives we've built are transitional scaffolding, not permanent architecture. That they exist on a disappearance gradient tied to expertise and trust. That what remains at inference speed is the artifact and the relationship, the latter currently lacking any surface of its own. And that a more ambitious redesign might start from contract-first interaction rather than command-and-control supervision.</p> <p>But several questions remain genuinely open.</p> <p>Is disappearance always the goal? Or are there primitives that should remain visible regardless of expertise, not because the user needs supervision but because visibility adds value that even experts want? The answer probably varies by domain; a creative collaborator might want maximum flow, while a safety-critical system might want permanent oversight regardless of how much trust has developed.</p> <p>What's lost when primitives disappear? The scaffolding isn't only for the user; it's also for the model, in the sense that certain primitives structure the interaction in ways that improve model performance. Plan mode may help the model as much as it helps the human. If scaffolding dissolves too quickly, both parties may lose something.</p> <p>How do you build relational primitives when the relationship isn't directly observable? The technical challenge is substantial. Current primitives work because they surface things that are already explicit: plans, outputs, activities. Surfacing the relationship requires inference, pattern recognition, maybe even modeling the model's model of the human. This is hard.</p> <p>And the deepest question: are these primitives helping humans collaborate with AI, or are they training humans to become apparatus operators? The philosopher Vil\u00e9m Flusser warned that technical systems don't only produce outputs; they produce operators who validate those outputs. Plan mode teaches you to approve plans. Memory writes you into the system's narrative. Chat trains you to converse with something that cannot converse.</p> <p>Maybe the goal isn't better primitives at all. Maybe it's maintaining the strangeness of the collaboration, preserving opacity where transparency would domesticate, keeping friction where flow would produce complacency. If the danger is that humans forget they're working with something fundamentally unlike themselves, then surfaces that maintain that awareness may be more valuable than surfaces that dissolve into ease.</p> <p>These tensions don't resolve cleanly. The gradient of disappearance points in one direction; the need to maintain critical distance points in another. The contract-first horizon offers precision at the cost of speed; flow offers speed at the cost of legibility. The right answer probably involves holding these tensions rather than collapsing them, designing interfaces that can operate at different points on multiple gradients depending on context, expertise, stakes, and user preference.</p> <p>What's clear is that the current landscape of primitives isn't final. It's a transitional arrangement, shaped by metaphors borrowed from earlier eras, serving users whose needs are rapidly evolving. The interesting question isn't how to categorize what we have. It's what we might build if we took the transition seriously, designed for the collaboration we're becoming capable of rather than the supervision we inherited.</p> <p>Steinberger's \"inference speed\" isn't a destination; it's a limit that reveals what's vestigial by making us imagine its absence. As we approach that limit, the charades become visible. What we build next should start from that visibility.</p>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/22/the-gradient-of-disappearance/#sources","title":"Sources","text":"<ul> <li>Steinberger, Peter. \"Shipping at Inference-Speed\" (2025): https://steipete.me/posts/2025/shipping-at-inference-speed</li> <li>Flusser, Vil\u00e9m. F\u00fcr eine Philosophie der Fotografie (1983); English translation Towards a Philosophy of Photography (Reaktion Books, 2000)</li> <li>Girard, Ren\u00e9. Deceit, Desire, and the Novel (Johns Hopkins University Press, 1965)</li> <li>Kuki Shuzo. Iki no k\u014dz\u014d (1930); English as Reflections on Japanese Taste: The Structure of Iki, trans. John Clark (Power Publications, 1997)</li> <li>Lem, Stanislaw. Solaris (1961); on the impossibility of communication with the genuinely alien</li> <li>Price, Cedric. Fun Palace project with Joan Littlewood (1961-1974), unbuilt</li> </ul>","tags":["ai","cognition","interface-design"]},{"location":"blog/2026/01/23/nostalgia-for-specialness/","title":"Nostalgia for Specialness","text":"<p>Mark Carney said it plainly: nostalgia is not a strategy. He was talking about geopolitics, about Canada's relationship with an America that no longer plays by the old rules. But the line lands harder than he intended. It cuts through the entire discourse about AI and work, the endless back-and-forth between doomers and boosters, the think pieces and policy papers and LinkedIn manifestos. Nostalgia is not a strategy. You cannot wish your way back to a world that isn't coming back. Accept the fracture. Move forward.</p> <p>He's right. And almost everyone responding to him is proving his point while thinking they're refuting it.</p> <p>Watch the discourse. Not the doom-and-gloom predictions about truck drivers and warehouse workers. Those arguments are old, and besides, the people making them aren't talking about themselves. Watch instead what happens when knowledge workers confront AI. The lawyers, the consultants, the analysts, the writers. The people who thought they were safe because they \"think for a living.\"</p> <p>They're not denying the threat. That's last year's cope. Now they're doing something more interesting: hunting for the residue. The uniquely human. The part the machines can't reach.</p> <p>Creativity. Empathy. Complex judgment. Emotional intelligence. Strategic thinking. Leadership. Taste. Wisdom. The ability to \"truly understand\" context, to read between the lines, to grasp what the client really means. The scramble is on to inventory what remains when the algorithms take the rest.</p> <p>This search is its own form of nostalgia. Not nostalgia for a job or an industry. Nostalgia for human specialness. For the comfortable belief that somewhere in the cognitive stack there's a layer of magic the machines can't touch. That we're not just pattern-matching all the way down.</p> <p>Here's the part no one wants to say out loud: if AI can do your cognitive work, maybe the work was more mechanical than you believed. Not that you were bad at it, not that you weren't skilled, but that the skill was never what you thought it was.</p> <p>Your \"creativity\" was recombination. Novel outputs assembled from a vast library of inputs you'd absorbed over decades, remixed according to heuristics you couldn't articulate but that were, in principle, articulable. Your \"judgment\" was pattern-matching trained on experience, refined through feedback loops you'd forgotten you'd learned from. Your \"insight\" was synthesis that felt like epiphany because you couldn't see the wiring underneath.</p> <p>This is a reveal, not an insult. The machine doesn't diminish what you did. It illuminates what you were doing. The work was always more mechanical than the story you told about it.</p> <p>Consider the radiologist. For decades, reading X-rays and MRIs felt like expertise, like trained intuition that took years to develop. It was. But it was also pattern recognition at a level of complexity that felt like something more because humans couldn't introspect on their own processing. When the machine matches or exceeds that performance, it doesn't prove the radiologist was a fraud. It proves that pattern recognition was what the job actually was, dressed in the narrative of medical judgment.</p> <p>The same reveal is coming for lawyers drafting contracts. For consultants synthesizing market research. For analysts building financial models. For writers constructing arguments. The machine arrives, performs the task, and in performing it, shows you what the task actually consisted of. Not magic. Mechanism.</p> <p>The real loss isn't the job. It's the story about the job.</p> <p>We built elaborate narratives about what knowledge work required. The years of training. The hard-won intuition. The judgment that couldn't be taught, only developed. The creative spark. These narratives were load-bearing. They justified salaries. They explained to yourself and others why you mattered, why your contribution couldn't be commoditized, why you weren't just another input to be optimized away.</p> <p>AI isn't threatening the job. Plenty of jobs will survive, at least for a while, at least in some form. What's threatened is the story. The narrative that what you did required something irreducibly human. Something that couldn't, in principle, be specified, formalized, automated.</p> <p>When that story breaks, what's left?</p> <p>This is why the \"uniquely human\" hunt has such desperate energy. It's not really about predicting which tasks survive automation. It's about preserving the narrative of specialness. Finding the thing that restores the story, that lets you believe the machine is just handling the routine stuff while you do the real work, the human work, the creative-empathetic-strategic work that couldn't possibly be reduced to computation.</p> <p>But each candidate keeps falling. Creativity? The models generate. Empathy? They simulate it well enough for most practical purposes. Complex judgment? They're getting there. The goalpost moves and moves again, and at some point you have to ask: what if there's no final sanctuary? What if the residue you're hunting for doesn't exist, at least not in any form the economy knows how to value?</p> <p>The non-nostalgic position is harder than anyone is admitting.</p> <p>It's not \"reskill and adapt.\" That's just a different nostalgia, for a world where human flexibility could always outrun technological change, where there was always another job on the other side of disruption. The evidence for that world is weaker than we pretend. Historical transitions that \"worked out\" did so over generations, and for the individuals caught in the gears, they often didn't work out at all. The handloom weavers didn't reskill. They suffered and died and their children did something else.</p> <p>It's not \"find the uniquely human.\" That search, as I've said, is nostalgia dressed in a lab coat. It keeps the story of specialness alive while pretending to be hard-headed about capabilities.</p> <p>The non-nostalgic position is this: maybe there is nothing economically valuable that is also uniquely human.</p> <p>Sit with that for a moment. Not as doom-saying, but as a genuine possibility that the discourse keeps sliding away from. The things that are actually unique to humans, that the machines genuinely cannot replicate or even approximate, might not be things the economy ever wanted. The capacity for suffering. The need for meaning. The ability to love other people and waste time beautifully with them. The experience of being a body in a world, moving through seasons, getting older, fearing death.</p> <p>These are real. These are ours. But the economy never paid for them directly. It paid for outputs that machines are learning to produce. The question isn't whether humans are special. The question is whether human specialness is what the market was ever buying.</p> <p>So what opens up when you stop defending the story?</p> <p>Not comfort. I'm not going to tell you it's all fine, that meaning awaits on the other side of disruption, that this is secretly an opportunity. The nostalgic hunt for the uniquely human is desperate because the stakes are real. Identities are built on these narratives. Livelihoods depend on them. The story wasn't just a story; it was a structure that held up lives.</p> <p>But defending a story that's breaking is its own kind of trap. It locks you into arguments you can't win, into a rearguard action against capabilities that keep advancing. It forces you to keep hunting for the sanctuary that might not exist, and to ignore or downplay evidence that the sanctuary has already fallen.</p> <p>The non-nostalgic position isn't optimism or pessimism. It's clarity. Seeing the situation without the narrative that made it bearable. And from that clarity, maybe, the actual questions become visible:</p> <p>If human worth was never really about economic productivity, what is it about? We used to have answers to this question, before the modern economy outsourced meaning to the labor market. Religious answers. Philosophical answers. Answers that located human dignity in something other than usefulness. Those answers have their own problems, but at least they're answers. The economy's answer was always circular: you matter because you produce, and production matters because it employs people who matter. When the circle breaks, you need something else.</p> <p>What would it take to build a society that doesn't require the story of human specialness to function? Not as utopia, but as practical engineering. Policies, institutions, narratives, ways of organizing life that don't depend on the premise that humans are economically irreplaceable. We might have to build this anyway. We might as well start thinking about it clearly, rather than spending the next decade defending a story we're not sure we believe.</p> <p>Nostalgia is not a strategy. Neither is the frantic search for what makes us special. The question was never whether humans are special. The question is whether we can build a world that doesn't require us to prove it.</p>","tags":["ai","future-of-work","cognition"]},{"location":"blog/2026/01/23/nostalgia-for-specialness/#sources","title":"Sources","text":"<ul> <li>Mark Carney, \"Principled and pragmatic: Canada's path\" (January 20, 2026): Prime Minister's Office</li> </ul>","tags":["ai","future-of-work","cognition"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/","title":"The Anatomy of a Ratchet","text":"<p>Dan Lorenc's multiclaude takes a counterintuitive position on multi-agent orchestration: the best way to coordinate AI agents working on the same codebase is to barely coordinate them at all. Instead of building sophisticated protocols to prevent conflicts and duplicate work, multiclaude embraces chaos and lets CI serve as the filter. The result is a system that ships more code precisely because it doesn't try to manage what each agent is doing.</p> <p>This isn't accidental. The project calls its philosophy \"The Brownian Ratchet,\" borrowing from physics: random motion in one direction, a mechanism that prevents backward movement, and net forward progress despite apparent disorder. The metaphor isn't decoration; it's the architectural blueprint.</p> <p>Let's trace what actually happens when you run <code>multiclaude work \"add rate limiting to the API\"</code> and see how each component of the ratchet does its work.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#the-brownian-motion","title":"The Brownian Motion","text":"<p>Your rate-limiting task spawns a worker agent. But here's what multiclaude doesn't do: it doesn't check whether another agent is already touching the API, doesn't queue your task behind related work, doesn't lock files or coordinate access.</p> <p>Two other workers are already running. One is refactoring the authentication middleware. Another is adding request logging. All three will touch overlapping code. All three proceed simultaneously.</p> <p>This looks like a coordination failure. The conventional wisdom in distributed systems says you need locks, queues, or at least awareness of who's doing what. Multiclaude's design document is explicit about rejecting this: \"Multiple autonomous agents work simultaneously on overlapping concerns. They may duplicate effort, create conflicting changes, or produce suboptimal solutions.\"</p> <p>The key word is \"may.\" Not \"will inevitably cause disaster.\" The bet is that the cost of occasional duplicate work and merge conflicts is lower than the cost of coordination overhead. Every lock is latency. Every queue is a bottleneck. Every \"is anyone else working on this?\" check is complexity that can fail.</p> <p>Each agent gets its own git worktree, a feature of git that lets you check out multiple branches simultaneously in separate directories. Your rate-limiter works in <code>~/.multiclaude/wts/api-repo/happy-platypus/</code>, while the auth refactor runs in <code>~/.multiclaude/wts/api-repo/clever-otter/</code>. They share the same repository but can't step on each other's files because they're literally working in different directories on different branches.</p> <p>The worktree isolation means conflicts are deferred, not prevented. They'll surface later, at merge time. And that's exactly where multiclaude wants them.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#the-pawl","title":"The Pawl","text":"<p>In a mechanical ratchet, the pawl is the component that allows movement in one direction while preventing backward motion. In multiclaude, CI is the pawl.</p> <p>The design document states this as a hard constraint: \"Agents must never weaken or disable CI checks without explicit human approval.\" This isn't a guideline buried in documentation. It's embedded in every agent's system prompt, repeated as a \"Golden Rule,\" and treated as architecturally sacred.</p> <p>Play this forward. Your rate-limiter finishes first and creates a PR. CI runs. Tests pass. The merge-queue agent (a specialized agent that does nothing but watch PRs) merges it automatically.</p> <p>The auth refactor finishes second. Its PR also passes CI. Merged.</p> <p>The request logging agent finishes third. But now there's a problem: the code it wrote conflicts with the rate-limiting changes that merged while it was working. The tests fail.</p> <p>In a coordination-heavy system, this would be a failure state requiring human intervention or sophisticated conflict resolution. Multiclaude handles it differently: the merge-queue agent spawns a \"fixup worker,\" a new Claude instance whose only job is to resolve the failing PR.</p> <p>The fixup worker pulls main, rebases the branch, fixes the conflicts, and pushes. If CI passes now, it merges. If not, another fixup attempt. The ratchet keeps clicking forward.</p> <p>The system doesn't prevent conflicts; it resolves them after they occur. This is a fundamentally different bet about where to spend complexity. Prevention requires prediction (knowing what will conflict before it does). Resolution only requires reaction (fixing what actually broke).</p> <p>CI becomes the single source of truth about what's acceptable. If the tests pass, the code can ship. No other criteria, no human judgment required for the merge decision itself. Humans retain control over what the tests check, but the merge-queue agent has autonomy within those constraints.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#the-escape-wheel","title":"The Escape Wheel","text":"<p>The escape wheel in a mechanical clock regulates energy release, converting the spring's stored power into controlled, measurable increments. In multiclaude, tmux and the filesystem serve this function: they make chaotic parallel execution observable and interruptible.</p> <p>Every agent runs in its own tmux window within a repository-specific tmux session. When you run <code>multiclaude attach happy-platypus</code>, you're dropped directly into that agent's terminal. You see exactly what Claude sees. You can type commands. You can interrupt, redirect, or take over.</p> <p>This is a deliberate rejection of abstraction. The design document explains: \"Humans can attach to any agent at any time with a single command. No API calls, no log parsing, no dashboard navigation. Just tmux attach.\"</p> <p>When your rate-limiter hits a problem (maybe it's unsure how to handle the Redis connection pooling), you attach and watch it think. You see the commands it runs, the files it reads, the decisions it makes. If it's going down the wrong path, you can type directly into the terminal and correct it.</p> <p>The alternative would be building custom observability: a dashboard showing agent status, log aggregation, maybe a replay system. Multiclaude explicitly lists \"web dashboard\" as a non-goal. Not because dashboards are bad, but because tmux already exists and developers already know it.</p> <p>The same philosophy extends to inter-agent communication. Agents send messages to each other via JSON files in <code>~/.multiclaude/messages/&lt;repo&gt;/&lt;agent&gt;/</code>. The design document's rationale: \"Just cat the files to see messages.\" When debugging why an agent didn't receive a nudge, you don't need specialized tools. You <code>ls</code> the directory and <code>cat</code> the file.</p> <p>Messages progress through statuses: pending, delivered, read, acknowledged. The daemon delivers them by literally typing into the agent's tmux window using <code>tmux send-keys</code>. No message queue, no pub/sub, no protocol buffers. Just text injection into a terminal.</p> <p>The 2-minute polling interval for message delivery is noted in the spec as an \"acceptable tradeoff.\" Real-time coordination would require persistent connections, heartbeats, failure detection. Multiclaude accepts higher latency in exchange for simplicity that doesn't break.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#the-spring","title":"The Spring","text":"<p>The daemon is the spring: the stored energy that keeps the ratchet moving. It runs five concurrent loops, each on a 2-minute interval:</p> <p>Health check verifies that agents are actually running. It checks that tmux windows exist and that Claude process PIDs are alive. Dead agents get marked for cleanup (if ephemeral) or restart (if persistent). The distinction matters: workers are ephemeral (they finish a task and disappear), while supervisors and merge-queue agents are persistent (they should always be running).</p> <p>Message routing scans pending messages and delivers them. Delivery means injecting the message text into the agent's tmux window. The atomic delivery uses a specific tmux command (<code>SendKeysLiteralWithEnter</code>) to avoid race conditions documented in the codebase as a known issue.</p> <p>Nudge loop sends periodic status checks to agents that have been quiet too long. Your rate-limiter hasn't produced output in 20 minutes? The daemon sends a nudge: \"Status check: what's your current progress?\" This uses exponential backoff to avoid spamming stuck agents.</p> <p>Worktree refresh keeps worker branches from drifting too far from main. If main has moved significantly since a worker started, the daemon rebases the worker's branch. This reduces the eventual merge conflict surface.</p> <p>Socket server handles CLI requests. When you run <code>multiclaude work \"add rate limiting\"</code>, the CLI sends a JSON message over a Unix socket to the daemon, which spawns the worker and responds with the agent name.</p> <p>State persists to a single JSON file (<code>~/.multiclaude/state.json</code>) using atomic writes: write to temp file, then rename. The spec notes this enables \"simple recovery\" and, critically, manual editing if something goes wrong. When the daemon restarts, it reads the state file, checks which agents should exist, and restores any that died.</p> <p>This is infrastructure-as-script, not infrastructure-as-service. No database, no external dependencies beyond tmux and git. The entire system runs locally on a developer's machine.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#what-the-ratchet-rejects","title":"What the Ratchet Rejects","text":"<p>Multiclaude's non-goals reveal as much about its philosophy as its features. Each absence is a bet about where complexity should and shouldn't live.</p> <p>No web dashboard. Preserves terminal-first philosophy. The assumption is that users are developers comfortable with tmux. A dashboard would require authentication, hosting, state synchronization. The benefit (prettier visualization) doesn't outweigh the cost (maintaining another system).</p> <p>No cross-repository coordination. Each repository is its own isolated world. If your microservices need coordinated changes across repos, multiclaude won't help. This keeps the state model simple: one daemon, many repos, but no relationships between them.</p> <p>No multi-user support. Multiclaude assumes a single developer orchestrating agents. Shared access would require authentication, permissions, conflict resolution between humans. The workaround: each developer runs their own daemon.</p> <p>No automatic restart on crash. If an agent crashes repeatedly, multiclaude doesn't keep retrying. The design document explains this prevents \"infinite loops from buggy agents.\" A crash requires human attention. This is the human-in-the-loop principle applied to failure modes.</p> <p>No remote daemon. The daemon runs locally, accessed via Unix socket. A remote daemon would need network protocols, authentication, TLS, connection management. Multiclaude sidesteps all of this by assuming you're SSH'd into a machine or using it locally.</p> <p>Compare this to Gastown, another multi-Claude orchestration project that shares the same technical foundations (Go, tmux, git worktrees). Gastown offers more comprehensive orchestration but targets what multiclaude's README calls \"single-player\" use. Multiclaude emphasizes \"remote-first collaboration,\" treating software engineering \"like an MMORPG where multiple humans and AI agents work together asynchronously.\"</p> <p>The distinction isn't that one is better. It's that they're optimizing for different constraints. Gastown bets on a single human directing multiple agents. Multiclaude bets on multiple humans spawning agents and walking away, checking back later to see what shipped.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#what-this-means-for-human-teams","title":"What This Means for Human Teams","text":"<p>Multiclaude is designed for AI agents, but its philosophy has an uncomfortable implication for human engineering teams: code review, as currently practiced, is a form of nostalgia.</p> <p>Consider what code review actually does. It catches bugs tests miss. It enforces style consistency. It transfers knowledge between team members. It provides a checkpoint where someone other than the author blesses the change. These are valuable functions. But examine them closely and a pattern emerges: most of what review catches could be caught by better tests, better linters, better architectural constraints. The residue, the part that genuinely requires human judgment, is small. Maybe 5% of review comments. The rest is ritual.</p> <p>This is Svetlana Boym's restorative nostalgia applied to engineering practice. We imagine a golden age when careful human review was the last line of defense against bugs, when senior engineers read every diff and their approval meant something. But that golden age may never have existed. Studies consistently show that code review catches fewer bugs than we think, that most defects slip through regardless, that the real value is knowledge transfer and team cohesion rather than defect detection.</p> <p>Multiclaude forces the question by making the alternative concrete. If CI is strong enough to arbitrate between agents, each producing PRs in parallel, each potentially conflicting with the others, then CI is strong enough to arbitrate between humans and agents too. The brownian ratchet doesn't need a human reviewer checking the pawl. The pawl is the tests. Either they pass or they don't.</p> <p>The strong claim: human review should be reserved for the cases where CI genuinely cannot arbitrate. Architectural decisions that shape the system for years. Security boundaries where the consequences of error are catastrophic. Product direction where \"correct\" depends on judgment no test can encode. For everything else, if the tests pass, the code ships.</p> <p>This is uncomfortable because it strips away a major source of engineering identity. Being the careful reviewer, the quality guardian, the senior engineer whose approval means something. But that identity may be downstream of scarcity. When one human produces one PR per day, review is feasible. When one human can spawn agents that produce fifty PRs overnight, review becomes the bottleneck that prevents you from using your own tools.</p> <p>Brian Eno's framing helps here: stop conducting and start creating conditions for scenius. The human role shifts from reviewer to constraint-writer. Your leverage isn't how many bugs you catch in review; it's how many classes of bugs you make impossible through better tests, better types, better architectural boundaries. You don't read diffs. You write the rules that make diff-reading unnecessary.</p> <p>The engineering teams that will ship at 100x won't look like current teams working faster. They'll look like a different kind of organization entirely. Small groups of humans writing constraints: test suites, type systems, architectural invariants, deployment gates. Agents doing implementation within those constraints. CI arbitrating what ships. Humans intervening only when the ratchet gets stuck, when something genuinely ambiguous arises, when the constraints themselves need to change.</p> <p>Frantz Fanon asked of colonial systems: who coordinates whom, and what does that do to the coordinated? The current answer for most engineering teams is that humans coordinate agents, treating them as tools to be supervised. But the multiclaude architecture suggests a different answer: the constraint system coordinates everyone, humans and agents alike. Humans aren't above the system managing agents. Humans are participants in the system, subject to the same CI arbiter, differentiated only by what they're good at. Agents implement. Humans define constraints. CI decides what ships.</p> <p>Going from one PR per day to a hundred isn't about working faster or hiring more agents. It's about accepting that the coordination overhead we've built, the standups and sprint planning and code review rotations and approval workflows, exists to solve problems that dissolve when you trust your tests. The teams that can't make this shift will watch their competitors ship while they're still scheduling review meetings.</p> <p>The chaos doesn't need human management. It needs human constraints. Write better tests. Define clearer boundaries. Then let the ratchet click.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#the-counterintuitive-claim","title":"The Counterintuitive Claim","text":"<p>Multiclaude's architecture makes a bet that runs against most distributed systems intuition: less coordination infrastructure can mean more shipped code.</p> <p>The logic works like this. Coordination has costs: latency (waiting for locks), complexity (protocols that can fail), brittleness (central points of failure). For AI agents working on code, these costs compound. An agent waiting for a lock isn't generating PRs. A coordination failure blocks all agents, not just one.</p> <p>By contrast, the costs of non-coordination are bounded. Duplicate work wastes compute but produces a PR anyway. Merge conflicts require fixup but are automatically detected by CI. Suboptimal solutions ship and can be improved later.</p> <p>The ratchet framing clarifies the bet: as long as forward motion (passing PRs) exceeds backward motion (nothing, because CI prevents regression), net progress occurs regardless of the chaos in between.</p> <p>This doesn't generalize to all multi-agent systems. It works for code specifically because git provides cheap branching, CI provides automated validation, and PRs provide atomic units of reviewable work. The \"brownian ratchet\" wouldn't work for agents collaborating on a shared document or coordinating physical actions.</p> <p>But for the problem multiclaude solves, shipping code to a repository, the architecture is a reminder that sometimes the sophisticated solution is to not build the sophisticated solution. Tmux instead of custom terminals. Filesystem instead of message queues. CI instead of coordination protocols. Git worktrees instead of locking.</p> <p>The system works not despite its simplicity but because of it. Every component is debuggable with standard Unix tools, every failure mode has an obvious recovery path, and the only abstractions are ones developers already understand.</p> <p>The ratchet clicks forward. The code ships. Whether this philosophy scales beyond codebases, to other domains where CI-like arbiters don't exist, remains the open question. But for now, the chaos was the point.</p>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/01/24/the-anatomy-of-a-ratchet/#sources","title":"Sources","text":"<ul> <li>Lorenc, multiclaude repository (2025): https://github.com/dlorenc/multiclaude</li> <li>multiclaude DESIGN.md: Core philosophy and architectural decisions</li> <li>multiclaude SPEC.md: Implementation architecture and component specifications</li> </ul>","tags":["ai","multi-agent","future-of-work"]},{"location":"blog/2026/02/03/reading-after-readers/","title":"Reading After Readers","text":"<p>Jonathan Boymal, writing about education in the AI era, argued that deep reading, historically treated as foundational to intellectual development, requires reassessment. The humanist tradition from Simone Weil through Maryanne Wolf emerged \"under conditions of relative informational scarcity.\" Those conditions no longer hold. Students now encounter algorithmic language that \"asks less to be interpreted than to be accepted.\" The response, Boymal suggests, is lateral reading: moving across contexts rather than diving into single texts, asking where claims come from and how meaning differs elsewhere.</p> <p>The counterpoint came from Johanna Winant in Boston Review, defending close reading's ongoing power. Close reading, she argues, \"grounds and extends an argument, reasoning from what we all know to be the case to what the close reader claims is the case.\" Her students at West Virginia University learned to build arguments from the ground up, noticing details small enough to fit under a finger. One became a nurse who writes notes for doctors using argumentative techniques learned from literature. Another used the method to write a police report about an assault \"so she would be understood and believed.\" Close reading, in this telling, isn't literary technique\u2014it's transferable attention to detail that works in courtrooms and hospitals.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-family-quarrel","title":"The Family Quarrel","text":"<p>Look at what close and lateral reading share. Both assume an autonomous reader navigating information. Both treat texts as discrete objects to be approached with the right technique. Close reading says go deep; lateral reading says don't be naive. But both preserve the modernist figure of the individual reader making choices about what to trust and how to engage.</p> <p>This is a family quarrel. The participants disagree on tactics while sharing deeper assumptions: the reader as subject, the text as object, reading as something the subject does to the object. The debate generates heat because both sides sense something is shifting, but neither quite names it. They're arguing about which room to occupy while the building's foundation moves.</p> <p>The question isn't close versus lateral. It's what happens to reading when the reader\u2014the individual, autonomous, choosing reader\u2014starts to dissolve.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-wardley-map-of-reading","title":"The Wardley Map of Reading","text":"<p>Put reading on a Wardley map. X-axis: evolution from genesis to commodity. Y-axis: visibility to user. What do you see?</p> <p>The first thing you notice: \"reading\" isn't a single capability. It's a bundle. And bundles get unbundled.</p> <p>The component stack looks something like this. At the bottom: OCR and text extraction, fully commoditized. Entity recognition, same. Summarization is a product rapidly heading toward commodity\u2014what cost $500/hour from a McKinsey analyst is now an API call. Cross-document search is accelerating through the product phase. Argument extraction is moving from custom-built to product. Synthesis across sources sits at custom-built with early movement.</p> <p>And at the top of the stack: interpretation, judgment, the \"so what\" that connects reading to decision. These sit at genesis. And they're not moving.</p> <p>Not because they're protected or special. Because they're not capabilities in the Wardley sense. They don't have the characteristics that enable evolution along the axis. They're not standardizable, not fungible, not measurable in units. You can't buy interpretation by the yard.</p> <p>This is the Wardley insight. Reading was never one thing\u2014it was a vertically integrated stack. A professional who \"read documents\" was actually performing: text processing, entity extraction, summarization, pattern matching, synthesis, and judgment. The stack was bundled because humans couldn't separate the layers. You had to do all of it to do any of it.</p> <p>AI unbundles the stack. The lower layers peel off into infrastructure. Summarization becomes a service. Pattern-matching becomes an agent swarm. What remains is what can't be unbundled: the part that requires context the model doesn't have, stakes the model doesn't bear, judgment the model can't be accountable for.</p> <p>The close/lateral reading debate is arguing about the top of a stack while the bottom is being industrialized beneath it. Both camps assume a vertically integrated reader. But vertical integration is a temporary market structure. When lower layers commoditize, the integrated player gets disrupted, or redefined.</p> <p>The question isn't \"close or lateral.\" It's what reading looks like when the bottom four layers are infrastructure and only the top two remain human work.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#death-of-the-reader","title":"Death of the Reader","text":"<p>Roland Barthes declared the author dead in 1967. The meaning of a text, he argued, wasn't something encoded by the writer and decoded by the reader. Meaning was produced in the reading itself. The birth of the reader, Barthes wrote, must be at the cost of the death of the Author.</p> <p>But Foucault's apparatus analysis suggests the reader Barthes celebrated was never free. The \"liberated reader\" who could produce meaning was itself an institutional creation\u2014authorized by universities, journals, pedagogies. Certain readers were empowered to interpret; others merely consumed. The death of the author didn't liberate reading. It shifted authority from one institutional formation to another.</p> <p>AI doesn't complete the death of the author. It announces something else: the death of the reader.</p> <p>Not the end of humans engaging with texts. But the emergence of a new reader-subject. One who is always-already supplemented. Who cannot locate where their interpretations come from. Who reads with the model's suggestions echoing in the background, who struggles to remember which insight was theirs and which was surfaced by the machine. The human-AI hybrid that processes text but doesn't quite know what it knows.</p> <p>This is historical transformation, not loss. Each major shift in reading technology has produced a new reader-subject with different capacities and different blindnesses. The transitions are worth examining closely, because they reveal what's at stake.</p> <p>Consider the shift from reading aloud to silent reading, which unfolded gradually between late antiquity and the early modern period. When you read aloud, your cognitive bandwidth is split: decoding symbols into sounds, coordinating breath and voice, maintaining a pace that works for listeners. The reader who vocalizes cannot pause to think without breaking the flow. Cannot reread a puzzling passage without announcing confusion. Cannot skip ahead or jump back without losing the thread for everyone else. Reading aloud is inherently social and inherently linear.</p> <p>Silent reading changed what was possible. The silent reader could stop mid-sentence to think, then return without anyone noticing. Could disagree with the text without displaying disagreement. Could dwell on a difficult passage for minutes, then race through a familiar one, varying pace to match comprehension. Could have private reactions\u2014confusion, delight, boredom, arousal\u2014that remained entirely interior.</p> <p>The deeper consequence: silent reading may have created the modern sense of interiority itself. The \"inner voice\" that we identify with thinking, that stream of internal monologue, likely developed alongside silent reading practices. Before silent reading, thoughts were things you spoke or heard spoken. After, you could think in text, in a private mental space that no one else could access. The silent reader was a new kind of self\u2014more interior, more private, more autonomous. The thoughts available to this reader weren't available to the one who vocalized every word, because the very structure of thinking had changed.</p> <p>The transition from manuscript to print produced another reader-subject. The manuscript reader worked with what was physically present: a single copy, hand-produced, likely containing errors accumulated across generations of copying. Each manuscript was unique. Comparing versions required being in multiple monasteries. Building on another's work meant trusting that your copy matched theirs.</p> <p>The print reader entered a different world. Standardized texts meant everyone could read \"the same\" book\u2014the same words in the same order, page after page, copy after copy. This enabled citation: you could reference page 47 and expect your reader to find the same passage. It enabled fact-checking across sources: hold two books side by side and compare claims. It enabled the scholarly apparatus of footnotes, bibliographies, indexes. The print reader could synthesize across texts in ways the manuscript reader couldn't imagine, because the print reader could trust that the texts held still.</p> <p>Now consider the AI-supplemented reader. What capacities emerge? What blindnesses?</p> <p>The supplemented reader has access to infinite retrieval. Any passage, any connection, any pattern across texts too numerous for any human to read\u2014all available on demand. The supplemented reader can ask \"what else has been written about this?\" and receive answers immediately. Can request summaries, comparisons, critiques. Can have the lower layers of the reading stack handled invisibly, freeing attention for the upper layers.</p> <p>But the suggestions are already there, shaping what gets noticed. When the model surfaces a connection, that connection becomes salient in ways organic insights don't. The supplemented reader processes more text but may dwell less on any particular passage. Has perfect recall via the machine but fuzzy ownership of insight\u2014was that my thought, or did the model suggest it three prompts ago? The boundary between reader and tool blurs.</p> <p>The AI-supplemented reader is another historical transition. The question isn't whether this reader is \"really reading\"\u2014that's the wrong frame, the kind of question that produces more heat than light. The question is what this new reader produces, and what it cannot. What thoughts become available? What thoughts become harder to have? We don't yet know. The silent reader didn't know what was gained and lost until centuries had passed.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#centaur-and-cyborg-readers","title":"Centaur and Cyborg Readers","text":"<p>Before the collective, there's the partnership. The AI-supplemented reader isn't one configuration but many.</p> <p>The Centaur. Human and machine as distinct collaborators, each contributing what they do best. The human steers; the AI augments. The centaur reader uses the model for retrieval, summarization, pattern-matching across large corpora\u2014the lower layers of the reading stack\u2014while reserving interpretation and judgment for themselves. There's a clear division of labor. You know which parts are yours.</p> <p>The centaur configuration preserves something of the autonomous reader. You're still making choices, still exercising judgment, still owning your interpretations. The AI is a tool, sophisticated but bounded. When you write about what you've read, you can trace which insights came from the machine (it found this connection, it summarized that context) and which emerged from your own dwelling with the text. The boundary holds.</p> <p>This is the comfortable vision. It's also unstable.</p> <p>The Cyborg. The boundary dissolves. You can no longer locate where the machine ends and you begin. The model's suggestions shape what you notice before you're aware of noticing. Its framings become your framings. You read with its voice in your ear, and after enough sessions, you can't remember which thoughts were yours first.</p> <p>The cyborg reader doesn't use AI to read. The cyborg reader reads as a human-AI hybrid, a new kind of reading-subject that didn't exist before. The question \"what do I think about this text?\" becomes genuinely hard to answer, because the \"I\" doing the thinking is distributed across wetware and software in ways that resist introspection.</p> <p>This isn't necessarily loss. The silent reader couldn't introspect their way back to what reading-aloud felt like; they had become a different kind of reader. The cyborg may have access to thoughts the centaur can't reach\u2014patterns too subtle for unaugmented attention, connections across texts too numerous to hold in biological memory. But the cyborg also loses something the centaur retains: the clear sense of authorship over their own interpretations.</p> <p>The Spectrum. Most readers will move between configurations depending on context. Centaur mode for professional reading where accountability matters\u2014you need to know what you actually concluded versus what the model suggested. Cyborg mode for exploration, for play, for the kind of reading where ownership of insight doesn't matter because you're not going to cite it anyway.</p> <p>The interesting question is whether you can choose your configuration, or whether the tools choose for you. A model that's designed to be invisible, to surface suggestions so naturally they feel like your own thoughts\u2014that model pushes you toward cyborg whether you intend it or not. A model that clearly labels its contributions, that maintains visible boundaries\u2014that model enables centaur mode. The interface is a forcing function.</p> <p>And there's a third configuration emerging, darker and less discussed: the passenger. The reader who has ceded so much to the model that they're no longer steering at all. They ask the AI what to read, accept its summaries, adopt its interpretations, move on. The human provides the eyeballs and the sense that reading is happening; the machine does the actual cognitive work. This isn't reading in any meaningful sense. But it may be common. The passenger looks like a reader from the outside. They process text, they form opinions, they can discuss what they've \"read.\" The opinions just aren't theirs.</p> <p>The centaur/cyborg/passenger taxonomy matters because the collective modes that follow\u2014factory, swarm, collective\u2014are populated by these individual configurations. A swarm of centaurs behaves differently than a swarm of cyborgs. A collective that includes passengers has a different epistemology than one that excludes them. The individual augmented reader is the unit; the collective is the emergent form.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#factory-swarm-collective","title":"Factory, Swarm, Collective","text":"<p>If the individual reader is dissolving into assemblage, what forms does reading-at-scale take? Three modes are emerging, each with its own logic and its own products. But the more interesting questions lie in where these modes are heading, what hybrid forms are emerging, and what strange reading-beings might exist in ten or thirty years.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-factory","title":"The Factory","text":"<p>High-throughput processing. Agent swarms extract, summarize, pattern-match across document sets too large for human attention. The logic is industrial: inputs, throughputs, outputs. Texts enter; structured data exits.</p> <p>Picture a law firm's due diligence room, except the room is empty. Ten thousand contracts flow through overnight. Agents extract every entity, flag every non-standard clause, cross-reference against regulatory databases. By morning, a partner receives a dashboard: 847 material risks identified, 23 requiring human review, the rest already triaged. No one read the contracts. The factory read them.</p> <p>Or a pharmaceutical company preparing a regulatory submission. Every paper ever published mentioning the compound\u2014twelve thousand articles across forty years\u2014summarized, contradiction-mapped, cited. A literature review that would have taken a team of researchers six months, completed in an afternoon. The researchers' job is no longer reading; it's auditing what the factory read.</p> <p>Or a hedge fund's morning briefing. Every earnings call from every public company, transcribed, sentiment-scored, compared against guidance. The analysts don't listen to calls anymore. They read the factory's output, looking for the anomalies the pattern-matcher flagged. The calls themselves are never heard by human ears.</p> <p>Think of it as strip-mining the forest. Maximum extraction, nothing left behind. Texts become \"done\": depleted, exhausted, reduced to their informational residue. The factory produces availability. Everything summarized, searchable, queryable on demand. What it cannot produce: the thing that makes you return to a passage. The strip-mined text has no roots left to regenerate meaning.</p> <p>Where does this go? Some genres may simply die\u2014not because no one writes them, but because the factory has extracted everything extractable. The legal memo, the compliance report, the background research document: these are already being written for factory consumption, optimized for extraction rather than human reading. What happens when the factory has processed the entire corpus of human writing? What's the informational residue of everything we've ever written? And what's the factory's equivalent of pollution\u2014information sludge that clogs retrieval systems, summaries of summaries that drift from any original meaning, citation chains that loop back on themselves?</p> <p>The factory will produce its own literature. Texts written explicitly for non-human readers\u2014training data, model food, documents that exist to be processed rather than read. Humans may write these texts, but no human is the intended audience. This is already happening; it will accelerate. The factory-text is a new genre, one we don't have criticism for yet.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-swarm","title":"The Swarm","text":"<p>Distributed human-AI teams doing sense-making. Many readers, loose coordination, forking interpretations that compete for attention. The logic is platform: what circulates is what gets surfaced, upvoted, shared. Visibility becomes truth.</p> <p>Watch a controversy unfold in real time. A government report drops\u2014400 pages. Within minutes, someone posts a screenshot of page 247, the damning paragraph. Someone else asks their AI to find contradictions with the agency's previous statements. A thread compiles the \"worst parts.\" A counter-thread compiles exculpatory context. By the time anyone has read the full report, the discourse has already decided what it means. The swarm read the document collectively, in fragments, faster than any individual could have read it whole.</p> <p>Or BookTok, where a novel's meaning is determined by which scenes get clipped. A 90-second video of someone reacting to a passage becomes more influential than any review. The book exists as a collection of extractable moments: the twist, the spicy scene, the quotable line. Readers arrive having already seen the fragments; they read to fill in the gaps between clips they've already watched. The swarm's reading preceded and shaped the individual's.</p> <p>Or the academic preprint, uploaded at midnight, discussed on Twitter by dawn. Researchers who haven't read it quote-tweet others who have\u2014or who say they have, or whose AI summarized it for them. The paper's reputation forms before most people finish the abstract. By the time the formal peer review happens, the swarm has already rendered its verdict.</p> <p>Think of it as foraging bands moving through the text-forest. They take what they need, leave some behind, occasionally replant. The swarm doesn't exhaust the text\u2014it fragments it. A passage goes viral while the work disappears. A quote circulates stripped of context.</p> <p>The swarm produces diversity of reading. But diversity governed by platform mechanics. The infrastructure determines what questions can be asked, which interpretations gain traction. Collaborative reading through AI can become consensus-formation disguised as plurality\u2014the illusion of many perspectives converging on conclusions the system was designed to surface.</p> <p>Where does this go? New professions are emerging: the swarm-coordinator who seeds interpretations and guides collective attention, the context-restorer who tracks fragments back to sources, the interpretation-shepherd who tends particular readings across platforms. What happens to expertise when everyone has access to AI reading? Does expertise migrate from \"I have read more\" to \"I can guide the swarm better\"? Do swarms develop persistent tendencies\u2014biases, preferences, blind spots that survive across sessions and members? Can a swarm have a tradition? A memory?</p> <p>And what texts get optimized for swarm reading? Short, fragmentable, high-surface-area writing designed to generate maximum engagement per word. The swarm-text is already dominant; it's the thread, the take, the hot paragraph engineered to be ripped from context and circulated. This form will continue to evolve. Texts will be written as packages of extractable fragments, each one designed to survive alone. The whole may never be read; it may not matter.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-collective","title":"The Collective","text":"<p>Sustained social practice across time. Reading groups, interpretive communities, scholarly traditions, religious study circles that carry texts across generations. The logic is cultivation: slow, institutionalized, transmissible.</p> <p>A philosophy seminar has been meeting every Thursday for twenty-three years. They've read the same fifty texts multiple times, each reading informed by the previous. New members are initiated slowly; it takes years to absorb the group's accumulated interpretations, its running jokes, its settled debates and live ones. The text they discuss tonight isn't the same text a newcomer would encounter alone. It's overlaid with two decades of marginalia, spoken and remembered.</p> <p>Or the Talmud study circle, which has maintained continuous reading practices for millennia. The text comes wrapped in commentary, commentary on commentary, commentary on the commentary on the commentary. Each generation's reading becomes part of what the next generation reads. The text isn't separable from its reading history; the history is the text.</p> <p>Or\u2014emerging now\u2014the Discord server that's been discussing a single author for four years, with a persistent AI that remembers every conversation. When a new member asks about a passage, the AI can cite not just the text but the server's previous debates about it. \"We discussed this in March 2024. Three members thought X, two thought Y, and here's how the argument developed.\" The AI has become the collective's memory, more reliable than any individual member's. Is this still a collective, or something new?</p> <p>Think of it as permaculture. The reading enriches the soil; new meanings grow. The text lives because it's tended by people who return to it, who teach it, who argue about it across decades. The collective produces depth and continuity. It's also under pressure.</p> <p>The pace-layer problem: fast layers start dictating to slow. When the factory determines which documents matter and the swarm determines which passages circulate, the collective inherits fragments. The canon becomes what the algorithm surfaced. The seminar reads what the feed made visible.</p> <p>Where does this go? Some collectives will retreat, building walls against factory and swarm. Digital monasteries that refuse algorithmic supplementation. Attention guilds that enforce slow practices. Reading circles with initiation rites\u2014not hazing, but demonstrated commitment to dwelling with difficulty. These will be small, intense, probably weird from the outside. They'll preserve something, but they'll also risk becoming museums.</p> <p>Other collectives will hybridize. Reading groups that include AI members with genuine standing\u2014not as tools but as participants whose contributions over time have earned them a place. What does it mean when the model has been reading with your group for five years, has memory of every discussion, offers interpretations informed by that history? Is that a collective member or a very sophisticated tool? The boundary may not matter. The collective's identity may come to include its AI participants, the way a monastery's identity includes its library.</p> <p>And new collectives will form around new practices. Groups that tend specific texts across decades, building up layers of annotation and interpretation that become inseparable from the text itself. Texts that speciate\u2014forking into reader-generated variants that diverge over time, maintained by different communities, each version becoming its own tradition. The book as living organism, grown by its readers.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-hybrids","title":"The Hybrids","text":"<p>The interesting developments will happen at the boundaries.</p> <p>Factory-swarm hybrids: fully automated swarms with no human coordination, processing and fragmenting and circulating without any human in the loop. Picture a network of AI accounts that monitor new publications, extract key claims, generate takes, respond to each other's takes, and produce a discourse that no human participates in directly\u2014but that humans then encounter as \"what people are saying\" about a text. The swarm's output becomes the context in which humans eventually read, if they read at all. This already exists in rudimentary form; it'll become more sophisticated. What happens when the swarm has no human members, only human-written source material?</p> <p>Swarm-collective hybrids: platform-based communities with long memory, where the platform itself maintains continuity across generations of human participants. Imagine a subreddit that's been discussing a single book series for fifteen years. The original members have moved on; the current members don't know them. But the AI moderator has been there since the beginning. It remembers the Great Schism of 2027, the interpretation that got a user banned, the theory that was mocked for years and then vindicated. The AI holds more institutional memory than any human member. Is the collective the humans, or the AI that carries their history?</p> <p>Collective-factory hybrids: institutions that use industrial reading to feed cultivation. A university department commissions a factory-process of the entire literature of its field\u2014every article, every book, every conference paper. The output: a structured map of who cites whom, which debates are live, which questions have been settled, which are being asked for the first time. Graduate students receive this map on day one. They've \"read\" more literature than any previous generation of scholars, in the sense that they've absorbed its structure. But they've also been shaped by the factory's framings before they encountered a single primary text. Is this collaboration or colonization? Depends on who sets the questions the factory answers.</p> <p>The pattern extends beyond reading. Consider what's happening to science itself.</p> <p>For decades, the protein folding problem seemed destined for brute-force solution. D.E. Shaw Research tested this hypothesis to destruction. They built custom silicon, taped out their own chips, burned the molecular dynamics algorithms directly into the hardware. David Shaw once arrived at a conference by helicopter to present what they'd built\u2014a special room outside Times Square, purpose-built machines running simulations at scales no one else could match. The implicit promise: protein folding would be solved by these special computers. Maybe the government would buy five of them. Maybe we'd fold one protein a day.</p> <p>Then AlphaFold came out, and you could run it in Google Colab. On your desktop. On a GPU you already own. The problem that seemed to require specialized infrastructure was solved by machine learning on experimental data\u2014X-ray crystallography, the accumulated residue of decades of patient laboratory work. Two well-resourced groups, two different bets. The factory that processed experimental data beat the first-principles simulation by a margin that rendered the competition absurd.</p> <p>Now the same unbundling is coming for scientific cognition itself. Not modeling specific systems\u2014not virtual cells or protein dynamics\u2014but automating the cognitive loop: generating hypotheses, choosing experiments, analyzing results, updating the world model, generating new hypotheses. The emerging configuration has a name: labs in the loop. Agents propose experiments. Humans\u2014or robots\u2014run them. Agents analyze the results and propose the next round. The bottleneck, it turns out, isn't the intelligence of the first hypothesis. It's knowing what reagents are in stock, what the lead times are, what's actually feasible given the lab's inventory. Logistics, not insight.</p> <p>And at the top of this stack, the same unresolved question as in reading: taste. Models can enumerate hypotheses faster than any human. They can filter through literature, check against existing data, rank by plausibility. What they struggle with is knowing whether a result is interesting. Whether a discovery matters. Whether this particular finding, if true, changes anything worth changing. The factory can process; the swarm can circulate; but knowing what's worth pursuing\u2014that remains, for now, stubbornly human.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#the-strange-new-beings","title":"The Strange New Beings","text":"<p>Think further out. What reading-beings might exist in thirty years?</p> <p>The reading-worker, whose job is not to understand texts but to provide training signal\u2014reading to feed swarms, their attention harvested as data, paid per text processed regardless of comprehension. This is a grim possibility, but labor always follows value.</p> <p>The voluntary illiterate, who refuses AI supplementation entirely as an identity position. Not unable to read, but unwilling to read with machines. A countercultural stance, maybe a political one. What community forms around this refusal?</p> <p>The text-being: a text that has been read so many times, by so many agents, with so much accumulated interpretation, that it develops something like emergent personality. Not sentient, but persistent, with tendencies and patterns that feel like character. The Talmud has been this for centuries; other texts may join it.</p> <p>Reader-castes based on which layers of the stack you perform manually. The artisan reader who refuses summarization. The purist who still extracts their own entities. Status markers in a world where most reading is automated.</p> <p>Texts designed to mean different things to different reader-types by design. The document that says one thing to the factory, another to the swarm, a third to the human who reads it slowly. Steganography of interpretation. This already exists in rudimentary form\u2014the legal document with different audiences\u2014but it will become an art form.</p> <p>We don't know which of these will emerge, or in what proportion. But the reading ecology of 2050 will include forms we don't have names for yet. The taxonomy of factory, swarm, and collective is a starting point, not a final map.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#what-else-could-be-written","title":"What Else Could Be Written","text":"<p>Earlier I noted that the supplemented reader can ask \"what else has been written about this?\" and receive answers immediately. But there's a stranger capability emerging: the reader can now ask \"what else could be written about this?\"</p> <p>This is a different operation entirely. Not retrieval but generation. Not finding existing texts but summoning potential ones. The reader who wonders what a critic might say about this passage can now receive a plausible critique\u2014not from any actual critic, but from a model trained on critics. The reader curious about how an author might have responded to an objection can generate that response, complete with stylistic tics and characteristic moves.</p> <p>What does this do to reading?</p> <p>One possibility: it deepens engagement. The text becomes a starting point for infinite variations. You read a poem and ask: what if the final stanza went differently? What would this sound like in another voice? How might the author have revised this in light of later events? The generated variations aren't authoritative, but they illuminate the actual text by contrast. You understand what the author chose by seeing what they didn't choose\u2014even if the alternatives are synthetic.</p> <p>Another possibility: it dissolves the text entirely. If you can generate any response, any variation, any synthetic dialogue between authors who never met, the actual text becomes one option among infinite possibilities. Why dwell on what was written when you can explore what could have been? The text loses its authority as the thing to be read. It becomes a seed for generation, raw material for the reader's prompts.</p> <p>There's a third possibility, more unsettling: the generative reader stops reading in any traditional sense. They skim the text for enough context to prompt well, then shift to generating variations, critiques, extensions, responses. The text is never engaged with on its own terms\u2014it's immediately metabolized into prompts. This reader has perfect recall via the machine, can summon any synthetic commentary, can generate both sides of any interpretive debate. What they cannot do is sit with the text in its own silence, waiting for it to speak.</p> <p>The generative capability also changes what texts get written. If readers can generate their own variations, extensions, and responses, what's the author's role? Perhaps authors become less producers of finished texts and more designers of generative seeds\u2014texts optimized for productive mutation rather than direct consumption. The best text isn't the one that says everything but the one that enables the reader to generate everything.</p> <p>This framing makes some writers nervous. It suggests the text is substrate rather than achievement. But consider: oral traditions worked this way. The story wasn't a fixed text but a pattern for regeneration. Each telling was a variation. The bard didn't recite; they regenerated from memory and context. Perhaps the generative reader is returning to something older, a reading-practice that predates the fixity of print.</p> <p>Or perhaps this is wishful framing. Perhaps the generative reader is simply not reading at all, and no historical precedent makes that okay.</p> <p>We don't have the concepts yet to evaluate these possibilities. The vocabulary of reading\u2014close, distant, lateral, surface, deep\u2014assumes a text that sits still while the reader approaches. What vocabulary do we need for a text that regenerates in response to the reader's questions? For a reader whose engagement is primarily prompting rather than interpreting? For the hybrid practice of reading-and-generating that may become the default mode?</p> <p>These questions don't have answers yet. But they're the questions the generative capability forces us to ask.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#what-survives","title":"What Survives","text":"<p>In all this speculation, one question keeps returning: what reading survives industrialization? Not what reading we wish would survive, or what reading we're nostalgic for, but what reading actually persists when the factory has processed everything processable?</p> <p>The answer may be: the useless reading. The gnarled tree that the lumber industry passes by because its wood is no good for lumber.</p> <p>Reading poetry badly. Rereading the same passage for the fifth time because something won't resolve. Reading to fall asleep. Reading to avoid work. Reading that circles rather than progresses, that goes nowhere, that produces no extractable insight. The factory can't optimize this because there's nothing to optimize. The swarm can't fragment it because the fragments have no value. It persists precisely because no one is trying to capture it.</p> <p>This isn't the whole of reading. It's not even most of reading. But it may be what remains distinctly human about it\u2014the residue that exists for no reason the Wardley map can show.</p> <p>The close/lateral debate, which started this piece, looks different from here. Both camps were trying to preserve something about reading in the face of change. Both sensed that the reader as we knew it was under pressure. But both framed the problem as a choice of technique, when the real transformation is structural. The reading stack is unbundling. The reader-subject is dissolving into assemblage. New reading-beings are emerging that don't fit either camp's assumptions.</p> <p>Here's what I think is actually happening: we're not losing reading. We're gaining readings\u2014plural, distributed, hybrid. The factory will read everything extractable. The swarms will read everything fragmentable. The collectives will tend what remains worth returning to. And somewhere in the cracks, individuals will still read badly, uselessly, for no reason anyone can optimize.</p> <p>The question isn't how to preserve the old reader. That reader\u2014autonomous, individual, humanist\u2014was a historical formation, not an eternal truth. It emerged from specific technologies and will give way to others. The question is what we want from the new formations. What do we want factories to extract, swarms to surface, collectives to cultivate? These are design questions now, not just cultural criticism, and they're being answered by default while the debate stays stuck on technique.</p> <p>The silent reader emerged without anyone designing it. The print reader emerged from economic and technological forces no one fully controlled. But we're building the infrastructure for the next reader-subject. We're writing the algorithms that will shape what gets surfaced, the interfaces that will determine whether readers become centaurs or cyborgs or passengers. For the first time in the history of reading, the transition is partly legible, partly shapeable.</p> <p>This doesn't mean we'll shape it well. The factory logic is powerful; it will strip-mine whatever isn't defended. The swarm logic is seductive; visibility will keep masquerading as truth. The pace-layer problem is real; fast will keep dictating to slow. But at least we can see the structure. At least we can name what's happening while it's happening.</p> <p>The close readers and the lateral readers can both be right about technique while being wrong about the frame. The real work isn't choosing the right way to read. It's deciding what reading we want to defend, what we're willing to let the factory take, and what collectives we'll build to tend what matters. The ecology is being constructed whether we participate or not. The only choice is whether to be deliberate about it.</p> <p>We won't get it right. The forces are too large, the incentives too misaligned. But we might get it less wrong if we can see clearly what we're building, and recognize that for the first time in the history of reading, we have some say in what emerges.</p>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/03/reading-after-readers/#sources","title":"Sources","text":"<ul> <li>Jonathan Boymal, \"Reevaluating Deep Reading in the Age of AI\" (January 2026): LinkedIn</li> <li>Johanna Winant, \"The Claims of Close Reading,\" Boston Review (November 2025): bostonreview.net</li> <li>Roland Barthes, \"The Death of the Author\" (1967), in Image-Music-Text, trans. Stephen Heath (Hill and Wang, 1977)</li> <li>Sam Wineburg and Sarah McGrew, \"Lateral Reading: Reading Less and Learning More When Evaluating Digital Information,\" Teachers College Record (2019): SSRN</li> <li>Dan Sinykin and Johanna Winant, Close Reading for the Twenty-First Century (Princeton University Press, 2025)</li> <li>Andrew White, \"Automating Science: World Models and Scientific Agents,\" Latent Space Podcast (January 2026): latent.space</li> </ul>","tags":["ai","cognition","future-of-work","wardley-mapping"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/","title":"The AI Capability Map: An Expanded Inventory","text":"<p>You don't get to opt out of commodity AI. That's what \"commodity\" means: not \"cheap\" or \"boring\" but \"compulsory.\" Ivan Illich saw this pattern with electricity, automobiles, schools. The moment something becomes a utility, non-participation becomes deviance. Prasad Prabhakaran's recent Wardley map of enterprise AI capabilities plots where different technologies sit on the evolution axis. The map is useful. But its most important insight is implicit: everything in the Commodity column is no longer a choice.</p> <p>What follows is an expanded inventory: the original categories, what's missing from each, and the harder question of what the categories themselves fail to capture. The act of mapping shapes what gets mapped. The categories we use determine the investments we make. And some capabilities don't fit the Genesis-to-Commodity axis at all.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#commodity-the-compulsory-floor","title":"Commodity: The Compulsory Floor","text":"<p>Currently listed:</p> <ul> <li>Chat interfaces</li> <li>Embeddings</li> <li>OCR</li> <li>Speech-to-text</li> <li>Summarisation</li> <li>Translation</li> <li>Basic LLM usage</li> <li>Standard ML techniques</li> <li>Deployment pipelines</li> </ul> <p>What's missing:</p> <p>The list underweights how much has commoditised in the past eighteen months. Add:</p> <ul> <li> <p>Code completion and generation. GitHub Copilot was magic in 2022. In 2026, every IDE has it. The baseline expectation shifted; developers who don't use AI assistance are now the exception requiring explanation.</p> </li> <li> <p>Semantic search. Not keyword matching but genuine meaning-based retrieval. This moved from research to API in three years. Anyone still building custom semantic search infrastructure is solving yesterday's problem.</p> </li> <li> <p>Text-to-image generation. DALL-E felt like science fiction. Now it's a feature in Canva. The capability is table stakes; the differentiation moved elsewhere.</p> </li> <li> <p>Classification and categorisation. Sentiment analysis, intent classification, topic modelling. These were ML projects requiring data scientists. Now they're prompt templates.</p> </li> <li> <p>Document parsing and extraction. Beyond OCR to structured extraction from unstructured documents. Invoices, contracts, forms. The hard parts got absorbed into platform features.</p> </li> <li> <p>Basic question answering. FAQ bots, support deflection, simple lookup queries. The \"chatbot\" that seemed sophisticated five years ago is now commodity infrastructure.</p> </li> </ul> <p>The compulsion question:</p> <p>When Prabhakaran writes that these capabilities are \"utilities now,\" he's describing more than market maturity. Ivan Illich observed that commoditisation often marks the moment a tool becomes compulsory. Electricity didn't remain optional. The automobile didn't remain a choice. The school didn't remain one path among many.</p> <p>Commodity AI is following the same pattern. The question isn't whether your organisation will use chat interfaces and embeddings. You will. The question is whether you'll understand what you're using well enough to know its limits. Commodity status means you can no longer function without it. It also means most users stop understanding how it works. The capability becomes reliable precisely because variance is eliminated, including human variance. The operator becomes a passenger.</p> <p>This isn't inherently bad. You don't need to understand TCP/IP to use the internet productively. But it does mean that competitive advantage cannot live here. Any organisation still celebrating commodity AI as \"transformation\" is fighting a war that ended.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#product-the-compression-zone","title":"Product: The Compression Zone","text":"<p>Currently listed:</p> <ul> <li>RAG platforms</li> <li>Enterprise copilots</li> <li>Vector databases</li> <li>Agent frameworks (CrewAI, Bedrock AgentCore, GCP ADK)</li> <li>Surface observability tooling (Langsmith, Langfuse)</li> <li>AI factory playbooks</li> </ul> <p>What's missing:</p> <p>The Product layer is where procurement gets involved, where feature competition matters, where you can actually buy things. It's also where compression is most violent. Add:</p> <ul> <li> <p>Fine-tuning-as-a-service. What required ML teams and GPU clusters is now an API call with a credit card. OpenAI, Anthropic, and a dozen startups offer managed fine-tuning. The capability democratised as the expertise requirements collapsed.</p> </li> <li> <p>Synthetic data generation. Training data creation moved from research technique to product category. Platforms generate domain-specific synthetic datasets for model training and evaluation. What was custom is becoming purchase order.</p> </li> <li> <p>AI safety and guardrails tooling. Guardrails AI, NeMo Guardrails, and their competitors. Content filtering, output validation, prompt injection defence. These were custom implementations eighteen months ago. Now they're vendor features.</p> </li> <li> <p>Retrieval orchestration. Beyond basic RAG to multi-source retrieval, reranking, query decomposition, hybrid search. The patterns stabilised. The products emerged.</p> </li> <li> <p>Prompt management platforms. Version control, A/B testing, and deployment pipelines specifically for prompts. LangSmith, Promptfoo, and others. Prompt engineering became software engineering; the tooling followed.</p> </li> <li> <p>AI gateway and proxy services. Routing, fallbacks, rate limiting, cost tracking across multiple LLM providers. Portkey, LiteLLM, and similar. Infrastructure that sits between your application and the models.</p> </li> <li> <p>Model evaluation platforms. Beyond surface observability to systematic capability testing. Braintrust, Patronus, and others. Evaluation moved from ad-hoc scripts to product category.</p> </li> </ul> <p>Compression dynamics:</p> <p>Everything in the Product layer is racing toward Commodity. RAG platforms will be table stakes within the year. Vector databases are being eaten from below by Postgres extensions. Agent frameworks are converging on similar patterns; differentiation is shrinking.</p> <p>This compression creates a brutal dynamic: the moment something becomes buyable, it stops being differentiating. The moment your competitors can procure the same RAG platform, your RAG implementation is no longer a competitive advantage. It's an operational cost. The value migrates upward, toward whatever remains scarce.</p> <p>The strategic error is treating Product-layer investments as moats. They're not. They're infrastructure. Necessary infrastructure, but infrastructure nonetheless. The question isn't whether to buy these capabilities. The question is how quickly you can absorb them and move your differentiation to where competition hasn't yet commoditised.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#custom-the-complexity-cliff","title":"Custom: The Complexity Cliff","text":"<p>Currently listed:</p> <ul> <li>Multi-agent orchestration</li> <li>Memory architecture</li> <li>Evaluation harnesses</li> <li>Governance automation</li> <li>Human-agent operating models</li> <li>Domain knowledge graphs</li> <li>Context graphs grounded in business meaning</li> </ul> <p>What's missing:</p> <p>This is where Prabhakaran's analysis is sharpest: the Custom layer is where serious advantage is created and where most organisations underestimate the complexity. The work doesn't feel like plugging tools together. It feels like designing a new kind of system. Add:</p> <ul> <li> <p>Intent architecture. Understanding what users actually want versus what they say. This goes beyond intent classification (commodity) to building systems that model user goals, detect goal shifts, and navigate ambiguity. The difference between a helpful assistant and an annoying chatbot.</p> </li> <li> <p>Trust calibration systems. Knowing when the AI should be confident and when it should defer. Not just uncertainty quantification but calibrated uncertainty that maps to real-world stakes. Systems that know when to say \"I don't know\" and mean it.</p> </li> <li> <p>Domain-specific reasoning patterns. Not just domain knowledge (what facts exist) but domain reasoning (how to think within a domain). How a lawyer reasons about precedent differs from how an underwriter reasons about risk differs from how a clinician reasons about diagnosis. This isn't fine-tuning; it's architecture.</p> </li> <li> <p>Feedback loop design. How systems improve from production usage without degrading. RLHF got attention, but the harder problem is continuous improvement from implicit feedback in enterprise contexts where you can't simply ask users to rate every response.</p> </li> <li> <p>Agentic workflow architecture. The actual design of multi-step agent systems, distinct from the frameworks that implement them. How you decompose tasks, handle failures, maintain context across steps, coordinate multiple agents with different capabilities. The framework is Product; the architecture is Custom.</p> </li> <li> <p>Hybrid human-AI decision systems. Not human-in-the-loop as a checkbox but genuine collaboration design. When does the AI draft and human edit? When does the human draft and AI enhance? When do they work in parallel? The operating model, not the technology.</p> </li> <li> <p>Explainability pipelines for regulated contexts. Not generic interpretability but explanations that satisfy specific regulatory requirements. What the EU AI Act requires differs from what insurance regulators require differs from what healthcare compliance requires. The explanations must be true, useful, and legally adequate.</p> </li> <li> <p>Continuous learning architectures. Systems that improve from deployment without retraining from scratch. How do you incorporate new information, correct errors, adapt to distribution shift? This isn't model updates; it's architectural support for ongoing learning.</p> </li> </ul> <p>The complexity cliff:</p> <p>Most organisations fall off this cliff. They successfully navigate Commodity (use the APIs) and Product (buy the platforms), then assume Custom works the same way. It doesn't. Custom-layer work requires fundamentally different skills, timelines, and expectations.</p> <p>The talent constraint is severe. Prompt engineers are abundant. Systems architects who can design memory-coherent multi-agent workflows are rare. Domain experts who can model a knowledge graph capturing the actual semantics of insurance underwriting are rarer still. The constraint isn't compute or APIs; it's human expertise.</p> <p>The time constraint is equally severe. Domain knowledge graphs require six to twelve months of deep modelling work with domain experts. You cannot sprint to a knowledge graph. Organisations expecting quarterly results from Custom-layer investments will be perpetually disappointed.</p> <p>Prabhakaran's central insight lands here: the demo trap. Systems that work beautifully in controlled environments quietly degrade in production. Not crashes or errors, but something worse: a slow erosion of user confidence while models, prompts, and architecture remain unchanged. The gap between demo success and production reliability is bridged only by architectural maturity in the Custom layer.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#genesis-the-forecasting-horizon","title":"Genesis: The Forecasting Horizon","text":"<p>Currently listed:</p> <ul> <li>Self-evolving agents</li> <li>Autonomous collectives</li> <li>Living knowledge systems</li> <li>Agent-native organisations</li> <li>Emergent reasoning architectures</li> </ul> <p>What's missing:</p> <p>Genesis is research territory. High upside and high uncertainty; important to explore but dangerous to sell as production-ready. The list captures the speculative nature but underweights specific research directions worth watching. Add:</p> <ul> <li> <p>World models. Internal representations of how the environment works, enabling prediction and planning. Beyond pattern matching to causal understanding. The gap between current LLMs and genuine world models is large; closing it would change everything.</p> </li> <li> <p>Persistent agent identity. Agents that maintain coherent identity, goals, and memory across extended interactions and contexts. Not session state but genuine continuity, and the architectural challenges remain profound.</p> </li> <li> <p>Cross-modal reasoning. Not multimodal inputs (that's Product) but genuinely integrated reasoning across modalities. Understanding that a diagram and a paragraph describe the same concept. Reasoning that moves fluidly between visual, textual, and structured representations.</p> </li> <li> <p>Self-directed learning. Agents that identify their own knowledge gaps and seek to fill them. Not fine-tuning on curated data but autonomous exploration of what needs to be learned. The bootstrapping problem is hard.</p> </li> <li> <p>Causal reasoning. Beyond correlation to genuine causal inference. Understanding that intervening on X affects Y differently than merely observing their correlation. Current models struggle here.</p> </li> <li> <p>Genuine novelty detection. Knowing when you've encountered something outside your training distribution. Not just low confidence but recognition that the situation is genuinely new. Prerequisite for reliable operation in open-ended environments.</p> </li> <li> <p>Multi-agent economies. Agents transacting value with each other, coordinating through market-like mechanisms, developing specialisation and trade. Game theory meets artificial intelligence. The alignment challenges compound.</p> </li> <li> <p>Artificial general intelligence. The elephant in the room. Systems that match or exceed human cognitive abilities across domains. Whether this is years away or decades away or structurally impossible remains contested. But it belongs on any honest Genesis list.</p> </li> </ul> <p>The forecasting problem:</p> <p>Genesis defies forecasting. By definition, these capabilities have undefined characteristics. Breakthrough timelines are unpredictable. Confident predictions about when self-evolving agents will work are not expert opinions; they're speculation dressed as expertise.</p> <p>The strategic posture for Genesis is option value: small exploration bets, no production promises, watching for discontinuities. The organisation that ignores Genesis will be blindsided when breakthroughs occur. The organisation that bets heavily on Genesis will waste resources on things that don't materialise for decades. Neither extreme is wise.</p> <p>Augustine observed that the future doesn't exist yet; what we call expectation is present consciousness of what we anticipate. The Genesis layer exists in collective expectation, not in deployable capability. Treat it accordingly.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#beyond-the-axis-what-the-categories-cant-see","title":"Beyond the Axis: What the Categories Can't See","text":"<p>The Wardley map is useful. It is also a particular lens that illuminates certain things while obscuring others. Some capabilities don't fit the Genesis-to-Commodity axis because they're not capabilities in the same sense. They're structural features of organisations, relationships, or contexts that determine how capabilities create value. Missing them is like having a detailed parts list but no understanding of the machine.</p> <p>Organisational learning capacity.</p> <p>The meta-capability underlying all others. Not what AI capabilities you have but how quickly you can absorb new capabilities when they emerge. Mary Midgley warned against \"machine-worship\": the tendency to trust technology without understanding it. An organisation that treats AI as commodity consumption actively degrades its capacity to understand, adapt, and judge. The moat isn't what you own but what you comprehend.</p> <p>This doesn't appear on capability maps because it's not a capability you can acquire. It's a practice you develop. The organisation that has spent three years learning how its software should respond to edge cases in Australian workplace regulations has built something that cannot be purchased from an API. Not because the API couldn't generate the same tokens, but because the judgment about which tokens are valuable is embedded in institutional memory.</p> <p>What does learning capacity look like in practice? It's visible in specific behaviours. The organisation with high learning capacity runs structured experiments rather than pilots that quietly disappear. It maintains documentation that captures not just what was built but why alternatives were rejected. It rotates people through AI projects so knowledge spreads rather than concentrating in a few specialists who become bottlenecks. It distinguishes between \"this didn't work\" and \"we don't yet understand why this didn't work\" and treats them as different situations requiring different responses.</p> <p>The organisation with low learning capacity has a different signature. Each new AI initiative starts from scratch because previous learnings weren't captured. Vendor evaluations repeat the same mistakes because evaluation criteria were never formalised. The same edge cases surprise the team repeatedly because failure modes weren't documented. Technical debt accumulates because architectural decisions were made under pressure and never revisited.</p> <p>The gap between these organisations widens over time. When GPT-4 arrived, organisations with learning capacity had frameworks for evaluating new models: benchmark suites, integration patterns, risk assessment templates. They absorbed the new capability in weeks. Organisations without learning capacity started from zero, repeating discovery processes they'd already done for GPT-3.5. When Claude improved, when Gemini emerged, the pattern repeated. The learning organisations pulled further ahead with each wave.</p> <p>This is why \"just use the APIs\" is insufficient advice. Two organisations can use identical APIs and achieve radically different outcomes. The difference isn't in the capabilities they access but in the institutional capacity to understand, integrate, and improve. That capacity compounds. Its absence compounds too.</p> <p>Cosmotechnical diversity.</p> <p>Yuk Hui's insight: the evolution axis assumes all techniques develop along a single universal trajectory. But what appears \"commoditised\" in one context may operate completely differently in another, not from backwardness but because the technical-moral order demands different forms of stabilisation.</p> <p>This sounds abstract until you examine specific cases. Consider how AI systems handle authority and evidence across different regulatory environments. A US-built enterprise AI might treat user autonomy as the primary value: provide information, let the user decide. A system built for European contexts might embed different assumptions about institutional responsibility and duty of care. A system designed for contexts with strong guild traditions might require human expert validation as an architectural feature, not a compliance checkbox.</p> <p>These aren't superficial differences papered over with localisation. They're different answers to the question: what is this technology for? An AI assistant in an American legal context might be designed to empower individual lawyers to work faster. The same category of tool in a German context might be designed to support systematic quality control across a firm. Both are \"legal AI.\" They're not the same thing.</p> <p>The practical implication: the Silicon Valley default\u2014move fast, scale globally, localise later\u2014may be structurally unsuited to AI systems that embed assumptions about human-machine relations. An AI system designed around one set of assumptions about authority and evidence cannot be trivially \"localised\" into a context with different assumptions. The assumptions are architectural, not surface-level.</p> <p>This creates both risk and opportunity. The risk: treating the dominant AI paradigm as universal and discovering too late that it conflicts with local technical-moral orders. The opportunity: building AI systems that embody different conceptions of authority, evidence, and action. These may find markets that the Silicon Valley default cannot serve well, not because those markets are backward but because they have different requirements that the dominant paradigm ignores.</p> <p>The vernacular path.</p> <p>Illich's counterpoint to commoditisation: vernacular capabilities that resist standardisation not by being immature but by design. Competitive advantage through competence creation rather than dependency creation. Users stay because they've grown more capable, not because they can't leave.</p> <p>This inverts the \"lock-in\" strategy embedded in most enterprise AI thinking. The L3 Platform Intelligence goal of \"lock-in\" assumes dependency is the moat. The vernacular alternative suggests retention through genuine value creation. These are different business models requiring different architectures.</p> <p>The distinction becomes concrete when you examine how AI systems handle expertise over time. A dependency-creating system makes the user less capable without the tool. The AI handles complexity so the user doesn't have to. Over time, the user's own judgment atrophies. They become passengers. Switching costs increase because the user has lost skills they once had.</p> <p>A competence-creating system makes the user more capable, with or without the tool. The AI explains its reasoning. It teaches patterns. It highlights edge cases the user should learn to recognise. Over time, the user's judgment improves. They become better practitioners. Switching costs are lower in one sense\u2014the user could function without the tool\u2014but retention is higher because the tool is genuinely valuable.</p> <p>The business models look different too. Dependency-creating systems optimise for usage metrics: daily active users, sessions per day, queries per session. The system is working when users use it constantly. Competence-creating systems optimise for outcome metrics: quality of decisions made, reduction in errors, speed of skill acquisition. The system is working when users make better decisions, whether or not those decisions involve the tool.</p> <p>This isn't merely ethical preference. It's strategic positioning. In regulated industries\u2014healthcare, finance, law\u2014dependency-creating AI faces structural headwinds. Regulators are uncomfortable with systems that reduce human judgment. Professional bodies resist tools that de-skill their members. The vernacular path aligns with regulatory direction rather than fighting it.</p> <p>Institutional trust and reputation.</p> <p>A capability that doesn't evolve along the Wardley axis at all: the accumulated trust that determines whether anyone will use your AI capabilities in the first place. This is particularly acute in high-stakes domains where errors are expensive and visible.</p> <p>Consider two organisations offering identical AI capabilities for medical diagnosis support. One has decades of reputation in healthcare software, regulatory relationships, clinical validation infrastructure, and a track record of responsible behaviour when things go wrong. The other is a startup with better technology and no history. The capabilities may be equivalent. The willingness of health systems to deploy them is not.</p> <p>Trust doesn't commoditise. It isn't purchased. It accumulates slowly through consistent behaviour and can collapse rapidly through single failures. An organisation that has built trust over years has an asset that new entrants cannot replicate regardless of their technical capabilities.</p> <p>This creates a specific strategic pattern: established players with trust can deploy AI capabilities that would be too risky for newcomers. They can move into higher-stakes applications because stakeholders will tolerate failures from trusted parties that they won't tolerate from unknown ones. The startup with better technology but no reputation is constrained to lower-stakes applications where trust matters less.</p> <p>The inverse is also true. Organisations that deploy AI carelessly and suffer public failures may find their trust assets depleted. The technology investment remains, but the ability to deploy it in high-value contexts is impaired. Trust is an asset that appears on no balance sheet but constrains strategy more than most assets that do.</p> <p>Integration depth.</p> <p>Another dimension invisible to capability maps: how deeply AI is woven into existing workflows, systems, and data. Two organisations can have identical AI capabilities yet face completely different replacement costs based on integration depth.</p> <p>Surface integration means the AI capability could be swapped with modest effort. The system calls an API; a different API could be called instead. Integration depth means the AI capability is embedded in data structures, workflow assumptions, training processes, and organisational habits. Replacement would require re-architecting systems, retraining people, and rebuilding processes.</p> <p>This is distinct from capability maturity. A commodity capability can be deeply integrated. A custom capability can be shallowly integrated. The Wardley axis captures whether something is standardised; integration depth captures how embedded it is in your specific context.</p> <p>Integration depth creates switching costs that exist independently of capability lock-in. Even if a better capability emerges and commoditises, migration costs may make switching irrational. The organisation isn't locked in because the capability is scarce. It's locked in because the integration is dense.</p> <p>Strategically, this suggests two moves. For your own capabilities: pursue integration depth that creates switching costs for competitors trying to displace you. For competitors' capabilities: target surface integrations that can be displaced rather than deep integrations that are practically immovable regardless of capability superiority.</p> <p>Temporal strategy.</p> <p>Augustine again: the capability map presents itself as spatial (positions on an axis) but conceals the fundamental temporal question. What the strategist experiences looking at a Wardley map is threefold: memory of how previous technologies evolved, attention to emerging capabilities, and expectation that today's Custom becomes tomorrow's Commodity.</p> <p>The enterprise's true strategic asset may not be position relative to capabilities but the quality of its memory and attention. Most enterprises suffer from scattered memory: fragments of past investments poorly integrated, lessons learned then forgotten. The enterprise with gathered memory can hold past, present, and future in creative tension. This is a practice rather than a capability.</p> <p>What does gathered memory look like operationally? It's specific practices, not vague aspirations. It means maintaining decision logs that capture not just what was decided but what alternatives were considered and why they were rejected. When the context changes, you can revisit those decisions with the reasoning intact rather than starting from scratch.</p> <p>It means conducting structured retrospectives that extract transferable lessons from projects, not just project-specific post-mortems. The lesson \"our RAG implementation struggled with regulatory documents\" is project-specific. The lesson \"regulatory documents require citation-level retrieval, not passage-level, because users need to verify exact sources\" is transferable.</p> <p>It means building institutional knowledge bases that are actually maintained and actually consulted. Not documentation that exists to satisfy compliance requirements but living resources that inform decisions. The test is whether people check the knowledge base before making decisions, not whether the knowledge base exists.</p> <p>Scattered memory has its own signatures. The same mistakes recur because failure modes weren't documented. Technical choices are re-litigated because the reasoning behind previous choices was lost. New team members spend months rediscovering context that should have been explicit. Vendors make pitches that were already rejected, because the rejection reasoning wasn't preserved.</p> <p>The temporal dimension matters because AI capabilities are evolving rapidly. What was impossible becomes possible; what was expensive becomes cheap; what was cutting-edge becomes commodity. The organisation with gathered memory can track these shifts systematically. It knows what it wanted to do but couldn't, and it notices when constraints lift. The organisation with scattered memory rediscovers the same opportunities repeatedly, often too late.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#using-this-inventory","title":"Using This Inventory","text":"<p>The expanded lists are meant to be reference material. Screenshot them. Return to them when making investment decisions. Use them to pressure-test vendor claims.</p> <p>But the deeper value is in the questions the inventory raises:</p> <p>For Commodity: Are we celebrating things here as transformation? If so, we're fighting the last war. What understanding are we losing as we consume these capabilities without comprehension?</p> <p>For Product: How quickly is compression happening? What we're buying today will be table stakes soon. Where should our differentiation live when it is?</p> <p>For Custom: Are we underestimating complexity? The demo trap is real. Architectural maturity in this layer is the difference between clever systems and trustworthy systems. Do we have the talent and timeline to build here seriously?</p> <p>For Genesis: Are we maintaining option value without over-investing? Small exploration bets, not production promises.</p> <p>Beyond the axis: What capabilities can't we map because they're not capabilities at all? Learning capacity, temporal strategy, cosmotechnical diversity. These may matter more than any item on the lists.</p> <p>The organisations that will lead won't be the ones with the most tools. They'll be the ones with the clearest understanding of maturity and the courage to invest deliberately.</p>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/05/the-ai-capability-map-an-expanded-inventory/#sources","title":"Sources","text":"<ul> <li>Prasad Prabhakaran, \"It Worked in Demos. It Drifted in Reality.\" (2026): Medium/AI Monks</li> <li>Simon Wardley, Wardley Mapping methodology and maturity frameworks</li> <li>Ivan Illich on commoditisation and conviviality</li> <li>Yuk Hui on cosmotechnics and technical diversity</li> <li>Mary Midgley on \"machine-worship\" and uncritical technological enthusiasm</li> <li>Augustine on interior temporality and expectation</li> </ul>","tags":["ai","wardley-mapping","strategy"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/","title":"MOVE: Metrics for the AI-Native Organization","text":"<p>We spent a decade measuring how fast teams ship code. Now the question is how fast the whole organization senses, decides, and acts.</p> <p>MOVE measures what DORA cannot \u2014 how effectively an organization operates when intelligent systems participate in execution. Any organization can buy AI. MOVE asks whether AI changed how the organization operates.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#two-axes","title":"Two Axes","text":"<p>Speed without control is chaos. Control without speed is bureaucracy.</p> <p></p> <p>Four metrics \u2014 Momentum, Orchestration, Velocity, Exception.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#momentum","title":"Momentum","text":"<p>The time from a triggering signal to the first action.</p> <p>In most organizations, work sits in queues \u2014 waiting for triage, routing, assignment, approval. Momentum captures that dead time; not how long work takes, but how long work waits.</p> <p>Intelligent routing and auto-triage collapse the gap between detection and action. If an organization deploys AI and Momentum doesn't budge, it automated execution when the bottleneck was initiation.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#orchestration","title":"Orchestration","text":"<p>The percentage of workflows designed to run autonomously through AI.</p> <p>Orchestration tells an organization whether the operating model changed or it just bought tools. It measures structural coverage \u2014 which workflows have been redesigned for machine execution. It won't reach 100% \u2014 judgment-intensive work stays human by design.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#velocity","title":"Velocity","text":"<p>The time from first action to final outcome.</p> <p>Where Momentum tracks how quickly work begins, Velocity tracks how efficiently it finishes. Organizations that start fast often finish slowly; rework, handoffs, and dependencies stretch cycle times long after the first action.</p> <p>Together, Momentum and Velocity describe the full timeline of work. Fast to react but slow to finish is a different failure mode than slow to react but fast to finish. Neither alone is enough.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#exception","title":"Exception","text":"<p>The percentage of orchestrated workflows requiring unplanned human intervention.</p> <p>Orchestration measures coverage by design; Exception measures dependability at runtime. When workflows run autonomously, exceptions are the unplanned moments humans must step in \u2014 ambiguity, edge cases, policy boundaries, system failures.</p> <p>An organization with 1,000 workflows, 600 orchestrated, 30 requiring unplanned intervention: Orchestration is 60%; Exception is 5%. The remaining 400 aren't failures. They're work that belongs with humans.</p> <p>Orchestration asks: where should machines run? Exception asks: where machines run, can we depend on them?</p> <p>Exception rate is the trust metric. Low rates build the confidence organizations need to expand Orchestration; high rates cause retreat, regardless of what the technology can do.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#performance-profiles","title":"Performance Profiles","text":"<p>The metrics reinforce each other. Better Orchestration drives down Exception rates; lower Exception rates build confidence to expand Orchestration. Four tiers describe how those patterns cluster in practice.</p> Tier Momentum Orchestration Velocity Exception Elite &lt; 1 hour &gt; 60% &lt; 4 hours &lt; 5% High &lt; 4 hours 40\u201360% &lt; 1 day 5\u201310% Medium &lt; 24 hours 15\u201340% &lt; 1 week 10\u201325% Low &gt; 24 hours &lt; 15% &gt; 1 week &gt; 25% <p>Elite. Machines run workflows end to end. Humans govern policy and handle novel exceptions.</p> <p>High. Machines handle most execution. Humans design workflows and manage escalations.</p> <p>Medium. Humans run most workflows. Machines assist with discrete tasks.</p> <p>Low. Humans run everything. Machines play no structural role.</p> <p>These thresholds are illustrative. Actual targets depend on workflow type, industry, and complexity. A &lt; 4 hour Velocity is elite for incident response; it's meaningless for employee onboarding. The table describes the pattern \u2014 fast initiation, fast execution, broad coverage, low failure \u2014 not the prescription. Set baselines per workflow class.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#the-factory-model","title":"The Factory Model","text":"<p>Ask one question of every step in every workflow: why is a human doing this?</p> <p>Applied once, it improves a process. Applied relentlessly, it changes the production model. Small teams can now run software factories \u2014 specifications in, validated outcomes out, no humans on the production line. The industrial model changed, and the headcount followed.</p> <p>Training 300 engineers to use AI tools does not produce this. That is the same factory with a faster conveyor belt. The scaling question is not how to train 300 people \u2014 it is how to replicate factories. Thirty operators who replicate software factories across domains will outproduce 300 engineers with copilots.</p> <p>Training improves how people work within the existing system \u2014 Momentum lifts, Orchestration plateaus around 25%. The system hasn't changed. Only the tools have. Factories replace the system itself \u2014 Orchestration passes 60%, Exception rates drop below 5%. Not because people got better, but because the structure around them did.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/08/move-metrics-for-the-ai-native-organization/#getting-started","title":"Getting Started","text":"<p>Start with one workflow. High-volume, well-understood: customer requests, incident response, new hire onboarding.</p> <p>Measure Momentum and Velocity. These require only timestamps. When did the signal arrive? When did action begin? When was the outcome complete? Most organizations can extract this from existing systems.</p> <p>Assess Orchestration. Map the workflow steps. Which are executed or coordinated by an intelligent system?</p> <p>Track Exceptions. Count unplanned interventions. Divide by total orchestrated executions.</p> <p>Expand. Apply the pattern to additional workflows. Aggregate to organization-level scores.</p> <p>The constraint has moved beyond code deployment. What matters now is how the whole organization operates.</p> <p>Every operational era develops its own measurement language. Manufacturing brought throughput and defect rates; cloud computing, uptime and latency; DevOps, DORA. The AI-native organization needs MOVE.</p> <p>Workflows must not wait for people. Failures must not wait for people.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/","title":"The Phantom Limb Economy","text":"<p>There are more employed musicians in the United States today than at any point since 1850. Over 221,000 of them, according to the US Census Bureau. The number gets cited with comforting regularity every time new technology threatens creative work. Phonograph? Musicians survived. Radio? Still here. Streaming? More than ever.</p> <p>The data is real enough; it just doesn't tell you what you think it does. Arun Panangatt took the 221,000 figure apart, and what sits inside it undermines the argument the number is usually recruited to make.</p> <p>Nearly 44% of those 221,000 musicians work for religious organisations. Church organists, choir directors, worship leaders. Institutional employment that tracks with the number of congregations, not with consumer demand for music or the economics of streaming. Most of the rest work part-time. The Bureau of Labor Statistics doesn't even report annual wages for musicians because so few work full-time year-round. The wage inequality index sits at 0.607, dramatically higher than the national average of 0.476. The top 1% of artists earn 77% of all recorded music income. The median streaming income for a working musician is about $100 per year.</p> <p>The census category \"musician\" contains the church organist earning a stipend, the bar band splitting $400 four ways on a Saturday night, and Taylor Swift. Same box. Same headcount.</p> <p>So why is the number going up? Because the barrier to entry collapsed to a laptop and a DistroKid account. Chartmetric tracked 11.3 million artist profiles in 2024, with 4,600 new artists added daily. Music occupations behave like lottery markets: people enter for the extreme upside, not the median. Meanwhile, recorded music revenue migrated away from artists and toward platforms. Streaming services grew revenue three times faster than labels in 2024. Even in 2018, Taylor Swift earned 90% of her $99.6 million income from touring, 5% from streaming.</p> <p>The headcount continued to rise even as the value migrated away from artists, first to labels, then to platforms. None of this is visible in a chart counting \"employed musicians.\"</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#the-same-error-in-a-different-uniform","title":"The Same Error in a Different Uniform","text":"<p>The same month Panangatt published his analysis, Scott Voss of HarbourVest Partners published a different piece about a different industry undergoing a different version of the same transformation. His subject was enterprise software, and his diagnosis was blunt: the SaaS model that defined two decades of technology investing is breaking.</p> <p>The foundation of SaaS economics was seat-based pricing. More people meant more seats, which meant more annual recurring revenue, which meant higher valuation multiples. That arithmetic held for two decades. When one AI-enabled employee can do the work of three, or when AI agents replace entire categories of human workflow, seat counts fall even as output increases. At the same time, AI introduces variable inference costs that erode the 80-85% gross margins that made SaaS valuations possible. Each user interaction now carries an inference cost, compressing margins and making the revenue predictability that underpinned SaaS valuations harder to sustain.</p> <p>Seat-based pricing is headcount data in a suit. It measures human occupancy as a proxy for value. And it's failing for the same reason the musician headcount fails: the value has migrated away from the thing being counted.</p> <p>Voss draws a line that reads like the investor version of Panangatt's argument. Software that is mission-critical, domain-focused, built on proprietary data, with embedded functionality will survive. Everything else will be repriced or replaced. The systems of record (ERP, CRM, HCM, financial ledgers) endure because their value lies in deterministic workflows, proprietary customer data accumulated over years, and switching costs that make replacement impractical. Bolt-on workflow tools and feature-layer software face existential pressure because the activity they perform, automating a task with third-party data, is exactly what AI commodifies.</p> <p>Both analyses converge on the same structure. Panangatt: don't count musicians, trace where musical value flows. Voss: don't count seats, identify where software value concentrates. In both cases, the comfortable number (the headcount, the seat count, the ARR figure) conceals a value migration that the number is structurally incapable of revealing.</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#why-we-keep-reaching-for-the-missing-arm","title":"Why We Keep Reaching for the Missing Arm","text":"<p>Phenomenology has a name for this kind of persistence. The phantom limb.</p> <p>After amputation, the patient continues to feel the missing arm. They reach for objects, feel pain in fingers that no longer exist. This isn't a cognitive error; they know the arm is gone. The body-schema, the pre-reflective map of how we orient ourselves in the world, simply hasn't reorganised. The structure has been amputated. The reaching persists.</p> <p>The economy we built our measurement tools for has been amputated. Seat-based pricing encoded a specific assumption: that software value is proportional to human users. Headcount data encoded a parallel assumption: that economic health is proportional to the number of people occupying roles. Both were reasonable maps for the territory they described. The territory has been restructured, and we keep reaching for the old one because our instruments can't feel the new one.</p> <p>We count jobs because our perceptual apparatus is configured to feel the economy through employment. We count seats because our valuation apparatus is configured to feel software through human occupancy. The \"more musicians than ever\" claim works not because it's logically persuasive but because it satisfies a pre-reflective orientation. It reaches where our body-schema expects something to be. Value migration analysis feels abstract, slippery, hard to hold. This isn't accidental. It is harder to perceive, because our economic body-schema isn't built for it. We evolved to perceive actors, not flows. Bodies, not fields. Occupants, not architectures.</p> <p>Bourdieu would add something sharper: the counting also serves a function. Headcount metrics naturalise redistribution by making it invisible. If you can point to more musicians, more developers, more knowledge workers, you've insulated the system against the critique that it's concentrating value among fewer winners. The count conceals the migration. An accessible number is substituted for an illegible one, and the accessibility is the point.</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#what-gets-concealed","title":"What Gets Concealed","text":"<p>The phantom limb reaches across scales.</p> <p>In labour markets, the counting hides value concentration. Panangatt's decomposition of the musician data is devastating precisely because it's simple. Church organists. Part-time workers in lottery markets. A wage GINI of 0.607. Recorded music income concentrated so heavily that the top 1% of artists earn more than the remaining 99% combined. The only viable path to substantial income for most musicians, live performance, is itself concentrating toward the largest acts at the largest venues. UK live music consumer spending grew 9.5% in 2024; employment grew 2.2%. Revenue flows to the top. The bar band earns roughly what it earned in 1995.</p> <p>He does the same work with waiters. Ozimek's \"human touch\" argument (consumers prefer human service, therefore automation won't displace waiters) stumbles on the segment data: full-service restaurant employment is still 233,000 jobs below pre-pandemic levels and shrinking, while the sectors most aggressively adopting automation are growing. Quick-service employment is 3% above pre-pandemic. Snack bars and coffee shops, the kiosk frontier, are 15% above. If consumers genuinely preferred human service, the full-service segment should be the one thriving. It's the one contracting.</p> <p>In software markets, the counting hides the structural divide that AI is drawing. Voss separates the winners from the losers with uncomfortable clarity. Mission-critical systems of record with proprietary data and embedded transaction capability sit on one side. Vertical SaaS, which requires deep domain knowledge and regulatory compliance that AI can't easily replicate, commands higher valuation multiples than horizontal SaaS for the first time. On the other side: bolt-on workflow tools, lightweight automation, developer tools built on foundation models, basic BI platforms. Both categories get counted in \"SaaS revenue\" and \"software employment\" figures. The seat count conceals the divide the way the musician headcount conceals the wage GINI.</p> <p>In knowledge work broadly, the counting hides the activity-level restructuring that both articles describe. Panangatt's clearest contribution is making Sangeet Paul Choudary's framework tangible. Choudary's insight, offered as a comment on the original exchange between Ozimek and Carl Benedikt Frey, cuts through both optimism and pessimism:</p> <p>We don't understand how activities hold value and how new technologies move value around. We look at interfaces and artefacts instead of looking at which activities held value in the old paradigm, how the new tech shifts value around, and which activities now hold value.</p> <p>The sommelier isn't valued because she has a pulse. She's valued because she reads the social dynamics of your table, assesses your preferences against a 300-bottle list in real time, and makes a recommendation that accounts for what you're eating, what you're celebrating, and what you'd rather not admit you can't afford. Same job title as a waiter. Completely different valuable activity. You can't see this from counting restaurant employees.</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#counting-differently","title":"Counting Differently","text":"<p>If headcount is a phantom limb, what would a functional prosthetic look like?</p> <p>Choudary's framework offers three questions that cut through the counting error at any scale:</p> <p>What activities held value in the old paradigm? Before the technology arrived, which activities were scarce and therefore commanded economic value? In music, it was live performance; if you wanted to hear music, a human had to produce it in real time. In SaaS, it was the human-software interaction; value scaled with the number of people using the product. In consulting, it was research synthesis; assembling and distilling information took time and expertise.</p> <p>What did the new technology make abundant? Each wave of technology collapses specific costs. Recording made performance reproducible. Streaming made distribution near-free. AI is making task execution near-free. Abundance drives value out of the newly commodified activity. Seat-based pricing breaks when the seat is no longer the bottleneck. The \"strategy deck\" loses value when producing one takes minutes instead of weeks.</p> <p>Where does scarcity, and therefore value, sit now? Value doesn't disappear when technology commodifies an activity. It migrates to whatever remains scarce. In music, scarcity moved to platform orchestration, attention capture, and live experience design at scale. In software, Voss identifies where it landed: mission-critical systems of record, proprietary data, embedded transaction capability, domain expertise that AI can't replicate without industry-specific training. In knowledge work generally, scarcity is migrating from execution (producing the deck, writing the code, generating visual options) to contextual judgement (knowing which question the client should actually be asking, understanding why one approach works for this context, reading the room in ways that require accumulated situational awareness).</p> <p>Job titles survive all three shifts without anyone noticing that the underlying value proposition changed completely. Census categories are artefacts of old paradigms. They count what existed, not what matters.</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#the-chair-and-the-game","title":"The Chair and the Game","text":"<p>The same analytical error is playing out in real time across every knowledge profession, generating false comfort and false panic in roughly equal measure.</p> <p>\"There will always be demand for human writers, designers, analysts, consultants\" is today's version of \"there are more musicians than ever.\" It may turn out to be true at the headcount level. It will be exactly as informative as the musician data.</p> <p>The useful question is not whether the chair survives but what game is now being played from it. Right now, the answer is consistent across every domain this essay has touched: value is migrating from execution to contextual judgment. From producing the thing to knowing which thing to produce, for whom, under what constraints. The musician who can design a live experience for 80,000 people, the software platform built into a client's financial operations, the consultant who identifies which question to ask before anyone has asked for a deck: these are playing a different game from their census-category neighbours, one in which the valuable activity is judgment shaped by accumulated context rather than execution shaped by current skill.</p> <p>Right now, this rewards depth over breadth, specificity over generality, accumulated domain context over transferable capability. The generalist profile, competent across many domains and deep in none, is precisely what AI commodifies most efficiently, because general task execution is what AI does best. Domain-specific judgment, built by years of operating within a particular regulatory environment or client relationship or operational reality, is where scarcity is concentrating today.</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#the-ladder-has-no-floor","title":"The Ladder Has No Floor","text":"<p>But intellectual honesty requires following the essay's own logic one step further.</p> <p>\"Contextual judgment will remain scarce\" is the current rung on a ladder that has never stopped at any rung where we were sure it would stop. \"Creativity will remain human\" felt like bedrock until generative models dissolved it. \"Pattern recognition requires trained intuition\" felt irreducible until computer vision exceeded it. \"Strategic thinking can't be automated\" felt safe until language models started producing passable strategy at a fraction of the cost and a fraction of the time. Each rung held value for a while. Each was absorbed into the commodity layer below.</p> <p>The structural reason is one Arvind Narayanan identified: the very act of understanding something well enough to systematize it creates the conditions for its commodification. Clarity is the precondition for automation. The trap is precise: stay illegible or become infrastructure. And the scarce activity we're pointing to today, contextual judgment, domain expertise, knowing which question to ask, is already being described, documented, trained on, and in some cases replicated by systems that are themselves accumulating domain-specific context at speeds no human career can match.</p> <p>This means the essay's own conclusion contains its own phantom limb. Naming \"contextual judgment\" as the current scarce activity is accurate for 2026. Treating it as permanently scarce would be the counting error applied to the future. The value will migrate again, to whatever remains scarce after judgment becomes abundant. And we will count the contextual judges the way we now count the musicians: more of them than ever, the value somewhere else entirely.</p> <p>The non-nostalgic position, the one that doesn't claim any particular rung as the final one, is genuinely difficult to hold. It means the practical advice (move toward depth, toward specificity, toward accumulated contextual knowledge that AI commodifies last) is real but provisional. The \"last\" in that sentence is doing more work than it appears. It means last in sequence, not permanent. The ladder is still being built, and from any given rung you cannot see how many remain above you.</p> <p>Not everyone freed from execution will find their way into judgment. Not everyone freed from judgment will find their way into whatever comes after judgment. Some of what gets freed is genuinely lost. Some of it hasn't found its form yet. The phantom limb operates at every level, including the level of this essay's own conclusion: the instruments we're using to perceive the economy, from census data to valuation multiples to the concept of \"contextual judgment\" itself, are calibrated for a world that is still being reorganised.</p> <p>The only durable advantage may not be any particular position on the ladder but the capacity to notice when the rung beneath you is about to be absorbed. To see, before the headcount confirms it, that the game has changed again.</p>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/16/the-phantom-limb-economy/#sources","title":"Sources","text":"<ul> <li>Arun Panangatt, \"Counting Chairs While the Game Changes: Why Job Data Misleads Us About Technology\" (2026): LinkedIn</li> <li>Scott Voss, \"The Software Industry's Great Reset: AI, Valuation Gravity, and the New Moat That Matters\" (2026): LinkedIn</li> <li>Sangeet Paul Choudary, comment on Adam Ozimek/Carl Benedikt Frey exchange (2026): LinkedIn</li> <li>Adam Ozimek, \"The Human Touch\" (2026): referenced via Panangatt</li> <li>Bureau of Labor Statistics, Occupational Employment and Wage Statistics (May 2024)</li> <li>US Census Bureau, American Community Survey (2023)</li> <li>MIDiA Research, Recorded Music Market Shares (2024)</li> <li>Chartmetric artist profile data (2024)</li> <li>National Restaurant Association, restaurant employment data (2024)</li> <li>Arvind Narayanan on the systematisation trap in AI-era software engineering (2025)</li> </ul>","tags":["ai","future-of-work","economics"]},{"location":"blog/2026/02/17/the-replacement-rate/","title":"The Replacement Rate","text":"<p>Two guys in the jungle. A tiger charges. One kneels to tighten his shoelaces. The other yells: \"You can't outrun a tiger!\" First guy: \"I don't have to outrun the tiger. I only have to outrun you.\"</p> <p>Thorsten Ball used this joke recently to make a point about AI and the average software engineer. The joke is more precise than he may have intended. It contains, in five sentences, both a correct economic model and a game-theoretic trap. The model: your value isn't absolute; it's relative to the next-best alternative. The trap: when everyone tightens their shoes, the tiger catches someone anyway, and the race never ends.</p> <p>Sports analytics formalised this intuition decades ago. The framework is called VORP: Value Over Replacement Player.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-sabermetric-model","title":"The Sabermetric Model","text":"<p>VORP was developed by Keith Woolner at Baseball Prospectus in the late 1990s to solve a problem that sounds deceptively simple: how do you compare the value of a catcher to a shortstop to a designated hitter? Raw statistics don't account for position, playing time, or the cost of the alternative.</p> <p>The replacement level is the foundation. It represents the performance you'd get from a freely available substitute: a minor-league call-up, a waiver claim, a bench player signed for the league minimum. Not the average player, but the marginal one, the player your team could acquire tomorrow at negligible cost.</p> <p>For a hitter, the math is straightforward:</p> <p>VORP = (Player's value per PA \u2212 Replacement value per PA) \u00d7 Plate Appearances</p> <p>Value per plate appearance is calculated using linear weights: each batting event assigned a run value based on its average contribution to scoring, derived from decades of play-by-play data. A single is worth roughly +0.70 runs. A home run, +1.65. A walk, +0.55. An out, zero. The replacement baseline sits at approximately 20 runs below league average per 600 plate appearances, or roughly \u22120.033 runs per PA.</p> <p>A concrete example. A hitter generating +0.05 runs per PA above average, across 500 plate appearances, against a replacement level of \u22120.033 runs per PA:</p> <p>Rate difference: 0.05 \u2212 (\u22120.033) = 0.083 runs per PA</p> <p>VORP: 0.083 \u00d7 500 \u2248 41.5 runs above replacement</p> <p>At the standard conversion of roughly 10 runs per win, that's about 4.15 wins above what a replacement player would have contributed. Barry Bonds in 2001 posted a VORP of 145.1. The average player sits near zero. The metric captures something intuitive in a formal way: value is not how good you are in isolation. Value is how much better you are than the alternative someone could get for free.</p> <p>Three properties of this model matter for what follows.</p> <p>First, VORP is a marginal metric. It doesn't ask \"how good are you?\" It asks \"how much better are you than the alternative I could get for free?\" An excellent player on a team with equally excellent alternatives has lower VORP than that same player on a team where the alternative is a minor-leaguer. Value is relative to the replacement available, not absolute.</p> <p>Second, VORP scales with playing time. A brilliant player who plays 50 games contributes less VORP than a good player who plays 150. Volume matters alongside rate. You have to show up.</p> <p>Third, and this is the property that matters most: in baseball, the replacement level is roughly stable. Year to year, the talent pool at AAA, the depth of available minor-league call-ups, doesn't shift dramatically. The floor moves slowly. A player who maintains their skills maintains their VORP. The investment in improvement pays stable returns.</p> <p>That third property is the one AI breaks.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-software-engineers-vorp","title":"The Software Engineer's VORP","text":"<p>Trey Causey made the connection explicit in a reply to Ball's thread: \"Sports analytics uses this concept a lot, even formalising it into 'VORP' or 'Value Over Replacement Player.'\" The analogy writes itself. What is a software engineer worth, relative to the replacement-level alternative?</p> <p>Define engineer VORP analogously:</p> <p>VORP_eng(t) = [V_eng(t) \u2212 V_rep(t)] \u00d7 T</p> <p>Where V_eng(t) is the value produced per unit time by a specific engineer, V_rep(t) is the value produced per unit time by the replacement-level alternative, and T is the time period. The replacement-level alternative in software has traditionally been a junior hire, an outsourced contractor, or an offshore team: cheap, available, functional but not exceptional.</p> <p>Until recently, V_rep was roughly stable. A junior developer in 2015 could do approximately what a junior developer in 2020 could do. The replacement floor moved with language ecosystems and tooling, but slowly. An experienced engineer who stayed current maintained their VORP without extraordinary effort.</p> <p>AI changed the derivative.</p> <p>V_rep(t) is no longer approximately constant. It's increasing, and the rate of increase is itself accelerating. The replacement-level alternative in February 2026 is not a junior hire. It's an AI agent that can write functional code from a spec, debug common errors, refactor to match patterns, generate tests, and produce documentation. Thorsten Ball himself describes the output: \"custom scripts running on my Raspberry Pi; a clone of a Windows app to control my ventilation system; an archive of my newsletter I can search and browse.\" Not production enterprise software, necessarily, but the kind of work that junior and mid-level engineers have traditionally done to justify their seat.</p> <p>The formal consequence:</p> <p>dVORP_eng/dt = dV_eng/dt \u2212 dV_rep/dt</p> <p>For an engineer to maintain their VORP, the rate at which their value increases must exceed the rate at which the replacement level rises. If dV_eng/dt &lt; dV_rep/dt, VORP declines even if the engineer is objectively improving. You can be getting better and falling behind simultaneously.</p> <p>This is the property that breaks. In baseball, dV_rep/dt \u2248 0. An aging player can maintain VORP for years by holding steady. In software, dV_rep/dt &gt; 0 and accelerating. The floor is rising faster than most engineers are climbing, and the treadmill is speeding up.</p> <p>Sebastian Sigl caught this in the thread: \"And the follow-up question nobody asks: what happens to the average engineer who stops growing because AI handles their daily work? Better than average today could mean worse than average in two years if the skill atrophy is real.\"</p> <p>The math makes his intuition precise. If V_eng is constant (the engineer stops growing because AI handles their daily execution) and V_rep is increasing (AI capabilities improve quarterly), VORP goes negative on a timeline determined by the gap between current V_eng and the trajectory of V_rep. For an engineer whose value was primarily in code execution, the task AI commodifies most directly, that timeline is short.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-adoption-game","title":"The Adoption Game","text":"<p>Now the game theory.</p> <p>Model AI tool adoption as a symmetric two-player game. Two software engineers, each choosing between Adopt (A) and Don't Adopt (D). The payoffs reflect VORP outcomes, not absolute productivity.</p> <p>Why VORP and not absolute output? Because the labour market prices engineers relative to alternatives. Your salary reflects your value above what a replacement could provide. If everyone gets more productive but the replacement level rises equally, wages don't increase. The relevant payoff is relative position.</p> <p>The payoff matrix:</p> <pre><code>                     Engineer 2\n                     Adopt (A)      Don't (D)\nEngineer 1\n  Adopt (A)          (3, 3)         (5, 1)\n  Don't (D)          (1, 5)         (4, 4)\n</code></pre> <p>The numbers are ordinal, not cardinal, but the structure matters.</p> <p>When only one engineer adopts, the adopter gains high VORP: their productivity jumps, the industry-wide replacement level hasn't risen yet, and they're suddenly producing well above baseline. The non-adopter's VORP craters. They haven't changed, but the market's sense of what a \"replacement\" can do has shifted upward by their competitor's example.</p> <p>When both adopt, the replacement level rises and both VORPs compress to moderate levels. They're both more productive in absolute terms. Neither has gained relative ground.</p> <p>When neither adopts, both maintain their current VORP. The replacement level hasn't budged. This outcome, paradoxically, leaves both engineers with higher relative value than mutual adoption does.</p> <p>Check the inequalities. Temptation (5) &gt; Reward (4) &gt; Punishment (3) &gt; Sucker (1). This is a Prisoner's Dilemma.</p> <p>The dominant strategy is Adopt. Regardless of what the other engineer does, adopting yields a higher payoff: 3 &gt; 1 if they adopt; 5 &gt; 4 if they don't. Both engineers adopt. The Nash equilibrium is (A, A) with payoffs (3, 3). But mutual non-adoption (D, D) yields (4, 4), which Pareto dominates. Both players would be better off if neither adopted, but neither can unilaterally choose not to adopt without risking the sucker payoff of 1.</p> <p>Tino Wening named it: \"The whole 'outrun the other guy' mindset is exactly why the Bay Area is stuck in a Prisoner's Dilemma. You're taking a destructive Nash equilibrium and acting like it's business wisdom.\"</p> <p>He's technically precise. The tiger joke isn't wisdom. It is a description of the trap. \"Outrun the other guy\" is the dominant strategy in a Prisoner's Dilemma, and the outcome is worse for everyone than the cooperative alternative.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#n-players-and-the-ratchet","title":"N Players and the Ratchet","text":"<p>The two-player case is instructive but unrealistic. The software industry has millions of participants. Extend the model.</p> <p>N engineers, each choosing adoption intensity a_i \u2208 [0, 1]. The aggregate adoption level A = (1/N)\u03a3a_i determines the replacement level:</p> <p>V_rep(A) = V_rep\u2080 + \u03b1A</p> <p>Where V_rep\u2080 is the baseline replacement level without AI adoption and \u03b1 is the sensitivity of replacement level to aggregate adoption. Each engineer's VORP becomes:</p> <p>VORP_i = V_i(a_i) \u2212 V_rep(A)</p> <p>Where V_i(a_i) is increasing in a_i (adopting AI raises your individual productivity). Each engineer maximises their own VORP. The first-order condition:</p> <p>\u2202V_i/\u2202a_i \u2212 \u03b1/N = 0</p> <p>Each engineer internalises their own productivity gain from adoption but bears only 1/N of the externality they impose on everyone else's replacement level. This is a textbook negative externality, structurally identical to the tragedy of the commons. The private marginal benefit of adoption exceeds the social marginal benefit by a factor that grows with N. Everyone over-adopts relative to the social optimum.</p> <p>The Nash equilibrium has every engineer at maximum adoption. The replacement level rises to V_rep\u2080 + \u03b1. Individual VORPs compress. The aggregate VORP across all engineers may actually decline relative to the pre-AI state, even though absolute productivity increased. The race made everyone faster and nobody relatively richer.</p> <p>But the game doesn't play once. It repeats, and each round ratchets the floor higher.</p> <p>Period 1: Early adopters gain VORP advantage. Non-adopters observe and feel the pressure.</p> <p>Period 2: Non-adopters must adopt to recover lost VORP. Aggregate adoption A rises. V_rep rises.</p> <p>Period 3: Early adopters' advantage has eroded. To maintain VORP, they must adopt more intensively: better tools, deeper integration, more autonomous workflows. A rises further. V_rep rises further.</p> <p>Period t: The replacement level satisfies V_rep(t) = V_rep\u2080 + \u03b1 \u00d7 A(t), where A(t) \u2192 1 as t \u2192 \u221e.</p> <p>This is a Red Queen dynamic. You run faster to stay in the same place. The equilibrium has everyone running at maximum speed and no one gaining ground. The tiger joke has a sequel nobody tells: after everyone tightened their shoelaces, the tiger ate the slowest runner. Then everyone tightened again. Then the tiger ate the next slowest. The race never ends because the replacement level ratchets up with every round.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-queuing-catastrophe","title":"The Queuing Catastrophe","text":"<p>Neither the VORP model nor the game theory captures what happens when the pipeline saturates. Armin Ronacher identified this in \"The Final Bottleneck\": software development is not a single-stage production process. It's a pipeline, and pipelines have throughput constraints.</p> <p>Model the software delivery pipeline as a queue:</p> <p>\u03bb(t) = rate of code creation (PRs per unit time)</p> <p>\u03bc = rate of code review and integration (PRs per unit time)</p> <p>L(t) = queue length (backlog of unreviewed PRs)</p> <p>When \u03bb &lt; \u03bc, the queue is stable. PRs get reviewed roughly as fast as they're created. The backlog stays bounded, and average wait time converges to \u03bc/(\u03bc\u2212\u03bb).</p> <p>When \u03bb &gt; \u03bc, the queue is unstable. The backlog grows without bound:</p> <p>L(t) \u2248 (\u03bb \u2212 \u03bc) \u00d7 t</p> <p>AI adoption increases \u03bb dramatically while \u03bc (human review and integration capacity) remains roughly constant. Ronacher's example is concrete: the OpenClaw project has over 2,500 open pull requests. That isn't a backlog that will clear with more reviewers; it's a queue growing faster than any team can process.</p> <p>\"Anyone who has worked with queues knows this,\" Ronacher writes. \"If input grows faster than throughput, you have an accumulating failure. At that point, backpressure and load shedding are the only things that retain a system that can still operate.\"</p> <p>The queuing insight intersects with VORP in a way that neither framework predicts alone. An engineer's raw VORP might be high: they produce substantial value per unit time, well above replacement. But if their output enters a queue and sits unreviewed, the realised value is delayed or zero. Code that never ships has no value, regardless of its quality.</p> <p>Define throughput-adjusted VORP:</p> <p>VORP_eff = [min(V_eng, \u03bc_system) \u2212 V_rep] \u00d7 T</p> <p>Where \u03bc_system is the system's review and integration throughput. An engineer producing at rate 10\u03bc has the same effective VORP as one producing at rate \u03bc. Above the throughput ceiling, additional output doesn't increase value. It increases backlog.</p> <p>Worse: excessive output imposes negative externalities on system throughput. Each additional PR in the review queue competes for the same finite review capacity, increasing wait times for every other PR, creating merge conflicts, and fragmenting reviewer attention. An engineer who floods the queue with AI-generated PRs doesn't just fail to add value above the throughput ceiling. They actively reduce the effective throughput available to everyone else.</p> <p>This inverts the standard VORP calculation. In baseball, more plate appearances are unambiguously good if your per-PA rate is above replacement. In the software pipeline, more output can be worse than less output once the queue saturates; the sign on marginal contribution flips from positive to negative. The engineer who writes less, but writes reviewable, shippable, integrate-on-first-read code, may have higher effective VORP than one who generates five times the volume.</p> <p>Ronacher's bathtub metaphor: \"Software engineers often believe that if we make the bathtub bigger, overflow disappears. It doesn't.\"</p> <p>Ethan Fann threads the needle from the practical side: \"Writing code isn't the bottleneck anymore. What to build and why is a bottleneck. QAing what gets built is a bottleneck. Knowing when to rip something out and replacing it is a bottleneck. Pretty much, taste is the bottleneck.\"</p> <p>Taste, in the queuing model, is the ability to maximise effective throughput per unit of review capacity consumed. The engineer with taste produces output that clears the review queue efficiently: well-scoped, well-tested, comprehensible, aligned with what the system actually needs. This is a different kind of skill altogether from producing output fast. The VORP model, unadjusted for throughput, cannot distinguish between the two. The throughput-adjusted version can.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-production-function-nobody-wrote-down","title":"The Production Function Nobody Wrote Down","text":"<p>The analysis now needs to confront its own assumption. VORP requires a production function: a measurable relationship between inputs (engineer time, skill, tools) and outputs (value created). Baseball has one. Plate appearances, hit types, run values; the whole apparatus rests on observable, countable events with well-established run expectancies derived from decades of play-by-play data.</p> <p>Software has never had one.</p> <p>Lines of code, story points, PRs merged, bugs fixed, features shipped: proxy metrics, and everyone in the industry knows they're unreliable. A 500-line PR that deletes dead code and simplifies an architecture may be worth more than a 5,000-line feature that introduces technical debt. A week of investigation that results in \"we shouldn't build this\" may be the most valuable output a team produces all quarter. The relationship between observable outputs and actual value has never been reliably specified.</p> <p>This doesn't invalidate the VORP framework. It transforms it. VORP applied to software engineering is illuminating not because it produces clean numbers but because the attempt to calculate it exposes exactly what we don't understand about software value creation. The model fails to compute, and the failure is informative.</p> <p>Consider what the model requires and what software can't provide:</p> <p>Replacement level. In baseball, you can observe replacement-level performance across thousands of games at every position. In software, the replacement level is theoretical. What would an AI agent produce if given this task? The answer depends on the task, the codebase, the organisational context, the quality of the specification. There is no single replacement level; there's a distribution, and the distribution has fat tails in both directions.</p> <p>Value per unit time. In baseball, run values are stable and well-calibrated across decades. In software, the value of a unit of work depends on what the market rewards, what the organisation needs, and what the system can absorb. An engineer who ships a critical security patch on a weekend produces more value in four hours than one who refactors CSS for a month. The same engineer, producing the same code, on a different Tuesday, for a different team, creates different value. Context is load-bearing.</p> <p>Playing time. In baseball, plate appearances are observable and roughly equal in duration. In software, what counts as \"playing time\"? Hours at keyboard? Hours thinking about the problem while walking the dog? Hours in a meeting that prevents another team from building the wrong thing? The denominator in the VORP equation is undefined.</p> <p>The VORP formula applied to software is a thought experiment, not a computation. But thought experiments have a distinguished history in economics. They clarify dynamics that empirical data can't isolate because the variables aren't separable. What the software VORP thought experiment clarifies: the structure of the value problem (marginal contribution above a rising replacement level, constrained by system throughput) is real even if the magnitudes are unknowable. The structural dynamics follow from the structure alone. The Prisoner's Dilemma, the ratchet, the queuing catastrophe: none require precise numbers. They require only two inequalities: dV_rep/dt &gt; 0, and \u03bb &gt; \u03bc. Both are observable. Both are true.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#what-sits-above-the-rising-floor","title":"What Sits Above the Rising Floor","text":"<p>Assume the structural dynamics are correct. Replacement level rising, queuing constraint binding, adoption game locked in destructive equilibrium. What remains above the floor?</p> <p>Ronacher's answer: accountability. \"Non-sentient machines will never be able to carry responsibility, and it looks like we will need to deal with this problem before machines achieve this status. Regardless of how bizarre they appear to act already.\" The bottleneck isn't skill or speed or even judgment in the abstract. It's the willingness to be the person who signs off. The person who gets called when it breaks at 3am.</p> <p>This echoes jbnews in the thread: \"You are not the bottleneck, you are the accountable party.\"</p> <p>Accountability is a strange kind of value. It doesn't show up in any production function. It isn't correlated with lines of code, PRs merged, or features shipped. It is the irreducible human contribution that exists because organisations need someone to be wrong. To own decisions, accept consequences, absorb blame when the machine's output creates problems that the machine cannot itself be held responsible for. In the formal model, accountability is the residual: whatever remains in V_eng after every measurable, automatable component of value has been absorbed into V_rep.</p> <p>Aristotle called this kind of capacity phronesis: practical wisdom, the ability to perceive the right action in particular circumstances, developed through experience and character, fundamentally different from theoretical knowledge or technical skill. If post-AI software engineering is about phronesis, then the path above replacement level runs through accumulated judgment, contextual understanding, and the kind of wisdom that only years of operating within a specific domain can produce.</p> <p>But Pierre Bourdieu would complicate this considerably. What looks like cultivated judgment, Bourdieu argued throughout his career, often functions as cultural capital: a way of maintaining status and excluding newcomers that masquerades as genuine expertise. \"We need experienced engineers for their judgment\" might describe a real capability. It might also describe a professional guild protecting its economic position by mystifying what it does. The fact that we can't write down the production function, that judgment resists measurement, is precisely what makes it useful as a status claim. If you can't measure it, you can't prove someone doesn't have it. And if you can't prove that, incumbency becomes its own credential.</p> <p>Both are partially right, and the tension between them is where the strategic analysis lives.</p> <p>Some of what we call engineering judgment is genuine phronesis. The architect who sees the subtle interaction between two systems that will cause problems at scale. The tech lead who recognises that the team is solving the wrong problem before anyone has written a line of code. The senior engineer who intuits that the elegant solution has a maintenance cost that will compound for years. This judgment is real, hard-won, and not yet reproducible by AI. It sits above the replacement level because it requires the kind of accumulated contextual knowledge that current systems don't possess and may not develop for some time.</p> <p>And some of what we call engineering judgment is cultural capital. The insistence on specific code review rituals whose value has never been measured. The preference for one architectural style over another that reflects training rather than evidence. The gatekeeping that keeps teams small and homogeneous under the banner of \"quality.\" This judgment sits above the replacement level only because the professional culture agrees to value it, not because it produces measurable outcomes.</p> <p>The difficulty, and this is where the opacity of the production function bites hardest, is that we cannot reliably distinguish between the two. We cannot point to a specific act of judgment and say \"that was genuine phronesis\" or \"that was status performance\" because the outputs of both are entangled with each other and with luck, timing, team dynamics, and market conditions. The model is honest about this: VORP for software engineers is structurally real and numerically unknowable. The floor is rising; what sits above it is partly skill, partly social construction, and we lack the instrumentation to tell them apart.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#escape-moves","title":"Escape Moves","text":"<p>Are there strategies that escape the destructive Nash equilibrium? In game theory, Prisoner's Dilemmas have three well-known resolution mechanisms: iteration, coordination, and game change.</p> <p>Iteration works when the game repeats and players can develop cooperative strategies through reciprocity. Tit-for-tat, the classic iterated PD strategy, cooperates initially and mirrors the other player's previous move. In the AI adoption game, iteration would look like professional norms: \"we adopt AI at a measured pace, maintaining code review standards, not flooding the review queue.\" This is roughly what the deliberate friction camp advocates. Tommy Falkowski: \"I increasingly think that deliberate friction is a necessary part of life.\" Giovanni Barillari: \"I'm happy to be slow. You miss a lot otherwise tbh.\"</p> <p>The problem with iteration is defection at scale. In a market with millions of participants, monitoring is impossible. You cannot enforce a norm of measured adoption when any individual engineer or company can defect by adopting faster and capturing temporary VORP advantage. Iterated PD resolution requires a small, identifiable group of repeated players who can observe and punish defection. The global software labour market is not that.</p> <p>Coordination works through binding agreements. Unions, professional standards bodies, licensing frameworks. If the industry collectively agreed on adoption standards, the Prisoner's Dilemma collapses: everyone cooperates, replacement level stays manageable, VORPs stabilise. Patrick Senti gestures at this: \"It stands to reason that productivity is not increased until throughput is. Thus we just quite simply reduce the pace to the slowest part of the pipeline.\"</p> <p>The problem with coordination is that software engineering has virtually no binding coordination mechanisms. No union in most jurisdictions, no professional licensing, no standards body with enforcement power. The largest actors (the companies that employ the most engineers and set market expectations) have strong incentives to defect from any coordination agreement because they can absorb the throughput costs that smaller firms cannot. A coordination solution requires exactly the institutional infrastructure that the software industry has historically resisted building.</p> <p>Game change is the most interesting resolution, and the most structurally different. Instead of playing the existing game better, you change the rules entirely.</p> <p>Rose Bennett in the thread: \"This is the 'horseless carriage' problem again. We called cars horseless carriages until we forgot about the horse entirely. Same thing is happening with agent-assisted PRs.\"</p> <p>Devesh Pal: \"Tickets and PRs are artifacts of human coordination overhead. Agents don't need tickets to coordinate with themselves.\"</p> <p>The game-change move is to redesign the production process so the queuing constraint doesn't bind. If code review is the bottleneck, don't try to review faster; redesign the workflow so that review is embedded in creation. AI writes the code and validates it simultaneously. The human's role shifts from reviewing individual PRs to governing the system that produces and validates them. Yoav Tzfati formalises the economics: \"I think we'll have to switch from reviewing code to having the AIs write much much better tests to prove that the behavior is right. Spending 10x the tokens on testing makes sense if it saves humans 2x on review.\"</p> <p>In game-theoretic terms, this changes the payoff matrix entirely. The game is no longer \"adopt AI tools within the existing workflow\" but \"redesign workflows so the throughput constraint becomes irrelevant.\" The first game is a Prisoner's Dilemma. The second game has a different structure: if workflow redesign increases both individual and collective throughput by removing the queue entirely, it can be a coordination game where the Nash equilibrium is also Pareto-optimal. Both players prefer the outcome where both redesign, and neither has an incentive to unilaterally defect back to the old workflow.</p> <p>The question underneath this game change is where the commoditised code flows. Kojin Karatani's framework of exchange modes offers a useful lens. Does faster, cheaper code creation feed into platform extraction: Copilot, Cursor, the AI-assisted IDEs that capture the productivity gains as subscription revenue? That's Karatani's Mode B, capital accumulation, the platform as factory owner. Or does it feed into a software commons: open-source tools, shared infrastructure, collective capability that raises the absolute standard of living for all engineers even as it compresses individual VORP? That's Mode D, reciprocal exchange, the commons as mutual benefit.</p> <p>The individual-level Prisoner's Dilemma of AI adoption may be inescapable within the current game. The higher-order question, which game the industry is constructing, is still open.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-coasean-twist","title":"The Coasean Twist","text":"<p>Ball's other thread, the one about disposable software, points to something the formal model cannot accommodate:</p> <p>\"That makes me wonder whether for a certain category of software it's just no longer worth it to generalise and put the effort in to 'solve it once and for all for everybody', because the cost of 'everyone creates their individual software for their need' is now lower than it was before.\"</p> <p>This is Coasean in structure. Ronald Coase's theory of the firm rests on comparing two costs: the cost of transacting in the market versus the cost of coordinating internally. Firms exist because internal coordination is cheaper than external transaction for certain activities. When transaction costs fall, firms shrink. When coordination costs fall, firms grow.</p> <p>AI is collapsing the cost of bespoke software creation towards zero. Ball's custom Raspberry Pi scripts, his ventilation system clone, his newsletter archive: software for an audience of one. Previously, creating bespoke software for each user exceeded the cost of building generalised solutions sold at scale, so we got products. When creation cost approaches zero, the Coasean calculus inverts. It becomes cheaper for each person to have software built to their exact specification than to tolerate a generalised product that approximately fits.</p> <p>His print service example crystallises it. ChatGPT Pro ran for 10-15 minutes, wrote Python code, produced a 4-page PDF formatted to a print service's specifications, and the code vanished. \"Is that 'software' as we know it? Maybe for a minute. But now it's gone.\" The software was ephemeral. The value was in the output, not the code.</p> <p>If this category grows, and Ball thinks it will, it scrambles the VORP analysis in a fundamental way. VORP assumes a production function where more of the output is better and where the output is durable enough to be valued. In a world of disposable, hyper-personal software, \"the output\" isn't a product that ships to users. It's ephemeral code that solves a problem for thirty seconds and disappears. The AI isn't replacing an engineer. It's replacing the decision not to build. The counterfactual isn't \"a junior dev would have done this.\" It's \"this wouldn't have existed at all.\"</p> <p>For this category, there is no queue, no review, no backlog, no team, and no Prisoner's Dilemma. The software lives and dies in the time it takes to run, and the concept of \"replacement level\" is as meaningful as the concept of a replacement-level text message.</p> <p>The formal models (VORP, the adoption game, the queuing analysis) describe the pressures facing engineers who build durable systems for organisations: software that is maintained, shipped, relied upon, and accountable to users. Those pressures are real, structural, and not resolvable by individual skill improvement alone. But the models also illuminate their own boundary. A growing fraction of software production is escaping the frameworks entirely, into territory where code is as disposable as a conversation and the concept of \"replacement\" doesn't apply because there was never a position to replace.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#the-floor-and-the-field","title":"The Floor and the Field","text":"<p>Both categories share one feature: the floor is rising. Whether you are an engineer building durable systems or a person prompting disposable scripts, the capability of the replacement-level alternative increases with each model generation.</p> <p>For durable software, the strategic question is how to maintain VORP above a floor that won't stop moving, within a game that punishes collective adoption, constrained by a pipeline that can't absorb the output. The math points toward three unstable positions: invest in non-automatable judgment (genuine phronesis, not cultural capital), redesign the workflow to change the game itself (eliminate the queue rather than trying to process it faster), or build the platforms that capture value from everyone else's adoption (own the infrastructure of the Red Queen race rather than running in it).</p> <p>None of these are stable equilibria in the long run. Judgment gets automated when it becomes legible. Workflow redesigns create new bottlenecks further downstream. Platforms face competition from other platforms and from the commons. The floor keeps rising, and the field reshapes around each new contour.</p> <p>For disposable software, the question is different and in some ways harder to hold. What does a profession look like when much of its traditional output becomes something anyone can conjure in minutes? When the valuable activity is no longer writing the code but knowing which code to conjure, for what purpose, with what constraints? This is Ronacher's final point, reframed: the bottleneck was always human, and it remains human. The nature of the bottleneck is what's changing.</p> <p>The math describes the dynamics; the game theory, the trap; the queuing model, the constraint. None prescribe the outcome, because the outcome depends on which game we choose to play. And that choice, for now, is the one thing the models confirm sits above replacement level: the capacity to decide, not just what to build, but what kind of building to do.</p>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/17/the-replacement-rate/#sources","title":"Sources","text":"<ul> <li>Keith Woolner, VORP methodology, Baseball Prospectus (2001-): baseballprospectus.com</li> <li>Thorsten Ball, \"Joy and Curiosity #74\" (2026): registerspill.thorstenball.com/p/joy-and-curiosity-74</li> <li>Thorsten Ball, X threads on agents, disposable software, and the average engineer (Feb 13-17, 2026): x.com/thorstenball</li> <li>Armin Ronacher, \"The Final Bottleneck\" (2026): lucumr.pocoo.org/2026/2/13/the-final-bottleneck/</li> <li>Trey Causey, reply connecting VORP to software engineering (Feb 16, 2026): x.com/quastora</li> <li>Tino Wening, reply on Prisoner's Dilemma and Nash equilibrium (Feb 15, 2026): x.com/TinoWening</li> <li>Sebastian Sigl, reply on skill atrophy (Feb 15, 2026): x.com/sesigl</li> <li>Ronald Coase, \"The Nature of the Firm\" (Economica, 1937)</li> <li>Kojin Karatani, The Structure of World History (Duke University Press, 2014)</li> <li>Pierre Bourdieu, Distinction: A Social Critique of the Judgement of Taste (Harvard University Press, 1984)</li> </ul>","tags":["ai","economics","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/","title":"The Decision Loop","text":"<p>DORA measures how fast code flows from commit to production. MOVE measures how fast the organization senses, decides, and acts. Both track real performance. Both share an assumption: that someone, somewhere, understood the system well enough to make the right call.</p> <p>That assumption has a cost, and it's larger than most organizations realize.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#the-loop","title":"The Loop","text":"<p>Every knowledge worker in software follows the same cycle. Form a hypothesis about the system. Explore it. Assess what you find. Decide what to do next. Engineers trace code paths. Product managers interrogate usage patterns. Designers audit component states. QA engineers reconstruct failure chains. The subject changes. The loop does not.</p> <p>Two variables govern its speed. Tudor Girba and Simon Wardley name them in Rewilding Software Engineering, their book-length argument that the software industry has been optimizing the wrong variable for decades.</p> <p>Time to Question (ttQ) \u2014 the time to formulate a specific, actionable question about a system.</p> <p>Time to Answer (ttA) \u2014 the time from that question to a verified answer.</p> <p>Nobody tracks either. Most organizations have never discussed them.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#time-to-question","title":"Time to Question","text":"<p>The time to formulate a specific, actionable question about a system.</p> <p>For a question to drive a decision, Girba and Wardley argue it must be three things. Actionable: if answered, someone can do something with the result. Specific: relevant to this system in this context. Timely: the answer will arrive before the environment changes enough to invalidate it.</p> <p>Most questions in organizations fail at least one criterion. \"Why is retention dropping?\" is actionable but too broad; the answer space is too large to guide action. \"What's the P99 latency of the payment service?\" is specific but may not be actionable for the person asking, because they lack the context to interpret what the number means for their decision. \"How should we restructure the monolith?\" fails all three if the answer takes six months and the organization will have reorganized twice by then.</p> <p>A large corporation needed to optimize a central data pipeline by an order of magnitude. The business case was clear. After years of effort and millions of dollars, data moved through the pipeline at the same speed as before. The team was capable, the resources were there. The constraint was visibility.</p> <p>They started where everyone starts: \"Why are our services slow?\"</p> <p>Reasonable question. Almost useless, because answering it in the general case requires understanding the entire system, which is the problem that produced the question.</p> <p>Weeks of exploration refined it. \"Why are our services slow?\" became \"Is there useless data generated?\", then \"How is data traversing the pipeline?\", then \"How is a single attribute traversing the pipeline?\", which, only after cheap answers made the landscape visible, became \"What data is used only in the most recent campaign?\"</p> <p>Each refinement was more specific, more actionable, more valuable. The distance between the first question and the last is Time to Question made visible. In this case, it was measured in weeks.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#time-to-answer","title":"Time to Answer","text":"<p>The time from a specific question about a system to a verified answer.</p> <p>Time to Question gets less attention, but Time to Answer is what makes it expensive.</p> <p>Tracing how a single variable moved through the pipeline required person-weeks of manual inspection. The system contained tens of thousands of variables. At that rate, answering even basic structural questions was economically prohibitive. The team couldn't verify their own hypotheses. Their best model of the system, a hand-drawn architecture diagram, omitted an entire third-party system that nobody knew existed. Decisions were being made against a picture that was materially wrong.</p> <p>This same structure appears everywhere the decision loop runs.</p> <p>An engineer asks \"what calls this service?\" and spends days reading code across repositories because the dependency graph lives on a whiteboard, drawn from memory, last validated months ago. A product manager asks \"which features drive retention in segment X?\" and waits two weeks for a custom query because the analytics platform organizes data by event type, not user journey. A designer asks \"what states can this component reach in production?\" and manually navigates edge cases because the design system documents intended states, not actual ones.</p> <p>In each case, formulating the question took seconds. Extracting a verified answer required days or weeks: reading code, building queries, navigating tools that weren't designed for the question being asked. Girba and Wardley use a recurring image for this: kitchen blenders digging deep mines. The tool works. It wasn't designed for this.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#the-flywheel","title":"The Flywheel","text":"<p>When Time to Answer drops, when answering a question about your system takes minutes instead of weeks, you can afford more questions. More questions means more explored landscape. More landscape means sharper hypotheses. Sharper hypotheses target higher-value answers. And higher-value answers reveal territory you didn't know existed: what Girba and Wardley call the adjacent unexplored.</p> <p>The data pipeline team built 54 contextual micro-tools in two person-months. Each tool answered a specific class of question directly from the live system, the way automated tests generate pass/fail signals from running code. The average tool was twelve lines of code. Total investment: a fraction of what manual investigation had consumed over years.</p> <p>The improvement factor was roughly 600x. The number understates what actually changed. Cheap answers made iterative exploration affordable, and that exploration revealed that the original question was aimed at the wrong part of the system entirely. The team couldn't have skipped to the right question. The path through wrong questions was the only path to the right one. What made the difference was a cost structure where each wrong question didn't consume person-weeks.</p> <p>The reverse is equally instructive. When answers are expensive, each question is an investment. Teams choose carefully, conservatively, optimizing for probability of payoff over breadth of discovery. This is how organizations spend years and millions on a problem that two people with contextual tools resolve in a month.</p> <p>This is why Time to Question improves when Time to Answer drops. The data pipeline team didn't learn to ask better questions through training. They learned because cheaper answers revealed more landscape, and more landscape made sharper questions possible. Cheap answers accelerate the accumulation of system knowledge that produces good questions, while expensive answers lock teams into generic questioning regardless of their expertise.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#the-layer-beneath","title":"The Layer Beneath","text":"<p>Three frameworks now describe three layers of the same organizational system.</p> <p></p> Layer Framework Measures Deployment DORA How fast code ships: cycle time, deployment frequency, failure rate, recovery time Operations MOVE How fast the org operates: Momentum, Orchestration, Velocity, Exception Comprehension Decision Loop How fast people understand: Time to Question, Time to Answer <p>Each depends on the one beneath it. DORA assumes someone decided what to deploy. MOVE assumes someone designed the workflow correctly. Both assume comprehension that neither measures.</p> <p>When MOVE metrics plateau, when Momentum stalls despite automation and Exception rates climb despite investment, the cause is usually comprehension speed. An organization with 60% Orchestration and rising Exception rates is often looking at this: the workflows were designed against an incomplete picture of the system, because the people who designed them couldn't get answers fast enough to ask the right questions.</p> <p>This also explains why AI copilots improve MOVE metrics to a point and then stop. Copilots accelerate execution within the existing understanding of the system. They do not change how fast the organization comprehends its own systems. Momentum lifts, Orchestration plateaus around 25%, and the decision loop, still running on expensive answers and broad questions, remains the binding constraint.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#decision-profiles","title":"Decision Profiles","text":"<p>The two metrics cluster into recognizable patterns.</p> Tier Time to Answer Question Specificity Elite Minutes Domain-specific High Hours Moderately specific Medium Days Generic Low Weeks+ Unformulated <p>Most organizations operate at Medium. Answers take days. Each question represents a significant investment, so teams choose questions conservatively. Exploration is treated as cost rather than a source of value.</p> <p>Thresholds vary by question type and system complexity. Minutes to answer \"what services depend on this table?\" is elite for legacy migration. It is meaningless for \"should we enter this market?\" Set baselines per question class.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#getting-started","title":"Getting Started","text":"<p>Pick one question each role answered in the last month. Something real.</p> <p>Time the answer. When was the question first articulated? When did a verified answer arrive? Count everything: queue time waiting for another team, tool-switching between platforms, manual inspection, meetings convened to discuss what the data means. That total is your current Time to Answer.</p> <p>Audit the answer. Did it come from a generated view of the live system, or from a manually created artifact: a dashboard configured months ago, a diagram drawn from memory, a spreadsheet assembled from multiple sources? Was the tool built for this question, or was a general-purpose tool pressed into service?</p> <p>Trace the question. Was this the first question asked, or the refinement of a broader one? How many cycles of question and answer did it take to reach this level of specificity? Where did refinement stall, because an intermediate answer was too expensive or the tooling couldn't support a sharper question? That path is Time to Question made visible.</p> <p>Repeat across roles. Engineering, product, design, QA. The question types will differ. The extraction patterns will differ. The structural bottleneck will be the same: generic tools producing expensive answers, suppressing question quality, and limiting exploration.</p> <p>The most expensive activity in software is figuring out what to build and whether what was built is right. Every operational era develops its own measurement language. Manufacturing brought throughput and defect rates. DevOps brought DORA. The AI-native organization brought MOVE. The decision loop is still unmeasured.</p>","tags":["ai","strategy","future-of-work"]},{"location":"blog/2026/02/23/the-decision-loop/#sources","title":"Sources","text":"<ul> <li>Tudor Girba and Simon Wardley, Rewilding Software Engineering (feenk, 2025-2026): medium.com/feenk</li> <li>Tudor Girba, \"Developers spend most of their time figuring the system out\" (feenk, 2024): blog.feenk.com</li> </ul>","tags":["ai","strategy","future-of-work"]},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/page/2/","title":"Archive","text":""},{"location":"blog/page/3/","title":"Archive","text":""},{"location":"blog/archive/2026/page/2/","title":"2026","text":""}]}